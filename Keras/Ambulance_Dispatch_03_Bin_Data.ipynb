{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767154e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054af39c",
   "metadata": {},
   "source": [
    "# readme\n",
    "## Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739e724",
   "metadata": {},
   "source": [
    "We designed this code to fit in a GitHub repository with files under 100MB.  Many of the files we input are over this limit, and after preprocessing (selecting features, binning features) we saved the data as a .csv file of about 150 MB so we can tweak later code without having to run the preprocessing again.  We saved it again after imputing missing data.  To keep the files in our GitHub repository under 100 MB, we saved these into a different directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00029d63",
   "metadata": {},
   "source": [
    "- CRSS Data Files\n",
    "    - We use six years of data, 2016-2021. Once later years come out, they can be easily added. \n",
    "    - Each year's data is 100-200 MB\n",
    "    - The files we're really interested in each year are these.  The names were uppercase until 2018, then lowercase, and also after 2018 the file sizes jumped.\n",
    "        - accident.csv or ACCIDENT.csv, now about 30 MB\n",
    "        - vehicle.csv or VEHICLE.csv, now about 180 MB\n",
    "        - person.csv or PERSON.csv, now about 150 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f95f5",
   "metadata": {},
   "source": [
    "- Big_Files\n",
    "    - CRSS_Files\n",
    "        - CRSS2016CSV (22 files, 160 MB)\n",
    "        - CRSS2017CSV (22 files, 189 MB)\n",
    "        - CRSS2018CSV (22 files, 169 MB)\n",
    "        - CRSS2019CSV (23 files, 633 MB)\n",
    "        - CRSS2020CSV (29 files, 719 MB)\n",
    "        - CRSS2021CSV (29 files, 736 MB)\n",
    "        \n",
    "        \n",
    "    - *Intermediate .csv files*\n",
    "- GitHub_Repository\n",
    "    - Code_Files\n",
    "        - Analyze_Proba\n",
    "        - Confusion_Matrices\n",
    "        - Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e1d2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7d47d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b4fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('Install Packages')\n",
    "\n",
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "#from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import scipy as sc\n",
    "print ('SciPy version:  {}'.format(sc.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print ('TensorFlow version:  {}'.format(tf.__version__))\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "from tensorflow import keras\n",
    "print ('Keras version:  {}'.format(keras.__version__))\n",
    "\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "from keras.layers import IntegerLookup\n",
    "from keras.layers import Normalization\n",
    "from keras.layers import StringLookup\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.utils import tf_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "#    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Library for reading Microsoft Access files\n",
    "#import pandas_access as mdb\n",
    "\n",
    "import sklearn\n",
    "print ('SciKit-Learn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import imblearn\n",
    "print ('Imbalanced-Learn version: {}'.format(imblearn.__version__))\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "#!pip install pydot\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "#np.random.seed(42) # NumPy\n",
    "#random.seed(42) # Python\n",
    "#tf.random.set_seed(42) # Tensorflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Installing Packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d0f6e",
   "metadata": {},
   "source": [
    "# Get Data and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b290fc",
   "metadata": {},
   "source": [
    "## Read CRSS Files\n",
    "- We have the CRSS dataset in \n",
    "    - Big_Files/CRSS_2020_Update/\n",
    "- In one directory for each year,\n",
    "    - CRSS2016CSV\n",
    "    - CRSS2017CSV\n",
    "    - CRSS2018CSV\n",
    "    - CRSS2019CSV\n",
    "    - CRSS2020CSV    \n",
    "    - CRSS2021CSV    \n",
    "- In each year, the CRSS dataset comes in three main files, \n",
    "    - Accident.csv\n",
    "    - Vehicle.csv\n",
    "    - Person.csv\n",
    "- Collect those and merge into three files,\n",
    "    - Accident_Raw.csv\n",
    "    - Vehicle_Raw.csv\n",
    "    - Person_Raw.csv\n",
    "- and also three files with category names,\n",
    "    - Accident_Raw_with_Names.csv\n",
    "    - Vehicle_Raw_with_Names.csv\n",
    "    - Person_Raw_with_Names.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13504c",
   "metadata": {},
   "source": [
    "### accident.csv from CRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data_Accident(NAMES):\n",
    "    print ('Import_Data_Accident()')\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "#    for year in ['2018']:\n",
    "    for year in ['2016','2017','2018']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/ACCIDENT.CSV'\n",
    "        temp = pd.read_csv(filename, index_col=None)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "#    for year in ['2020']:\n",
    "    for year in ['2019','2020','2021']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/accident.csv'\n",
    "        temp = pd.read_csv(filename, index_col=None)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "    \n",
    "    if NAMES==0:\n",
    "        for feature in df:\n",
    "            if 'NAME' in feature:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print (df.shape)\n",
    "    print ()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bca27",
   "metadata": {},
   "source": [
    "### vehicle.csv from CRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data_Vehicle(NAMES):\n",
    "    print ('Import_Data_Vehicle()')\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "    for year in ['2016','2017','2018']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/VEHICLE.CSV'\n",
    "        temp = pd.read_csv(filename, index_col=None, low_memory=False)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    for year in ['2019','2020','2021']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/vehicle.csv'\n",
    "        temp = pd.read_csv(filename, index_col=None, encoding='latin1', low_memory=False)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    if NAMES==0:\n",
    "        for feature in df:\n",
    "            if 'NAME' in feature:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print (df.shape)\n",
    "    print ()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7b6cf",
   "metadata": {},
   "source": [
    "### person.csv from CRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62699818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data_Person(NAMES):\n",
    "    print ('Import_Data_Person()')\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "    for year in ['2016','2017','2018']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/PERSON.CSV'\n",
    "        temp = pd.read_csv(filename, index_col=None)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    for year in ['2019','2020','2021']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/person.csv'\n",
    "        temp = pd.read_csv(filename, index_col=None, encoding='latin1')\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    if NAMES==0:\n",
    "        for feature in df:\n",
    "            if 'NAME' in feature:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print (df.shape)\n",
    "    print ()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d05f7",
   "metadata": {},
   "source": [
    "### Get Data\n",
    "- The Get_Data_from_Original() reads the (original) CRSS files from the CRSS directory, preprocesses it, and writes it to files in a folder outside this GitHub repo (because the files are too large for my subscription), and returns the dataframes.\n",
    "- The Get_Data_from_Temp_Files() reads the temp files and returns the dataframes.  I created this option for running repeatedly during writing and debugging, because it's much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a66f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data_from_Original():\n",
    "    print ('Get_Data_from_Original()')\n",
    "    \n",
    "    df_Accident = Import_Data_Accident(0)\n",
    "    df_Vehicle = Import_Data_Vehicle(0)\n",
    "    df_Person = Import_Data_Person(0)\n",
    "    \n",
    "    df_Accident.to_csv('../../Big_Files/Accident_Raw.csv', index=False)\n",
    "    df_Vehicle.to_csv('../../Big_Files/Vehicle_Raw.csv', index=False)\n",
    "    df_Person.to_csv('../../Big_Files/Person_Raw.csv', index=False)\n",
    "    \n",
    "\n",
    "    df_Accident = Import_Data_Accident(1)\n",
    "    df_Vehicle = Import_Data_Vehicle(1)\n",
    "    df_Person = Import_Data_Person(1)\n",
    "    \n",
    "    df_Accident.to_csv('../../Big_Files/Accident_Raw_with_NAMES.csv', index=False)\n",
    "    df_Vehicle.to_csv('../../Big_Files/Vehicle_Raw_with_NAMES.csv', index=False)\n",
    "    df_Person.to_csv('../../Big_Files/Person_Raw_with_NAMES.csv', index=False)\n",
    "    \n",
    "\n",
    "    return df_Accident, df_Vehicle, df_Person\n",
    "\n",
    "#df_Accident, df_Vehicle, df_Person = Get_Data_from_Original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ba441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_New_Files():\n",
    "    print ('Check_New_Files')\n",
    "    Files = [\n",
    "        'Accident_Raw',\n",
    "        'Vehicle_Raw',\n",
    "        'Person_Raw',\n",
    "        'Accident_Raw_with_Names',\n",
    "        'Vehicle_Raw_with_Names',\n",
    "        'Person_Raw_with_Names'\n",
    "    ]\n",
    "    for filename in Files:\n",
    "        df = pd.read_csv('../../Big_Files/' + filename + '.csv', low_memory=False)\n",
    "        print (filename, df.shape)\n",
    "    \n",
    "    return 0    \n",
    "\n",
    "#Check_New_Files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data_from_Temp_Files():\n",
    "    print ('Get_Data')\n",
    "    df_Acc = pd.read_csv('../../Big_Files/Accident_Raw.csv', low_memory=False)\n",
    "    df_Veh = pd.read_csv('../../Big_Files/Vehicle_Raw.csv', low_memory=False)\n",
    "    df_Per = pd.read_csv('../../Big_Files/Person_Raw.csv', low_memory=False)\n",
    "    \n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Veh.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    return df_Acc, df_Veh, df_Per\n",
    "\n",
    "#df_Acc, df_Veh, df_Per = Get_Data_from_Temp_Files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabf849",
   "metadata": {},
   "source": [
    "## Drop Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce07f8",
   "metadata": {},
   "source": [
    "- We now have three dataframes from the Accident, Vehicle, and Person files.  \n",
    "- Some features are repeated, so we will drop the ones in Vehicle or Person that appear in Accident, and drop those in Person that appear in Vehicle. \n",
    "- There are two repeated features we need to keep for merging the three data:\n",
    "    - CASENUM tells us to which accident the vehicle and person correspond\n",
    "    - VEH_NO tells us which vehicle the person was in.\n",
    "- Some features have no predictive power and/or resemble random numbers, like the VIN (Vehicle Identification Number) and the minute of the accident time.  \n",
    "- For details on the features, see the *Crash Report Sampling System Analytical User's Manual 2016-2020.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cc781",
   "metadata": {},
   "source": [
    "### Drop Repeated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_Repeated_Features(df_Acc, df_Veh, df_Per):\n",
    "    print ('Drop_Repeated_Features()')\n",
    "    Acc_Cols = df_Acc.columns.tolist()\n",
    "    Veh_Cols = df_Veh.columns.tolist()\n",
    "    Per_Cols = df_Per.columns.tolist()\n",
    "    \n",
    "    Drop_Veh = [x for x in Veh_Cols if x in Acc_Cols]\n",
    "    Drop_Per = [x for x in Per_Cols if (x in Acc_Cols or x in Veh_Cols)]\n",
    "        \n",
    "    \"\"\"\n",
    "    print ('Drop_Veh:')\n",
    "    for item in Drop_Veh:\n",
    "        print (item)\n",
    "    print ()\n",
    "\n",
    "    print ('Drop_Per:')\n",
    "    for item in sorted(Drop_Per):\n",
    "        print (item)\n",
    "    print ()\n",
    "    \"\"\"    \n",
    "    \n",
    "    # We need to keep these for merging the dataframes.\n",
    "    Drop_Veh.remove('CASENUM')\n",
    "    Drop_Per.remove('CASENUM')\n",
    "    Drop_Per.remove('VEH_NO')\n",
    "    \n",
    "    df_Veh.drop(columns=Drop_Veh, inplace=True)\n",
    "    df_Per.drop(columns=Drop_Per, inplace=True)\n",
    "\n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Vet.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    return df_Acc, df_Veh, df_Per\n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819611e1",
   "metadata": {},
   "source": [
    "### Drop Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d43160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_Irrelevant_Features(df_Acc, df_Veh, df_Per):\n",
    "    \n",
    "    print ('Drop_Irrelevant_Features')\n",
    "    \n",
    "    Drop_Accident = [\n",
    "        'CF1',\n",
    "        'CF2',\n",
    "        'CF3',\n",
    "        'MINUTE',\n",
    "        'MINUTE_IM',\n",
    "        'PSU_VAR',\n",
    "        'PSUSTRAT',\n",
    "        'STRATUM',\n",
    "        'WEATHER1',\n",
    "        'WEATHER2',\n",
    "        'WEIGHT',\n",
    "    ]\n",
    "    \n",
    "    df_Acc.drop(columns=Drop_Accident, inplace=True)\n",
    "    \n",
    "    # List of features in df_Veh that aren't repeats from df_Acc \n",
    "    # that we don't want to use, even for imputation, because\n",
    "    # they're only for some years or are like random numbers\n",
    "    Drop_Vehicle = [\n",
    "        'DR_SF1',\n",
    "        'DR_SF2',\n",
    "        'DR_SF3',\n",
    "        'DR_SF4',\n",
    "        'DR_ZIP',\n",
    "        'GVWR',\n",
    "        'GVWR_FROM',\n",
    "        'GVWR_TO',\n",
    "        'HAZ_ID',\n",
    "        'ICFINALBODY',\n",
    "        'MCARR_I1',\n",
    "        'MCARR_I2',\n",
    "        'MCARR_ID',\n",
    "        'TRLR1GVWR',\n",
    "        'TRLR1VIN',\n",
    "        'TRLR2GVWR',\n",
    "        'TRLR2VIN',\n",
    "        'TRLR3GVWR',\n",
    "        'TRLR3VIN',\n",
    "        'UNITTYPE',\n",
    "        'V_CONFIG',\n",
    "        'V_Config',\n",
    "        'VEH_SC1',\n",
    "        'VEH_SC2',\n",
    "        'VIN',\n",
    "        'VPICBODYCLASS',\n",
    "        'VPICMAKE',\n",
    "        'VPICMODEL',\n",
    "    ]\n",
    "    \n",
    "    df_Veh.drop(columns=Drop_Vehicle, inplace=True)\n",
    "    \n",
    "    Drop_Person = [\n",
    "        'ATST_TYP',\n",
    "        'DRUGRES1',\n",
    "        'DRUGRES2',\n",
    "        'DRUGRES3',\n",
    "        'DRUGTST1',\n",
    "        'DRUGTST2',\n",
    "        'DRUGTST3',\n",
    "        'DSTATUS',\n",
    "        'HELM_MIS',\n",
    "        'HELM_USE',\n",
    "        'P_SF1',\n",
    "        'P_SF2',\n",
    "        'P_SF3',\n",
    "        'STR_VEH',\n",
    "    ]\n",
    "    \n",
    "    df_Per.drop(columns=Drop_Person, inplace=True)\n",
    "    \n",
    "    \n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Veh.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    \n",
    "    return df_Acc, df_Veh, df_Per"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5fdb7",
   "metadata": {},
   "source": [
    "## Bin Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97705a78",
   "metadata": {},
   "source": [
    "- Almost all of our features are categorical data.  \n",
    "- Many useful features have more than fifty unique categories, and a general rule of thumb with a dataset of many features is that the machine learning algorithm works optimally with five to ten unique values per feature.  \n",
    "- To reduce the number of values in each feature we binned them together, for instance changing \"Hour\" from 24 values to seven.  Most CRSS features have a code like '99' to signify 'Unknown.'\n",
    "\n",
    "| Bin | Hours | Meaning |\n",
    "|---|---|---|\n",
    "| 0 | 5, 6 | Early Morning |\n",
    "| 1 | 7, 8, 9, 10 | Morning |\n",
    "| 2 | 11, 12, 13, 14 | Mid-Day |\n",
    "| 3 | 15, 16, 17 | Rush Hour |\n",
    "| 4 | 18, 19 | Early Evening |\n",
    "| 5 | 20, 21, 22 | Evening | \n",
    "| 6 | 23, 0, 1, 2, 3, 4 | Late Night |\n",
    "| 'Unknown' | 99 | |\n",
    "\n",
    "- We decided which hours to put together by looking at, for each hour, the proportion of crash person who needed an ambulance.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179472f6",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a27082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Individual_Feature_with_Dict(df, data, feature, A):\n",
    "    D = {}\n",
    "    for B in A:\n",
    "        for b in B[1]:\n",
    "            D[b] = B[0]\n",
    "\n",
    "    data[feature] = df[feature].replace(D)\n",
    "    \n",
    "#    print (feature)\n",
    "#    print (df[feature].value_counts())\n",
    "#    print ('isna(): ', df[feature].isna().sum())\n",
    "#    print (data[feature].value_counts())\n",
    "#    print ()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_Binning(df):\n",
    "    print ('Analyze_Binning')\n",
    "    print ('Feature, nUnique, nUnknown')\n",
    "    Cols = df.columns.values.tolist()\n",
    "    Cols = sorted(Cols)\n",
    "    N = df.shape[0]\n",
    "    A = []\n",
    "    for feature in Cols:\n",
    "        u =  len(df[feature].unique())\n",
    "        nUnknown = df[df[feature] == 'Unknown'].shape[0]\n",
    "        A.append([feature, u, nUnknown, round(nUnknown/N*100,0)])\n",
    "    B = pd.DataFrame(A, columns=['Feature', 'nUnique', 'nUnknown', 'pUnknown'])\n",
    "    display(B)\n",
    "    \n",
    "    C = B[B['pUnknown'] > 20]\n",
    "    display(C)\n",
    "        \n",
    "    print ()\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8b4f4",
   "metadata": {},
   "source": [
    "### Bin Accident Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Accident_Dataset(df_Acc):\n",
    "    print ('Build_Accident_Dataset()')\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # Reference\n",
    "    data['CASENUM'] = df_Acc['CASENUM']\n",
    "    \n",
    "    feature = 'HOUR_IM'\n",
    "    A = [\n",
    "        [0, [5,6]], # Early Morn\n",
    "        [1, [7,8,9,10]], # Morning\n",
    "        [2, [11,12,13,14]], # Mid_Day\n",
    "        [3, [15,16,17]], # Rush_Hour\n",
    "        [4, [18,19]], # Early_Eve\n",
    "        [5, [20,21,22]], # Evening\n",
    "        [6,[23,0,1,2,3,4]], # Late Night\n",
    "        ['Unknown', [99]],\n",
    "             ]\n",
    "    \n",
    "    feature = 'HOUR_IM'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    feature = 'HOUR'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    feature = 'INT_HWY'\n",
    "    A = [\n",
    "        [0, [0,9]],\n",
    "        [1, [1]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    A =  [\n",
    "        [0, [2]], # Dark\n",
    "        [1, [3,4,6]], # Dawn_Lighted\n",
    "        [2, [5]], # Dusk\n",
    "        [3, [1,7]], # Daylight\n",
    "        ['Unknown', [8,9]], \n",
    "    ]\n",
    "    feature = 'LGTCON_IM'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    feature = 'LGT_COND'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    feature = 'MONTH'\n",
    "    A = [\n",
    "        [0, [1,2,3,12]], # Winter\n",
    "        [1, [4,5,10,11]], # Spring_Fall\n",
    "        [2, [6,7,8,9]], # Summer\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    feature = 'PEDS'\n",
    "    B = [x for x in list(df_Acc[feature].unique()) if x not in [0,1]]\n",
    "#    print (B)\n",
    "    A = [\n",
    "        ['0', [0]],\n",
    "        ['1', [1]],\n",
    "        ['2', B] # Multiple\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    feature = 'PERMVIT'\n",
    "    B = [x for x in list(df_Acc[feature].unique()) if x not in [1,2]]\n",
    "#    print (B)\n",
    "    A = [\n",
    "        [0, [1]],\n",
    "        [1, [2]],\n",
    "        [2, B] # Multiple\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    feature = 'REL_ROAD'\n",
    "    A =  [\n",
    "        [0, [2,3,4,5,6,8,10,12,98,99]], # Not on Road\n",
    "        [1, [1,11]], # On road\n",
    "        [2, [7]], # Parking area\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    A = [\n",
    "        ['0', [2,5,6,19]],\n",
    "        ['1', [1,7,16]],\n",
    "        ['2', [4,8,18]],\n",
    "        ['3', [3,17,20]],\n",
    "        ['Unknown', [98,99]],\n",
    "    ]\n",
    "    feature = 'RELJCT2_IM'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    feature = 'RELJCT2'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'SCH_BUS'\n",
    "    A = [\n",
    "        ['0', [0]],\n",
    "        ['1', [1]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'URBANICITY'\n",
    "    A = [\n",
    "        ['1', [1]],\n",
    "        ['2', [2]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    feature = 'VE_TOTAL'\n",
    "    B = [x for x in list(df_Acc[feature].unique()) if x not in [1,2,3]]\n",
    "#    print (B)\n",
    "    A = [\n",
    "        ['1', [1]],\n",
    "        ['2', [2]],\n",
    "        ['3', [3]],\n",
    "        ['4', B] # Multiple\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    A = [\n",
    "        ['0', [3,5]],\n",
    "        ['1', [1]],\n",
    "        ['2', [2]],\n",
    "        ['3', [10]],\n",
    "        ['4', [4,6,7,8,11,12]],\n",
    "        ['Unknown', [98,99]],\n",
    "    ]\n",
    "    feature = 'WEATHR_IM'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    feature = 'WEATHER'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    A = [\n",
    "        ['0', [1,7]], # Weekend\n",
    "        ['1', [2,3,4,5,6]], # Weekday\n",
    "        ['Unknown', [9]],\n",
    "    ]\n",
    "    feature = 'WKDY_IM'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    feature = 'DAY_WEEK'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'WRK_ZONE'\n",
    "    A = [\n",
    "        ['0', [0]],\n",
    "        ['1', [1,4]],\n",
    "        ['2', [2]],\n",
    "        ['3', [3]],\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    \n",
    "    feature = 'VE_FORMS'\n",
    "    B = [x for x in list(df_Acc[feature].unique()) if x not in [1,2,3]]\n",
    "#    print (B)\n",
    "    A = [\n",
    "        ['1', [1]],\n",
    "        ['2', [2]],\n",
    "        ['3', [3]],\n",
    "        ['4', B] # Multiple\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    \n",
    "    feature = 'PVH_INVL'\n",
    "    B = [x for x in list(df_Acc[feature].unique()) if x not in [0]]\n",
    "#    print (B)\n",
    "    A = [\n",
    "        ['0', [0]], # None\n",
    "        ['1', B] # Some\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    \n",
    "    feature = 'PERNOTMVIT'\n",
    "    B = [x for x in list(df_Acc[feature].unique()) if x not in [0]]\n",
    "#    print (B)\n",
    "    A = [\n",
    "        ['0', [0]], # None\n",
    "        ['1', B]  # Some\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    \n",
    "    feature = 'NUM_INJ'\n",
    "    B = [x for x in list(df_Acc[feature].unique()) if x not in [0,1,2,3,99]]\n",
    "#    print (B)\n",
    "    A = [\n",
    "        ['0', [0]],\n",
    "        ['1', [1]],\n",
    "        ['2', [2]],\n",
    "        ['3', [3]],\n",
    "        ['4', B], # Multiple\n",
    "        ['Unknown', [99]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    \n",
    "    feature = 'NO_INJ_IM'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "        \n",
    "    # Split into five bins, each about 20% of samples, ordered by correlation\n",
    "    feature = 'PSU'\n",
    "    A = [\n",
    "        ['0', [15,75,34,57,40,66,76,80,52,64,68,60,50,10,24,55,47,49,31,]],\n",
    "        ['1', [62,53,63,72,17,56,30,48,35,]],\n",
    "        ['2', [65,82,25,32,83,78,12,45,58,13,]],\n",
    "        ['3', [67,14,26,70,28,22,33,81,29,20,54,77,]],\n",
    "        ['4', [27,61,39,41,51,59,38,37,46,44,]],\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    \n",
    "    # Split into five bins, each about 20% of samples, ordered by correlation\n",
    "    feature = 'PJ'\n",
    "    A = [\n",
    "        ['0', [3087,147,2904,3292,3069,47,3090,1225,2800,149,610,598,2705,1645,587,1688,2211,2171,2412,3089,2679,453,2139,2537,2764,1222,1801,4113,2514,2582,2722,2298,189,1741,1750,85,1766,1223,1684,2513,2775,171,4144,4056,1231,173,4047,2330,299,172,1362,1634,2793,1747,308,256,307,1692,1055,1838,96,1070,1227,1392,1678,1230,3262,3106,2735,3224,2160,91,1805,313,4147,2286,3076,1315,2586,1460,1757,1709,1802,1800,2591,542,209,46,305,4107,3122,565,1693,2001,1763,1811,2881,3077,268,1308,3247,1762,1804,2883,1219,4016,3073,2035,295,606,205,1829,260,718,4149,839,2592,4152,1764,1921,]],\n",
    "        ['1', [618,1803,4135,234,250,2702,2087,1695,1733,526,2906,2905,206,2018,2854,2973,4125,2749,245,4015,174,297,2972,1053,306,311,2292,970,2598,1708,321,4150,1290,359,1723,257,2803,1197,92,2670,1207,458,322,1208,285,86,261,267,315,2811,455,1036,2809,1714,459,4148,232,4151,4138,1646,640,4141,161,4153,2034,1710,2365,3296,2607,591,2807,1259,]],\n",
    "        ['2', [1736,2851,432,210,625,936,262,461,4114,3004,437,1191,1056,4142,45,2857,4143,1374,4055,1746,2509,1635,3294,4019,2364,457,97,4146,1482,1041,4145,1265,2091,1459,892,87,1114,1637,4139,893,456,2136,2682,2810,466,170,1919,516,2808,448,314,460,3070,3119,]],\n",
    "        ['3', [464,3139,4028,3291,1075,2799,1069,2907,318,329,469,4045,123,1088,4093,1724,1052,652,1835,148,1925,1571,452,3245,2792,479,967,441,508,211,2825,4012,1040,2763,2137,1484,1699,3131,1283,2853,1255,3011,440,2199,2855,3248,567,2197,2666,4140,1117,2759,3010,310,2782,214,309,966,1920,1928,208,138,2152,159,1481,3202,3133,1573,2797,2819,1038,1098,1577,3017,590,1079,543,896,2802,130,1366,1278,721,650,1472,900,137,165,]],\n",
    "        ['4', [454,1262,500,2092,1568,3019,1628,3246,162,506,1570,336,954,1050,505,90,269,1260,2812,1383,504,160,965,972,929,3159,4137,2717,2168,573,163,2151,1477,1369,1080,1933,3013,1163,1078,1361,1930,4136,386,341,687,382,569,571,1247,503,2687,2748,378,877,517,2755,509,641,136,338,905,568,578,2025,3124,525,514,3253,1721,515,1319,1381,362,334,3200,375,388,337,369,3201,1043,353,3203,360,4036,4029,2411,3209,]],\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    \n",
    "    feature = 'MAN_COLL'\n",
    "    A = [\n",
    "        ['0', [2]],\n",
    "        ['1', [0]],\n",
    "        ['2', [6,8]],\n",
    "        ['3', [1]],\n",
    "        ['4', [7,11,9,10]],\n",
    "        ['Unknown', [98,99]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    \n",
    "    feature = 'MANCOL_IM'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    \n",
    "    # Grouped these by the groupings given in the Analytical Users Manual \n",
    "    # because 78% of the crashes were '12', crash with another vehicle.  \n",
    "    feature = 'HARM_EV'\n",
    "    A = [\n",
    "        ['0', [1,2,3,4,5,6,7,16,44,51,72]], # Non Collision\n",
    "        ['1', [12,54,55]], # Collision with MVIT\n",
    "        ['2', [8,9,10,11,14,15,18,45,49,73,74,91]], # Collision with Object Not Fixed\n",
    "        ['3', [17,19,20,21,23,24,25,26,30,31,32,33,34,35,38,39,40,41,42,43,46,48,50,52,53,57,58,59,93]], # Collision with Fixed Object\n",
    "        ['Unknown', [98,99]],        \n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "    \n",
    "    feature = 'EVENT1_IM'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    feature = 'TYP_INT'\n",
    "    A = [\n",
    "        ['0', [11,10,3,]],\n",
    "        ['1', [1]],\n",
    "        ['2', [2]],\n",
    "        ['3', [7,4,6,5,]],\n",
    "        ['Unknown', [98,99]],        \n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Acc, data, feature, A)\n",
    "\n",
    "    \n",
    "    for feature in [\n",
    "        'YEAR',\n",
    "        'REGION',\n",
    "        'ALCOHOL', \n",
    "        'ALCHL_IM',\n",
    "        'MAX_SEV',\n",
    "        'MAXSEV_IM',\n",
    "        'RELJCT1',\n",
    "        'RELJCT1_IM',\n",
    "    ]:\n",
    "        data[feature] = df_Acc[feature]\n",
    "\n",
    "    A = data.columns.values.tolist()\n",
    "    B = df_Acc.columns.values.tolist()\n",
    "    C = [b for b in B if b not in A]\n",
    "    for c in C:\n",
    "        u = len(df_Acc[c].unique())\n",
    "        print (c, u)\n",
    "    \n",
    "    print ()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7594a",
   "metadata": {},
   "source": [
    "### Bin Vehicle Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Vehicle_Dataset(df_Veh):\n",
    "    print ('Build_Vehicle_Dataset()')\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # Reference\n",
    "    data['CASENUM'] = df_Veh['CASENUM']\n",
    "    data['VEH_NO'] = df_Veh['VEH_NO']\n",
    "    \n",
    "\n",
    "    feature = 'ACC_TYPE'\n",
    "    A = [\n",
    "        ['0', [61,60,51,50,53,59,52,55,58,6,54,1,10,14,16,5,2,7,8,4,0,62,3,9,89,69,41,64,66,87,90,91,]], #  21.7211 %\n",
    "        ['1', [83,34,35,88,68,65,86,30,82,38,73,39,]], #  20.5914 %\n",
    "        ['2', [98,25,22,11,31,77,12,40,85,24,26,32,71,81,79,29,27,43,]], #  22.3576 %\n",
    "        ['3', [21,33,42,48,75,72,80,15,78,28,76,44,45,84,49,]], #  22.3708 %\n",
    "        ['4', [20,67,23,74,47,70,46,93,13,92,63,36,37,]], #  12.9592 %\n",
    "        ['Unknown', [99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'BDYTYP_IM'\n",
    "    A = [\n",
    "        ['0', [86,87,82,89,81,83,84,80,88,85,90,95,11,97,96,58,12,45,32,91,10,2,59,3,30,]], #  7.1472 %\n",
    "        ['1', [4,]], #  36.3414 %\n",
    "        ['2', [1,19,42,5,8,16,6,52,]], #  10.832 %\n",
    "        ['3', [14,]], #  15.7266 %\n",
    "        ['4', [9,20,22,40,]], #  16.7815 %\n",
    "        ['5', [34,31,15,29,39,55,92,17,21,50,93,48,7,28,51,61,67,63,62,66,65,78,64,72,60,71,73,94,41,13,]], #  13.1715 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'BODY_TYP'\n",
    "    A = [\n",
    "        ['0', [86,87,82,89,81,83,80,84,88,85,90,95,11,97,96,58,45,12,32,91,10,2,3,59,1,30,]], #  7.8638 %\n",
    "        ['1', [4,]], #  36.434 %\n",
    "        ['2', [19,42,5,8,16,6,]], #  9.943 %\n",
    "        ['3', [14,]], #  15.5655 %\n",
    "        ['4', [52,9,20,22,40,]], #  16.9082 %\n",
    "        ['5', [34,31,15,29,39,55,92,17,21,50,93,48,28,7,51,61,67,63,62,66,65,78,64,72,60,71,73,94,41,13,]], #  13.2856 %\n",
    "        ['Unknown', [98, 99, 49, 79, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'BUS_USE'\n",
    "    A = [\n",
    "        ['0', [5,]], #  0.0207 %\n",
    "        ['1', [0,]], #  99.5385 %\n",
    "        ['2', [6,7,8,1,4,]], #  0.4407 %\n",
    "        ['Unknown', [98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'CARGO_BT' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [0,]], #  96.4126 %\n",
    "        ['1', [22,10,5,2,4,12,8,1,97,3,96,11,7,6,9,]], #  3.5873 %\n",
    "        ['Unknown', [98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    # More that 20% of samples missing\n",
    "    feature = 'DEFORMED' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [6,]], #  39.2768 %\n",
    "        ['1', [4,]], #  25.1503 %\n",
    "        ['2', [2,]], #  31.9667 %\n",
    "        ['3', [0,]], #  3.6062 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "#    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'DR_PRES' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [0,]], #  0.0216 %\n",
    "        ['1', [1,]], #  99.9784 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'EMER_USE'\n",
    "    A = [\n",
    "        ['0', [6,5,]], #  0.1932 %\n",
    "        ['1', [0,]], #  99.6845 %\n",
    "        ['2', [4,3,2,]], #  0.1222 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'FIRE_EXP' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [1,]], #  0.2076 %\n",
    "        ['1', [0,]], #  99.7924 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'HAZ_CNO'\n",
    "    A = [\n",
    "        ['0', [9,]], #  0.0011 %\n",
    "        ['1', [0,]], #  99.9774 %\n",
    "        ['2', [1,2,8,3,4,6,5,]], #  0.0215 %\n",
    "        ['Unknown', [88, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'HAZ_INV' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [1,]], #  99.9604 %\n",
    "        ['1', [2,]], #  0.0396 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'HAZ_PLAC' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [0,]], #  99.9654 %\n",
    "        ['1', [2,1,]], #  0.0345 %\n",
    "        ['Unknown', [8, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'HAZ_REL'\n",
    "    A = [\n",
    "        ['0', [2,]], #  0.0066 %\n",
    "        ['1', [0,]], #  99.9682 %\n",
    "        ['2', [1,]], #  0.0252 %\n",
    "        ['Unknown', [8, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'HIT_RUN' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [0,]], #  94.8247 %\n",
    "        ['1', [1,]], #  5.1753 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'IMPACT1'\n",
    "    A = [\n",
    "        ['0', [0,14,61,9,81,3,]], #  9.5695 %\n",
    "        ['1', [12,]], #  42.3692 %\n",
    "        ['2', [62,11,]], #  8.9371 %\n",
    "        ['3', [10,1,82,2,8,4,63,19,20,83,]], #  12.8232 %\n",
    "        ['4', [6,]], #  22.3991 %\n",
    "        ['5', [7,5,13,18,]], #  3.9018 %\n",
    "        ['Unknown', [98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'IMPACT1_IM'\n",
    "    A = [\n",
    "        ['0', [0,14,61,9,81,3,]], #  9.5846 %\n",
    "        ['1', [12,]], #  42.5618 %\n",
    "        ['2', [62,11,]], #  8.9862 %\n",
    "        ['3', [10,1,82,2,4,8,63,20,83,]], #  12.9432 %\n",
    "        ['4', [6,]], #  22.0047 %\n",
    "        ['5', [19,7,13,5,18,]], #  3.9193 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'J_KNIFE'\n",
    "    A = [\n",
    "        ['0', [2,]], #  0.0483 %\n",
    "        ['1', [0,]], #  97.4523 %\n",
    "        ['2', [3,1,]], #  2.4994 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'M_HARM'\n",
    "    A = [\n",
    "        ['0', [74,10,1,5,21,42,32,35,19,46,39,30,6,93,20,45,23,3,58,52,34,2,25,26,24,33,31,44,17,38,41,43,7,91,40,48,57,59,53,]], #  10.0639 %\n",
    "        ['1', [12,]], #  81.0472 %\n",
    "        ['2', [14,16,55,49,18,50,72,11,73,54,51,8,15,9,]], #  8.8889 %\n",
    "        ['Unknown', [98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'MAK_MOD'\n",
    "    A = [\n",
    "        ['0', [6010,76733,72704,71705,22001,43399,50031,2401,74706,30441,53702,71709,34705,99705,7017,20013,50709,99707,37733,37704,76709,20988,37702,53709,3884,12012,37709,73709,76703,98709,72709,98703,53705,98701,73704,76705,53706,58034,37706,98705,72706,73706,37703,76706,34709,50706,53734,73734,98706,73705,53704,76704,98704,37705,73703,53401,98702,50799,76701,99709,34706,77706,76702,71706,37739,37701,50705,76734,74705,53703,41401,42040,98733,99703,55032,98707,7470,9499,42053,69039,32054,38399,9037,99701,72705,93989,64031,73702,19006,76998,69038,6017,49055,7011,98734,20017,7004,55046,76739,22499,30032,35398,20038,20019,35053,74709,12008,94999,50399,12882,36398,31399,19027,12989,98907,42033,77709,12018,32047,21020,36399,41047,12998,13403,18402,14499,12015,52035,24008,69042,24002,54032,53999,59043,49441,22009,22023,19017,37734,24441,92989,12403,21017,98739,18003,22398,53036,2431,6398,21005,9019,10041,19019,23988,69398,69399,84998,45044,6444,42406,52040,7481,63035,67037,67399,65031,14037,49041,37035,48046,98982,13422,21002,35034,39036,54031,9038,14020,35056,52401,7898,20402,52471,52039,9442,55499,13482,52048,39032,20016,21441,21023,51051,6041,18441,39399,65399,53033,24003,20441,12013,20989,63498,55399,54499,13017,20471,41056,35404,63499,52399,12443,6052,20022,39035,2402,41044,20015,49398,38883,63403,19025,34703,63041,30399,13012,24001,19399,18401,55421,52999,54999,42031,20020,20004,18499,52403,35399,18026,55037,6044,7020,53034,6043,13002,6499,18399,12004,12032,49471,14443,20029,41402,67036,12424,20444,47036,42870,39038,34045,22032,7043,67033,59421,23471,34037,58499,25401,13401,47401,12399,63399,22403,36401,35050,37399,20443,35032,7444,20401,58043,41045,35471,20405,63037,22002,18025,20399,13005,63038,37033,22441,9020,62405,3499,52402,12016,12471,22016,37499,23401,35499,98398,14017,45040,14036,53499,7399,18019,63032,63033,35999,18002,63031,49399,52404,13999,20498,21021,14399,48044,14004,20406,20039,6018,35043,13399,42399,22018,14444,38401,18405,49050,62401,41499,35048,58038,12021,20009,41035,20403,53040,24006,55441,12006,7498,19021,38421,51040,34038,45037,12441,99739,23883,58398,55403,22010,53739,89881,73739,32421,20036,12035,49044,49048,24009,14038,55045,55036,7025,20027,42423,52046,12037,18022,51399,20023,35039,14401,55422,19003,55042,]], #  20.071 %\n",
    "        ['1', [49051,14006,32059,20032,18007,63999,24007,59399,38402,55035,41399,22999,92988,30999,13001,7021,53399,21003,51042,51499,51039,24499,20037,12442,49033,22019,6999,53035,20002,22020,22022,18020,18023,22399,54037,30051,12003,52037,53405,35403,6042,20010,20007,24399,39031,49405,19422,20025,18404,7029,41037,55999,49049,55404,2408,35047,12017,62424,23441,42499,6421,42405,6054,21399,63398,42045,94989,58399,35402,37031,34399,49032,62499,41055,12473,58037,98998,12402,20404,7404,84983,2405,19431,2499,7402,20028,14021,54038,23431,52034,24999,58999,63034,]], #  19.9417 %\n",
    "        ['2', [49999,52047,49043,30047,23399,12025,22401,20431,6442,42055,7471,38405,2482,32403,24011,22005,19401,18010,49034,55044,7039,51404,12425,42057,7024,51046,42043,7027,53404,7026,6051,34034,58042,30040,37039,35052,20445,12401,7472,58404,41049,37041,59033,2001,34999,34039,49040,23999,35051,41050,49056,6399,37032,55033,7461,24401,32999,12499,58032,12023,7442,54041,49499,35446,23402,19018,2407,49404,13015,23472,69054,20499,36038,59401,12027,63401,21401,7041,20999,20470,51034,99988,21022,3421,41471,24005,47035,59999,7499,19005,12422,54036,42042,37999,2404,]], #  20.9377 %\n",
    "        ['3', [37402,58036,55402,19024,63402,19026,14402,49481,67035,7403,41999,12999,12444,48499,67032,51047,2406,20482,23423,39034,49046,13402,59031,35049,12498,34048,51049,22008,48999,18018,14003,45401,49035,12981,69035,6441,13013,42403,53402,63036,12024,49401,32399,54399,37404,35422,42401,45399,19421,54035,42999,34404,37422,2483,20473,7028,48401,35401,34402,34403,55401,42048,51043,35443,51045,38471,12988,58039,30042,49402,23481,41051,20423,20421,36402,49403,3431,30046,35472,39401,59405,35473,52472,51401,54044,37401,37471,59035,19480,59034,49052,12022,42044,19020,20481,49038,63039,49442,49472,30402,37403,63441,48399,21001,21999,32044,63040,30499,2403,23421,67031,48034,35481,7422,23499,58035,19022,54421,19023,48421,]], #  19.0517 %\n",
    "        ['4', [2999,18999,49053,32499,20024,13014,49047,58422,32405,59032,59038,34035,55039,12481,38499,90981,34401,7999,39037,62423,41403,20461,54039,20398,13421,48038,12421,32052,49045,34049,32042,59403,37441,18421,19499,67034,6055,32048,24398,20001,7018,6014,19423,23422,30403,35421,51999,7482,7463,82461,42050,52499,37421,18024,29005,42047,37037,98983,55041,41053,58033,99998,62425,34047,32043,14999,58041,59499,42051,63422,34421,41441,69040,59402,23898,55398,7462,6016,82983,20026,49422,23461,41054,41052,48045,58403,82981,59040,31037,2422,49482,12870,6443,32049,51050,53041,54402,58044,23008,19014,42421,32045,22024,90982,39999,92983,12398,51041,58047,32422,12461,48047,34036,38882,62421,12423,59404,55040,12462,30052,34042,32051,48403,7443,20034,42461,20870,34499,32402,18398,51048,42039,59037,20021,58421,32401,20422,42054,84988,20880,3402,45421,19999,23880,53032,7019,51402,37038,31401,30443,55038,13016,13499,41421,30036,42404,98809,98988,63421,42398,37405,18004,58040,23498,20850,55043,98999,58402,24010,53481,98898,45042,84981,30421,49421,98884,45031,86881,38472,84999,86882,38999,58045,20881,35055,48402,58401,7880,12470,2498,82884,99989,34043,12898,94982,34044,82989,23882,38884,85881,45499,42058,12881,51881,84884,98850,82881,82870,85884,87884,84881,52882,39039,23881,35461,12880,20898,20040,98806,98808,37398,47399,98804,82999,51884,32040,94983,87881,12850,23870,62404,90989,86898,20882,19481,89890,20884,82898,59036,86884,84898,7881,99898,99399,97997,12884,99499,99999,99884,90988,98881,51898,85898,38898,23989,9017,98498,90983,29398,23981,23884,41398,48031,51882,54398,19398,7398,99881,73732,30398,87890,98890,98981,7005,13398,42036,82988,89882,42034,55034,34046,62422,69031,99890,30442,82982,23470,39398,82890,41498,32041,3482,47034,98882,92982,42402,25499,41043,22025,32050,14015,82462,84989,82883,84481,84982,94461,25999,45041,9999,51998,82498,42046,82850,7421,98908,7042,9002,18005,7884,51044,22017,21398,25441,2421,23890,20472,52498,93988,23466,38498,20890,62403,35898,23850,13481,42038,20466,99850,85999,86999,98805,87999,33033,48043,29399,38473,9034,14039,3441,35441,36039,21499,87898,3999,7001,46039,12036,84890,12007,98598,7007,99870,41046,12033,20442,7013,25890,7870,35042,94981,34398,86890,99981,84883,38890,64033,85890,9399,42422,47398,98902,9001,38403,84850,20035,7015,35044,41034,53403,20981,7850,29001,54401,25498,54033,35038,82499,10037,51053,32055,20008,84498,1399,48398,48037,51890,82882,14031,49042,87882,43032,59039,36999,54034,9008,49498,45398,18001,62402,32404,51398,35498,42462,52898,18498,85883,98731,20883,47031,47037,98870,7002,42041,53701,45999,51989,82998,32046,38404,64032,54040,30048,32398,47999,67398,99599,42850,42056,10034,51403,25884,1001,34405,38881,62426,98904,99982,86883,89898,53733,20407,42037,7033,53398,32056,51988,93981,30033,30043,85882,34422,32058,1008,38988,51052,38441,12890,53031,98883,92981,30053,31036,21018,31999,49054,87883,13423,39499,31422,93983,89884,10045,32406,59042,39402,38474,30034,45045,32060,49057,55406,14398,42898,19009,84421,62427,48498,72707,18021,]], #  19.9996 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'MAKE'\n",
    "    A = [\n",
    "        ['0', [74,76,71,72,50,73,77,43,53,98,64,65,21,9,52,22,14,18,92,24,37,39,63,]], #  28.1315 %\n",
    "        ['1', [35,6,36,55,67,]], #  6.162 %\n",
    "        ['2', [20,]], #  12.6429 %\n",
    "        ['3', [13,69,34,]], #  2.0354 %\n",
    "        ['4', [49,]], #  11.7578 %\n",
    "        ['5', [30,]], #  1.5107 %\n",
    "        ['6', [12,]], #  13.4181 %\n",
    "        ['7', [19,2,]], #  5.6145 %\n",
    "        ['8', [41,58,7,42,54,47,93,23,59,25,48,38,62,3,32,29,51,31,45,90,10,94,86,89,84,85,82,87,97,33,46,1,]], #  18.727 %\n",
    "        ['Unknown', [99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'MAX_VSEV' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [3,5,6,4,2,]], #  17.1638 %\n",
    "        ['1', [1,]], #  17.3546 %\n",
    "        ['2', [0,]], #  65.4815 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Replace these features with an \"VEH_AGE\" feature \n",
    "        that will tell how old the vehicle is.  \n",
    "    Such a feature will require the \"YEAR\" feature from ACCIDENT, \n",
    "        so we will first need to merge the datasets.\n",
    "    \n",
    "    feature = 'MDLYR_IM'\n",
    "    A = [\n",
    "        ['0', [1929,1947,1962,1968,1951,1956,1974,1982,1978,1955,1953,1960,1959,1950,1970,1986,1975,1985,1981,1966,1965,1987,1983,1973,1931,1977,1979,1984,1964,1976,1991,1980,1971,1988,1993,1992,1990,1994,1998,1997,1995,1989,1996,2002,1999,2001,2000,2005,]], #  22.8558 %\n",
    "        ['1', [2003,2020,2004,2019,2006,]], #  17.6131 %\n",
    "        ['2', [2007,1969,1940,2016,2017,2018,]], #  21.4618 %\n",
    "        ['3', [2009,2008,2015,]], #  18.3831 %\n",
    "        ['4', [2013,2014,2021,2012,1967,2011,2010,1957,1972,1948,1952,1928,1932,1933,1963,1954,1958,1934,1961,]], #  19.6865 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'MOD_YEAR'\n",
    "    A = [\n",
    "        ['0', [1929,1947,1962,1968,1951,1956,1974,1982,1978,1955,1953,1960,1959,1950,1970,1986,1975,1985,1981,1966,1965,1987,1983,1973,1931,1977,1979,1984,1964,1976,1991,1980,1971,1988,1993,1992,1990,1994,1998,1995,1997,1996,1989,2002,2001,1999,2000,2005,]], #  22.8303 %\n",
    "        ['1', [2003,2004,2020,2006,2019,]], #  17.5677 %\n",
    "        ['2', [2007,2009,1969,1940,2016,2008,]], #  22.0677 %\n",
    "        ['3', [2017,2018,2015,]], #  18.0075 %\n",
    "        ['4', [2013,2014,2021,2012,2011,2010,1967,1957,1972,1948,1952,1928,1932,1933,1963,1954,1958,1934,1961,]], #  19.5268 %\n",
    "        ['Unknown', [9998, 9999, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "    \"\"\"\n",
    "    feature = 'MDLYR_IM'\n",
    "    data[feature] = df_Veh[feature]\n",
    "    feature = 'MOD_YEAR'\n",
    "    data[feature] = df_Veh[feature]\n",
    "\n",
    "    feature = 'MODEL'\n",
    "    A = [\n",
    "        ['0', [709,703,701,706,704,705,702,707,799,733,734,907,739,12,56,11,16,4,19,471,424,9,22,29,50,6,3,20,2,21,59,37,18,7,43,13,]], #  20.8933 %\n",
    "        ['1', [399,52,17,444,25,36,1,38,998,406,408,15,39,47,31,35,32,405,48,]], #  19.3475 %\n",
    "        ['2', [431,33,27,989,23,445,34,40,404,44,425,446,988,26,51,401,407,28,]], #  22.7449 %\n",
    "        ['3', [402,42,49,46,443,442,24,499,473,57,54,483,403,441,472,423,41,5,55,480,398,498,]], #  21.1888 %\n",
    "        ['4', [481,421,45,422,53,470,14,482,463,461,10,8,999,983,982,870,462,981,883,809,882,880,58,881,806,808,804,884,898,850,997,890,732,908,466,805,598,902,731,599,426,904,474,60,427,]], #  15.8254 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'MXVSEV_IM' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [3,5,4,2,6,]], #  17.1044 %\n",
    "        ['1', [1,]], #  17.4622 %\n",
    "        ['2', [0,]], #  65.4333 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'NUM_INJV' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [26,8,11,7,5,6,4,9,3,2,]], #  10.333 %\n",
    "        ['1', [1,]], #  24.1729 %\n",
    "        ['2', [10,14,12,]], #  0.0098 %\n",
    "        ['3', [0,]], #  65.4842 %\n",
    "        ['Unknown', [99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'NUMINJ_IM' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [26,8,11,7,6,5,4,9,3,2,]], #  10.2074 %\n",
    "        ['1', [1,]], #  24.3463 %\n",
    "        ['2', [10,14,12,]], #  0.0095 %\n",
    "        ['3', [0,]], #  65.4369 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'NUMOCCS'\n",
    "    A = [\n",
    "        ['0', [59,26,35,31,37,10,14,33,13,8,20,27,11,]], #  0.2623 %\n",
    "        ['1', [2,]], #  24.6466 %\n",
    "        ['2', [6,7,21,]], #  1.1684 %\n",
    "        ['3', [1,]], #  54.7647 %\n",
    "        ['4', [12,9,]], #  0.0809 %\n",
    "        ['5', [3,]], #  10.4213 %\n",
    "        ['6', [38,5,17,4,19,16,34,25,28,24,43,49,23,15,29,22,18,40,32,55,53,50,44,51,30,39,41,75,47,95,52,54,62,60,56,58,46,65,57,48,36,45,77,]], #  8.6562 %\n",
    "        ['Unknown', [99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'P_CRASH1' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [14,0,7,3,17,6,]], #  6.8626 %\n",
    "        ['1', [1,]], #  50.4172 %\n",
    "        ['2', [11,]], #  10.3122 %\n",
    "        ['3', [12,98,2,16,]], #  6.6395 %\n",
    "        ['4', [5,]], #  15.4783 %\n",
    "        ['5', [15,4,10,8,9,13,]], #  10.2902 %\n",
    "        ['Unknown', [99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'P_CRASH2'\n",
    "    A = [\n",
    "        ['0', [8,9,12,13,14,54,1,4,6,5,62,2,66,3,55,]], #  25.2772 %\n",
    "        ['1', [17,67,72,63,68,91,64,78,15,]], #  14.9584 %\n",
    "        ['2', [10,71,98,19,18,92,21,65,90,51,70,59,]], #  7.7214 %\n",
    "        ['3', [53,]], #  19.6503 %\n",
    "        ['4', [73,60,74,87,61,89,52,]], #  15.3052 %\n",
    "        ['5', [11,88,16,50,56,20,80,82,81,84,85,83,]], #  17.0873 %\n",
    "        ['Unknown', [99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "    # More that 20% of samples missing\n",
    "    feature = 'P_CRASH3' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [15,98,6,7,0,11,10,8,9,12,5,]], #  15.3542 %\n",
    "        ['1', [16,]], #  13.4695 %\n",
    "        ['2', [1,]], #  71.1762 %\n",
    "        ['Unknown', [99, ]]\n",
    "    ]\n",
    "#    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'PCRASH1_IM'\n",
    "    A = [\n",
    "        ['0', [14,0,7,3,17,6,]], #  6.8303 %\n",
    "        ['1', [1,]], #  50.6172 %\n",
    "        ['2', [11,]], #  10.2666 %\n",
    "        ['3', [12,98,2,16,]], #  6.6712 %\n",
    "        ['4', [5,]], #  15.2315 %\n",
    "        ['5', [15,4,10,8,9,13,]], #  10.3832 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'PCRASH4' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [7,3,4,5,2,0,]], #  3.8805 %\n",
    "        ['1', [1,]], #  96.1194 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'PCRASH5' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [6,]], #  0.1972 %\n",
    "        ['1', [4,]], #  10.8203 %\n",
    "        ['2', [0,3,5,2]], #  10.7314 %\n",
    "        ['3', [1,]], #  76.6633 %\n",
    "        ['4', [7,]], #  1.5877 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'ROLINLOC' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [7,3,6,1,5,4,2,]], #  2.9048 %\n",
    "        ['1', [0,]], #  97.0952 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    # This feature is nearly the same as the next, \n",
    "    # as we have reduced them to \"Rollover\" and \"No Rollover.\"\n",
    "    # Do not add to dataset.\n",
    "#    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'ROLLOVER' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [1,9,2,]], #  2.951 %\n",
    "        ['1', [0,]], #  97.0489 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'SPEC_USE'\n",
    "    A = [\n",
    "        ['0', [19,4,10,1,5,]], #  0.5028 %\n",
    "        ['1', [0,]], #  98.7952 %\n",
    "        ['2', [3,20,8,6,21,13,2,22,7,23,12,11,]], #  0.7021 %\n",
    "        ['Unknown', [98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'SPEEDREL' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [3,2,5,8,4,]], #  6.1165 %\n",
    "        ['1', [0,]], #  93.8834 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'TOW_VEH' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [3,0,]], #  97.4427 %\n",
    "        ['1', [6,5,1,2,4,]], #  2.5551 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'TOWED' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [2,]], #  33.0112 %\n",
    "        ['1', [7,]], #  8.3598 %\n",
    "        ['2', [5,]], #  58.629 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "    # More that 20% of samples missing\n",
    "    \"\"\"\n",
    "    feature = 'TRAV_SP'\n",
    "    data[feature] = pd.cut(\n",
    "        df_Veh[feature], \n",
    "        [-1,9,19,29,39,49,59,69,79,997,999], \n",
    "#        labels=['0-9','10-19','20-29','30-39', '40-49', '50-59',\n",
    "#                '60-69', '70-79', '80+', 'Unknown'\n",
    "#               ]\n",
    "        labels = [0,1,2,3,4,5,6,7,8,'Unknown']\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Crosstabs = pd.crosstab(df_Veh[feature], data[feature])\n",
    "    print ()\n",
    "    display (Crosstabs)\n",
    "    \n",
    "    print (data[feature].value_counts())\n",
    "    \"\"\"\n",
    "\n",
    "    feature = 'V_ALCH_IM' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [1,]], #  3.044 %\n",
    "        ['1', [2,]], #  96.956 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'VALIGN'\n",
    "    A = [\n",
    "        ['0', [3,2,4,]], #  8.3849 %\n",
    "        ['1', [1,]], #  88.9094 %\n",
    "        ['2', [0,]], #  2.7058 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'VEH_ALCH' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [1,8,]], #  3.0238 %\n",
    "        ['1', [2,]], #  96.9762 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'VEVENT_IM'\n",
    "    A = [\n",
    "        ['0', [74,10,1,5,21,42,32,19,35,46,39,30,4,93,20,3,45,23,58,52,2,34,6,25,26,24,33,31,44,38,17,41,43,7,91,40,48,57,59,53,]], #  10.0888 %\n",
    "        ['1', [12,]], #  81.0178 %\n",
    "        ['2', [49,16,14,55,18,50,72,11,73,54,51,8,15,9,]], #  8.8932 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    # More that 20% of samples missing\n",
    "    feature = 'VNUM_LAN'\n",
    "    A = [\n",
    "        ['0', [2,]], #  44.9531 %\n",
    "        ['1', [4,]], #  14.8614 %\n",
    "        ['2', [3,]], #  18.2764 %\n",
    "        ['3', [5,]], #  10.9793 %\n",
    "        ['4', [7,1,6,0,]], #  10.9298 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "#    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'VPROFILE'\n",
    "    A = [\n",
    "        ['0', [6,5,4,3,]], #  5.9771 %\n",
    "        ['1', [1,]], #  83.0272 %\n",
    "        ['2', [2,0,]], #  10.9956 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'VSPD_LIM' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [90,55,]], #  10.9312 %\n",
    "        ['1', [80,75,70,50,65,]], #  14.7318 %\n",
    "        ['2', [45,]], #  21.5203 %\n",
    "        ['3', [60,]], #  1.9729 %\n",
    "        ['4', [40,]], #  11.2788 %\n",
    "        ['5', [35,]], #  19.1915 %\n",
    "        ['6', []], #  9.1917 %\n",
    "        ['7', [25,30,20,15,0,10,5,]], #  11.1816 %\n",
    "        ['Unknown', [98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "    # I don't think VSURCOND is knowable, except as it's the same as WEATHER.\n",
    "    feature = 'VSURCOND' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [5,11,7,8,6,]], #  0.2729 %\n",
    "        ['1', [1,]], #  81.6571 %\n",
    "        ['2', [2,]], #  13.4651 %\n",
    "        ['3', [10,4,3,0,]], #  4.6048 %\n",
    "        ['Unknown', [98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'VTCONT_F'\n",
    "    A = [\n",
    "        ['0', [1,]], #  0.1033 %\n",
    "        ['1', [0,]], #  62.2729 %\n",
    "        ['2', [4,]], #  0.006 %\n",
    "        ['3', [3,]], #  37.5464 %\n",
    "        ['4', [2,]], #  0.0713 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'VTRAFCON' # Modified by hand\n",
    "    A = [\n",
    "        ['0', [29,40,28,9,4,98,65,]], #  2.0442 %\n",
    "        ['1', [0,]], #  62.2529 %\n",
    "        ['2', [3,]], #  24.9677 %\n",
    "        ['3', [1,20,7,8,23,50,2,21,]], #  10.7352 %\n",
    "        ['Unknown', [97, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'VTRAFWAY'\n",
    "    A = [\n",
    "        ['0', [1,]], #  45.1904 %\n",
    "        ['1', [2,]], #  17.174 %\n",
    "        ['2', [5,]], #  5.757 %\n",
    "        ['3', [3,]], #  23.3378 %\n",
    "        ['4', [6,4,0,]], #  8.541 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Veh, data, feature, A)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59293638",
   "metadata": {},
   "source": [
    "### Bin Person Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Person_Dataset(df_Per):\n",
    "    print ('Build_Person_Dataset()')\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # Reference\n",
    "    data['CASENUM'] = df_Per['CASENUM']\n",
    "    data['VEH_NO'] = df_Per['VEH_NO']\n",
    "  \n",
    "    feature = 'AGE' # Built by hand from data in Correlation_Ordered_AGE_IM.tex\n",
    "    A = [\n",
    "        ['0', [*range(0,15)]],\n",
    "        ['1', [*range(15,19)]],\n",
    "        ['2', [*range(19,53)]],\n",
    "        ['3', [*range(53,74)]],\n",
    "        ['4', [*range(74,150)]],\n",
    "        ['Unknown', [998,999]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "    feature = 'AGE_IM'\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "    feature = 'AIR_BAG' # Modified:  What does 28 signify?\n",
    "    A = [\n",
    "        ['0', [8,1,3,9,7,2,0,]], #  19.879 %\n",
    "        ['1', [20,]], #  80.1172 %\n",
    "        ['Unknown', [28, 98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'ALC_RES' # Done by hand.  Basically, \"Untested\" and \"Tested\"    \n",
    "    A = [\n",
    "        ['0', [0, 996,997,998]], \n",
    "        ['1', [*range(1,945,1)]], \n",
    "        ['Unknown', [995, 999, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'ALC_STATUS'\n",
    "    A = [\n",
    "        ['0', [2,1,]], #  1.9717 %\n",
    "        ['1', [0,]], #  98.0283 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    # More that 20% of samples missing\n",
    "    feature = 'DRINKING'\n",
    "    A = [\n",
    "        ['0', [1,]], #  3.0974 %\n",
    "        ['1', [0,]], #  96.9026 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "#    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    # More that 20% of samples missing\n",
    "    feature = 'DRUGS'\n",
    "    A = [\n",
    "        ['0', [1,]], #  1.1139 %\n",
    "        ['1', [0,]], #  98.8861 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "#    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'EJECTION'\n",
    "    A = [\n",
    "        ['0', [1,3,8,2,]], #  3.2104 %\n",
    "        ['1', [0,]], #  96.7896 %\n",
    "        ['Unknown', [7, 9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'EJECT_IM'\n",
    "    A = [\n",
    "        ['0', [1,3,8,2,]], #  3.055 %\n",
    "        ['1', [0,]], #  96.9451 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'HOSPITAL'\n",
    "    A = [\n",
    "        ['0', [0]],\n",
    "        ['1', [1,2,3,4,5,6,]],\n",
    "        ['Unknown', [8,9]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "    feature = 'INJSEV_IM'\n",
    "    A = [\n",
    "        ['0', [3,5,2,6,4,]], #  13.7916 %\n",
    "        ['1', [1,]], #  14.6972 %\n",
    "        ['2', [0,]], #  71.5113 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'INJ_SEV'\n",
    "    A = [\n",
    "        ['0', [3,5,2,6,4,]], #  13.8659 %\n",
    "        ['1', [1,]], #  14.5494 %\n",
    "        ['3', [0,]], #  71.5847 %\n",
    "        ['Unknown', [9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'LOCATION'\n",
    "    # Take out because it only applies to pedestrians and such.\n",
    "\n",
    "    feature = 'PERALCH_IM'\n",
    "    A = [\n",
    "        ['0', [1,]], #  2.4886 %\n",
    "        ['1', [0,]], #  97.5114 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'PER_NO'\n",
    "    data[feature] = df_Per[feature]\n",
    "    # Irrelevant, except for sorting\n",
    "\n",
    "    feature = 'PER_TYP'\n",
    "    A = [\n",
    "        ['0', [9,]], #  0.0203 %\n",
    "        ['1', [2,]], #  26.1757 %\n",
    "        ['2', [1,]], #  73.4507 %\n",
    "        ['3', [3,]], #  0.3533 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'REST_MIS'\n",
    "    A = [\n",
    "        ['0', [7,1,]], #  7.2463 %\n",
    "        ['1', [0,]], #  92.7536 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'REST_USE'\n",
    "    A = [\n",
    "        ['0', [16,5,17,19,20,7,6,29,0,2,1,97,]], #  7.5268 %\n",
    "        ['1', [3,]], #  85.8489 %\n",
    "        ['2', [12,8,11,10,4,]], #  6.6243 %\n",
    "        ['Unknown', [98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'SEAT_IM'\n",
    "    A = [\n",
    "        ['0', [56,52,55,51,54,53,38,18,28,12,21,]], #  4.862 %\n",
    "        ['1', [13,]], #  14.5985 %\n",
    "        ['2', [11,]], #  73.6819 %\n",
    "        ['3', [23,22,33,32,42,31,50,43,41,48,]], #  6.8577 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'SEAT_POS'\n",
    "    A = [\n",
    "        ['0', [56,52,55,51,54,53,38,18,28,12,29,49,]], #  0.7622 %\n",
    "        ['1', [13,]], #  13.8574 %\n",
    "        ['2', [21,19,39,]], #  4.1646 %\n",
    "        ['3', [11,]], #  74.6203 %\n",
    "        ['4', [23,22,33,42,32,50,31,43,41,48,]], #  6.5957 %\n",
    "        ['Unknown', [98, 99, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'SEX'\n",
    "    A = [\n",
    "        ['0', [2,]], #  45.5467 %\n",
    "        ['1', [1,]], #  54.4533 %\n",
    "        ['Unknown', [8, 9, ]]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'SEX_IM'\n",
    "    A = [\n",
    "        ['0', [2,]], #  45.5047 %\n",
    "        ['1', [1,]], #  54.4953 %\n",
    "        ['Unknown', []]\n",
    "    ]\n",
    "    data = Build_Individual_Feature_with_Dict(df_Per, data, feature, A)\n",
    "\n",
    "\n",
    "    feature = 'VEH_NO'\n",
    "    data[feature] = df_Per[feature]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8142873",
   "metadata": {},
   "source": [
    "## Merge Accident, Vehicle, and Person Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05306685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(df_Acc, df_Veh, df_Per):\n",
    "    print ('Merge()')\n",
    "    print ()\n",
    "\n",
    "    data = pd.merge(\n",
    "        df_Acc, df_Veh, \n",
    "        on=['CASENUM'],\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "    \n",
    "    print ('df_Acc.shape')\n",
    "    print (df_Acc.shape)\n",
    "    print ('df_Veh.shape')\n",
    "    print (df_Veh.shape)\n",
    "    print ('data.shape')\n",
    "    print (data.shape)\n",
    "    print ()\n",
    "\n",
    "    \n",
    "    data = pd.merge(\n",
    "        data, df_Per, \n",
    "        on=['CASENUM', 'VEH_NO'],\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "    \n",
    "    print ('df_Acc.shape')\n",
    "    print (df_Acc.shape)\n",
    "    print ('df_Veh.shape')\n",
    "    print (df_Veh.shape)\n",
    "    print ('df_Per.shape')\n",
    "    print (df_Per.shape)\n",
    "    print ('data.shape')\n",
    "    print (data.shape)\n",
    "    print ()\n",
    "\n",
    "\n",
    "    print (data.head())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50547523",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06756c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering(data):\n",
    "    print ('Feature_Engineering')\n",
    "    data['VEH_AGE'] = data['YEAR'] - data['MOD_YEAR'] + 1\n",
    "    data['VEH_AGE'] = data['VEH_AGE']/5\n",
    "    data['VEH_AGE'] = data['VEH_AGE'].apply(np.floor)\n",
    "    data['VEH_AGE'][data['VEH_AGE'] >= 4] = 4\n",
    "    data.loc[data['MOD_YEAR'].isin([9998,9999]), 'VEH_AGE'] = 'Unknown'\n",
    "#    display(pd.crosstab(data.VEH_AGE, data.MOD_YEAR))\n",
    "#    print (data['MOD_YEAR'].value_counts())\n",
    "#    print ()\n",
    "#    print (data['VEH_AGE'].value_counts())\n",
    "#    print ()\n",
    "    data['VEH_AGE_IM'] = data['YEAR'] - data['MDLYR_IM'] + 1\n",
    "    data['VEH_AGE_IM'][data['VEH_AGE_IM'] >= 5] = 5\n",
    "    data.drop(columns = ['MOD_YEAR', 'MDLYR_IM'], inplace=True)\n",
    "    \n",
    "    print ()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5af9a",
   "metadata": {},
   "source": [
    "## Run:  Get Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75392b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Preprocess_Data():\n",
    "    print ('Preprocess_Data()')\n",
    "#    df_Acc, df_Veh, df_Per = Get_Data_from_Original()\n",
    "    df_Acc, df_Veh, df_Per = Get_Data_from_Temp_Files()\n",
    "    df_Acc, df_Veh, df_Per = Drop_Repeated_Features(df_Acc, df_Veh, df_Per)    \n",
    "    df_Acc, df_Veh, df_Per = Drop_Irrelevant_Features (df_Acc, df_Veh, df_Per)\n",
    "\n",
    "    Analyze_Binning(df_Acc)\n",
    "    df_Acc = Build_Accident_Dataset(df_Acc)\n",
    "    Analyze_Binning(df_Acc)\n",
    "\n",
    "    Analyze_Binning(df_Veh)\n",
    "    df_Veh = Build_Vehicle_Dataset(df_Veh)\n",
    "    Analyze_Binning(df_Veh)\n",
    "    \n",
    "    Analyze_Binning(df_Per)\n",
    "    df_Per = Build_Person_Dataset(df_Per)\n",
    "    Analyze_Binning(df_Per)\n",
    "    \n",
    "    data = Merge (df_Acc, df_Veh, df_Per)\n",
    "    \n",
    "    data = Feature_Engineering (data)\n",
    "    \n",
    "    data.to_csv('../../Big_Files/CRSS_Discretized_All_12_22_22.csv', index=False)\n",
    "    print ('Finished Preprocess_Data()')\n",
    "\n",
    "Preprocess_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bface0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c9b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86570e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec6303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
