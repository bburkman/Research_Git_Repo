{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767154e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e1d2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7d47d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b4fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install Packages\n",
      "Python version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:26:08) [Clang 14.0.6 ]\n",
      "NumPy version: 1.24.2\n",
      "SciPy version:  1.7.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bburkman/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.11.0\n",
      "Keras version:  2.11.0\n",
      "Pandas version:  1.5.3\n",
      "SciKit-Learn version: 1.2.2\n",
      "Imbalanced-Learn version: 0.10.1\n",
      "Finished Installing Packages\n"
     ]
    }
   ],
   "source": [
    "print ('Install Packages')\n",
    "\n",
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "#from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import scipy as sc\n",
    "print ('SciPy version:  {}'.format(sc.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print ('TensorFlow version:  {}'.format(tf.__version__))\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "from tensorflow import keras\n",
    "print ('Keras version:  {}'.format(keras.__version__))\n",
    "\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "from keras.layers import IntegerLookup\n",
    "from keras.layers import Normalization\n",
    "from keras.layers import StringLookup\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.utils import tf_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "#    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Library for reading Microsoft Access files\n",
    "#import pandas_access as mdb\n",
    "\n",
    "import sklearn\n",
    "print ('SciKit-Learn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import imblearn\n",
    "print ('Imbalanced-Learn version: {}'.format(imblearn.__version__))\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "#!pip install pydot\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "#np.random.seed(42) # NumPy\n",
    "#random.seed(42) # Python\n",
    "#tf.random.set_seed(42) # Tensorflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Installing Packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437f109",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919fb2db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Get_Data():\n",
    "    print ('Get_Data()')\n",
    "    data = pd.read_csv(\n",
    "        '../../Big_Files/CRSS_Imputed_All_05_19_23.csv',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Get_Data()')\n",
    "    print ()\n",
    "    return data\n",
    "\n",
    "def Test_Get_Data():\n",
    "    data = Get_Data()\n",
    "    display (data.head())\n",
    "    \n",
    "Test_Get_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d540640",
   "metadata": {},
   "source": [
    "# Remove_Pedestrian_Crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Pedestrian_Crashes(data):\n",
    "    print ('Remove_Pedestrian_Crashes()')\n",
    "    display(data.PEDS.value_counts())\n",
    "    n = len(data[data.PEDS>0])\n",
    "    print ('Removing %d crashes that involve a pedestrian.' % n)\n",
    "    data = data[data.PEDS==0]\n",
    "    return data\n",
    "\n",
    "def Test_Remove_Pedestrian_Crashes():\n",
    "    data = Get_Data()\n",
    "    print (len(data))\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    print (len(data))\n",
    "\n",
    "Test_Remove_Pedestrian_Crashes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb9dce",
   "metadata": {},
   "source": [
    "## Engineer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering_Cross_Two(data):\n",
    "    print ('Feature_Engineering_Cross_Two')\n",
    "    Pairs = [\n",
    "        ['AGE', 'SEX', 'AGE_x_SEX'],\n",
    "        ['AGE', 'SCH_BUS', 'AGE_x_SCH_BUS']\n",
    "    ]\n",
    "    for P in Pairs:\n",
    "        data[P[2]] = data[P[0]].map(str) + '_x_' + data[P[1]].map(str)\n",
    "    \n",
    "    print ()\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b900000",
   "metadata": {},
   "source": [
    "## Thin Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_Features(data):\n",
    "    print ('Thin_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "        'PERMVIT',\n",
    "        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "        'RELJCT2',\n",
    "        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "        'VE_FORMS',\n",
    "        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "        'BODY_TYP',\n",
    "        'BUS_USE',\n",
    "        'EMER_USE',\n",
    "        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "        'MODEL',\n",
    "        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "        'LOCATION',\n",
    "        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "        'VEH_AGE',\n",
    "        'AGE_x_SEX',\n",
    "        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "Test_Thin_Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e759eda",
   "metadata": {},
   "source": [
    "## Really Thin Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4202cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Really_Thin_Features(data):\n",
    "    print ('Really_Thin_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "        'AGE_x_SEX',\n",
    "#        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Really_Thin_Features():\n",
    "    data = Get_Data()\n",
    "    data = Really_Thin_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "Test_Really_Thin_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11264ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_to_Minimal_Features(data):\n",
    "    print ('Thin_to_Minimal_Features()')\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "#        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "#        'REL_ROAD',\n",
    "#        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "#        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "#        'VALIGN',\n",
    "#        'VNUM_LAN',\n",
    "#        'VPROFILE',\n",
    "#        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "#        'VTRAFCON',\n",
    "#        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "#        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "#        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "#        'AGE_x_SEX',\n",
    "#        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_to_Minimal_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_to_Minimal_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "Test_Thin_to_Minimal_Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52366e1a",
   "metadata": {},
   "source": [
    "## Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cefa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Dummies(data, target):\n",
    "    print ('Get_Dummies')\n",
    "    data = data.astype('category')\n",
    "    Target = data.pop(target)\n",
    "    data_Dummies = pd.get_dummies(data, prefix = data.columns)\n",
    "    data_Dummies = data_Dummies.join(Target)\n",
    "#    for feature in data_Dummies:\n",
    "#        print (feature)\n",
    "    print ()\n",
    "\n",
    "    return data_Dummies\n",
    "\n",
    "def Test_Get_Dummies():\n",
    "    print ('Test_Get_Dummies')\n",
    "    A = pd.DataFrame({\n",
    "        'A': ['a', 'b', 'a'], \n",
    "        'B': ['b', 'a', 'c'], \n",
    "        'C': [1, 2, 3]})\n",
    "    C = Get_Dummies(A, 'C')\n",
    "    display(C)\n",
    "    print ()\n",
    "\n",
    "Test_Get_Dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d43d6e",
   "metadata": {},
   "source": [
    "## Test-Train Split\n",
    "- We're using sklearn's train_test_split rather than Pandas's sample because the former has a 'stratify' option that will put the same proportion of HOSPITAL==1 into each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Data(data, target, test_size):\n",
    "    print ('Split_Data()')\n",
    "    X = data.drop(columns=[target])\n",
    "    y = data[target]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=test_size, \n",
    "        #random_state=42\n",
    "    )\n",
    "    \n",
    "    a = y_train[y_train==1].shape[0]\n",
    "    b = y_test[y_test==1].shape[0]\n",
    "    print (\n",
    "        x_train.shape, \n",
    "        y_train.shape, a, round((a/(a+b)*100),2), '%')\n",
    "    print (\n",
    "        x_test.shape, \n",
    "        y_test.shape, b, round((b/(a+b)*100),2), '%'\n",
    "    )\n",
    "    print ()\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b22f0",
   "metadata": {},
   "source": [
    "# Imbalanced Data Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984f24a",
   "metadata": {},
   "source": [
    "## Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a61c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tomek_Links(X_train, y_train):\n",
    "    print ('Tomek_Links()')\n",
    "    M = len(y_train)\n",
    "    N = len(y_train)\n",
    "    n = len(y_train[y_train==1])\n",
    "    p = (N-n)/n\n",
    "    print ('Before Tomek Links:')\n",
    "    print ('%d samples, %d hospitalized, %d not hospitalized' % (N, n, N-n))\n",
    "    print ('%f percent of samples hospitalized' % (n/N*100))\n",
    "    print ('There are %f negative samples for each positive.' % ((N-n)/n))\n",
    "    print ()\n",
    "\n",
    "    X_train, y_train = TomekLinks().fit_resample(X_train, y_train)\n",
    "    N = len(y_train)\n",
    "    n = len(y_train[y_train==1])\n",
    "    p = (N-n)/n\n",
    "    print ('After Tomek Links:')\n",
    "    print ('%d samples, %d hospitalized, %d not hospitalized' % (N, n, N-n))\n",
    "    print ('%f percent of samples hospitalized' % (n/N*100))\n",
    "    print ('There are %f negative samples for each positive.' % ((N-n)/n))\n",
    "    print ('Removed %d samples, or %.2f%% of the set.' % (M-N, (M-N)/M*100))\n",
    "    print ()\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce9a6e",
   "metadata": {},
   "source": [
    "## Condensed Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Condensed_Nearest_Neighbour(X_train, y_train):\n",
    "    print ('Condensed_Nearest_Neighbour()')\n",
    "    N = X_train.shape[0]\n",
    "    print ('X_train.shape before = ', X_train.shape)\n",
    "    print ('y_train.shape before = ', y_train.shape)\n",
    "    print ()\n",
    "    cnn = CondensedNearestNeighbour(n_neighbors=None)\n",
    "    X_train, y_train = cnn.fit_resample(X_train, y_train)\n",
    "    n = X_train.shape[0]\n",
    "    print ('X_train.shape after = ', X_train.shape)\n",
    "    print ('y_train.shape after = ', y_train.shape)\n",
    "    print ()\n",
    "    print ('Removed %d samples, or %.2f%% of the set.' % (N-n, (N-n)/N*100))\n",
    "    print ()\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bdd5a1",
   "metadata": {},
   "source": [
    "# Undersample Data\n",
    "- These functions take the three versions of the dataset, which correspond to these names in the paper:\n",
    "    - Thin (Hard)\n",
    "    - Really_Thin (Medium)\n",
    "    - Thin_to_Minimum (Easy)\n",
    "- runs Tomek Links on them once, then again, and saves the results to file.\n",
    "- Each of the three sets takes about 90 minutes to run on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8628b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def Undersample_Data_Thin(round_text):\n",
    "    print ('Undersample_Data_Thin()')\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_Features(data)\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.80)\n",
    "#    data = X_train\n",
    "#    data[target] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "\n",
    "    # CNN took 6 minutes at 99% decreased set size.  \n",
    "#    X_train, y_train = Condensed_Nearest_Neighbour(X_train, y_train)\n",
    "\n",
    "    # 413,913 samples before Tomek\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "    # Two rounds of Tomek took one hour 30 minutes\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # Write to csv and read back in, \n",
    "    #    so we can play with the stuff later without having to redo the Tomek Links, \n",
    "    #    which can take a long time.\n",
    "    \n",
    "    # 399,515 samples after Tomek, v1\n",
    "    # 399,714  v2\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "    \n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # 396,511 after Tomek twice v1\n",
    "    # 396,718 v2\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    print ()\n",
    "    \n",
    "def Undersample_Data_Really_Thin(round_text):\n",
    "    print ('Undersample_Data_Really_Thin()')\n",
    "\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Really_Thin_Features(data)\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.80)\n",
    "#    data = X_train\n",
    "#    data[target] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "\n",
    "    # CNN took 6 minutes at 99% decreased set size.  \n",
    "#    X_train, y_train = Condensed_Nearest_Neighbour(X_train, y_train)\n",
    "\n",
    "    # 413,913 Samples\n",
    "\n",
    "    X_train.to_csv('../../Big_Files/X_train_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "    # Two rounds of Tomek took one hour 30 minutes\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # Write to csv and read back in, \n",
    "    #    so we can play with the stuff later without having to redo the Tomek Links, \n",
    "    #    which can take a long time.\n",
    "    \n",
    "    # 406,691 Samples v1\n",
    "    # 406,781 v2\n",
    "    X_train.to_csv('../../Big_Files/X_train_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    \n",
    "    # 405,288 Samples v1\n",
    "    # 405,368 v2\n",
    "\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    X_train.to_csv('../../Big_Files/X_train_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    print ()\n",
    "    \n",
    "def Undersample_Data_Thin_to_Minimal(round_text):\n",
    "    print ('Undersample_Data_Thin_to_Minimal()')\n",
    "\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_to_Minimal_Features(data)\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.80)\n",
    "#    data = X_train\n",
    "#    data[target] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "\n",
    "    # CNN took 6 minutes at 99% decreased set size.  \n",
    "#    X_train, y_train = Condensed_Nearest_Neighbour(X_train, y_train)\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "\n",
    "    # Two rounds of Tomek took one hour 30 minutes\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # Write to csv and read back in, \n",
    "    #    so we can play with the stuff later without having to redo the Tomek Links, \n",
    "    #    which can take a long time.\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    print ()\n",
    "    \n",
    "#Undersample_Data_Thin('_v1')\n",
    "#Undersample_Data_Really_Thin('_v1')\n",
    "#Undersample_Data_Thin_to_Minimal('_v1')\n",
    "\n",
    "#Undersample_Data_Thin('_v2')\n",
    "#Undersample_Data_Really_Thin('_v2')\n",
    "#Undersample_Data_Thin_to_Minimal('_v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577bab5",
   "metadata": {},
   "source": [
    "## Undersampling Results\n",
    "- Start with 747,342 samples\n",
    "- Remove 33,776 samples iwth pedestrians to get 713,566 samples\n",
    "- Split 70/30 to have 499,496 samples in training set, 214,070 in test set\n",
    "- In training set, 499,496 samples, 78,926 hospitalized, 420,570 not hospitalized\n",
    "\n",
    "\n",
    "| Feature Set | Random Seed | Tomek Round | # Samples Removed | % Samples Removed |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Hard | 1 | 1 | 17,851 | 3.57 |\n",
    "| Hard | 2 | 1 | 7,794 | 3.56 |\n",
    "| Hard | 1 | 2 | 3,664 | 0.76 |\n",
    "| Hard | 2 | 2 | 3,751 | 0.78 |\n",
    "| Medium | 1 | 1 | 8,839 | 1.77 |\n",
    "| Medium | 2 | 1 | 8.825 | 1.77 |\n",
    "| Medium | 1 | 2 | 1,736 | 0.35 |\n",
    "| Medium | 2 | 2 | 1,656 | 0.34 |\n",
    "| Easy | 1 | 1 | 6 | 0.00 |\n",
    "| Easy | 2 | 1 | 3 | 0.00 |\n",
    "| Easy | 1 | 2 | 0 | 0.00 |\n",
    "| Easy | 2 | 2 | 0 | 0.00 |\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93129c5b",
   "metadata": {},
   "source": [
    "# Custom Metrics\n",
    "https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28552cd",
   "metadata": {},
   "source": [
    "## Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balanced_Accuracy(y_true, y_pred):\n",
    "    return K.mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://aakashgoel12.medium.com/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae44b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n",
    "### Define F1 measures: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    def recall_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "        recall = TP / (Positives+K.epsilon())\n",
    "        return recall\n",
    "\n",
    "\n",
    "    def precision_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "        precision = TP / (Pred_Positives+K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(y_true, y_pred):\n",
    "    y_true = tf.dtypes.cast(y_true, tf.float64)\n",
    "    y_pred = tf.dtypes.cast(y_pred, tf.float64)\n",
    "    P = K.sum(y_true)\n",
    "    N = K.sum(1 - y_true)\n",
    "    # Note that Tensorflow and Keras round using \"banker's rounding,\"\n",
    "    # where halves round to the nearest even integer, so\n",
    "    # round(0.5) = 0, but round (1.5) = 2\n",
    "    Discrete_y_pred = K.round(y_pred)\n",
    "    TRUE = K.equal(y_true, Discrete_y_pred)\n",
    "    TRUE = tf.dtypes.cast(TRUE, tf.float64)\n",
    "    FALSE = 1-TRUE\n",
    "    Discrete_TP = Discrete_y_pred * TRUE\n",
    "    TP = K.sum(Discrete_TP)\n",
    "    FN = P - TP\n",
    "    Discrete_TN = (1 - Discrete_y_pred) * TRUE\n",
    "    TN = K.sum(Discrete_TN)\n",
    "    FP = N - TN    \n",
    "\n",
    "#    CM = confusion_matrix(y_true, y_pred)\n",
    "#    print (CM)\n",
    "#    P = CM[1][0] + CM[1][1]\n",
    "#    N = CM[0][0] + CM[0][1]\n",
    "#    TN = CM[0][0]\n",
    "#    FP = CM[0][1]\n",
    "#    FN = CM[1][0]\n",
    "#    TP = CM[1][1]\n",
    "#    print ('TP = ', TP, ' FN = ', FN, ' FP = ', FP, ' TN = ', TN)\n",
    "    \n",
    "    return P, N, TP, FN, TN, FP\n",
    "\n",
    "def Test_Custom_Metric():\n",
    "    y_true = [0.0,1.0,0.0,1.0]\n",
    "    y_proba = [0.2, 0.49, 0.75, 0.9]\n",
    "    y_pred = [round(x) for x in y_proba]\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = tf.convert_to_tensor(y_true)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    print (y_true)\n",
    "    print (y_pred)\n",
    "    P, N, TP, FN, TN, FP = custom_metric(y_true, y_pred)\n",
    "    print ('TP = ', TP, ' FN = ', FN, ' FP = ', FP, ' TN = ', TN)\n",
    "    \n",
    "Test_Custom_Metric()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy_Metric(y_true, y_pred):\n",
    "    P, N, TP, FN, TN, FP = custom_metric(y_true, y_pred)\n",
    "    metric = (TP+TN)/(TP + FN + FP + TN + K.epsilon())\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24413b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision_Metric(y_true, y_pred):\n",
    "    P, N, TP, FN, TN, FP = custom_metric(y_true, y_pred)\n",
    "    metric = TP/(TP + FP + K.epsilon())\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_Metric(y_true, y_pred):\n",
    "    P, N, TP, FN, TN, FP = custom_metric(y_true, y_pred)\n",
    "    metric = TP/(TP + FN + K.epsilon())\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balanced_Accuracy_Metric(y_true, y_pred):\n",
    "    P, N, TP, FN, TN, FP = custom_metric(y_true, y_pred)\n",
    "    metric = ( TN/(2*(TN + FP + K.epsilon())) + TP/(2*(FN + TP + K.epsilon())))\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_Metric(y_true, y_pred):\n",
    "    precision = Precision_Metric(y_true, y_pred)\n",
    "    recall = Recall_Metric(y_true, y_pred)\n",
    "    metric = 2/(1/(precision + K.epsilon()) + 1/(recall + K.epsilon()))\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gmean_Metric(y_true, y_pred):\n",
    "    P, N, TP, FN, TN, FP = custom_metric(y_true, y_pred)\n",
    "    precision = TP/(TP + FP + K.epsilon())\n",
    "    specificity = TN/(TN + FP + K.epsilon())\n",
    "    metric = K.sqrt(precision * specificity)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d911f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balanced_Precision_Metric(y_true, y_pred):\n",
    "    P, N, TP, FN, TN, FP = custom_metric(y_true, y_pred)\n",
    "    metric = (TP * N)/(TP * N + FP * P + K.epsilon())\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f07d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balanced_F1_Metric(y_true, y_pred):\n",
    "    precision = Balanced_Precision_Metric(y_true, y_pred)\n",
    "    recall = Recall_Metric(y_true, y_pred)\n",
    "    metric = 2/(1/(precision + K.epsilon()) + 1/(recall + K.epsilon()))\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dcfd4d",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf4a61",
   "metadata": {},
   "source": [
    "## Alpha Weighted Binary Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_weighted_binary_crossentropy_with_parameter(alpha = 0.5):\n",
    "    def alpha_weighted_binary_crossentropy(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        weights = tf.where(tf.equal(y_true,1),alpha, 1-alpha)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        loss = keras.backend.mean(product)\n",
    "        return loss\n",
    "    return alpha_weighted_binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_weighted_binary_crossentropy_with_class_weight_parameters(weight_0 = 1.0, weight_1 = 1.0):\n",
    "    # Weights for each class = (nSamples Total)/(2* (nSamples in Class))\n",
    "    def alpha_weighted_binary_crossentropy(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "        weights = tf.where(tf.equal(y_true,1),weight_1, weight_0)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        loss = keras.backend.mean(product)\n",
    "        return loss\n",
    "    return alpha_weighted_binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74dedab",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    # The dataset has  259077  elements.\n",
    "    # The target group has  31891  elements.\n",
    "    # Our target is  12.3095 % of the dataset.\n",
    "    # There are  8.12  negative elements for each positive.    \n",
    "#    p = 8.12\n",
    "    p = 5.94\n",
    "\n",
    "    alpha = (p/(p+1))*1.0\n",
    "\n",
    "    gamma_1 = 0.0 # Must be float for the tf.math.pow() function to work.\n",
    "    gamma_2 = 0.0\n",
    "    y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "    binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "#    print (binary_crossentropy.numpy())\n",
    "    weights = tf.where(tf.equal(y_true,1),alpha, 1-alpha)\n",
    "#    print (weights.numpy())\n",
    "    focal = tf.where(tf.equal(y_true,1), (1.0-y_pred), (y_pred))\n",
    "    power = tf.where(tf.equal(y_true,1), gamma_1, gamma_2)\n",
    "    focal_power = tf.math.pow(focal,power)\n",
    "#    print (focal.numpy())\n",
    "#    print (power.numpy())\n",
    "#    print (focal_power.numpy())\n",
    "    product = tf.multiply(binary_crossentropy, weights)\n",
    "    focal_power_product = tf.multiply(product, focal_power)\n",
    "#    print (focal_power_product.numpy())\n",
    "    loss = keras.backend.mean(focal_power_product)\n",
    "#    print (loss.numpy())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaef565",
   "metadata": {},
   "source": [
    "## Focal Loss with Parameters\n",
    "- Adapted from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_with_parameters(alpha = 0.5, gamma_0=0.0, gamma_1=0.0):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "#        tf.clip_by_value(y_pred, 0.00001, 0.99999) # Make sure we don't blow up the logarithm\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        weights = tf.where(tf.equal(y_true,1),alpha, 1.0-alpha)\n",
    "        focal = tf.where(tf.equal(y_true,1), (1.0-y_pred), (y_pred))\n",
    "        power = tf.where(tf.equal(y_true,0), gamma_0, gamma_1)\n",
    "        focal_power = tf.math.pow(focal,power)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        focal_power_product = tf.multiply(product, focal_power)\n",
    "#        tf.clip_by_value(focal_power_product, 0.00001, 0.99999)\n",
    "        loss = keras.backend.mean(focal_power_product)\n",
    "        if math.isnan(loss):\n",
    "            print ('loss is nan')\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "get_custom_objects().update({'focal_loss_with_parameters': focal_loss_with_parameters()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_with_parameters_2(alpha=.25, gamma=2.0):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c9b6f",
   "metadata": {},
   "source": [
    "## Test Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b909c30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Test_Loss_Functions():\n",
    "    \n",
    "    ### Data as list y_test and y_prob\n",
    "    y_test = [0.0]*500 + [1.0]*500\n",
    "    y_test_binary = [0]*500 + [1]*500\n",
    "#    y_test = [0.0, 1.0]*5\n",
    "#    y_test_binary = [0,1]*5\n",
    "#    y_prob = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 0.999]\n",
    "    y_prob = [random.random() for x in range (1000)]\n",
    "#    print (y_prob)\n",
    "    \n",
    "    ### Data as tensors y_true and y_pred\n",
    "    y_true = np.array(y_test, dtype=np.float32)\n",
    "    y_true = tf.convert_to_tensor(y_true)\n",
    "    y_pred = np.array(y_prob, dtype=np.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "    ####################################################\n",
    "    print ('Test with p==1.0, alpha = 0.5, gamma = 0.0')\n",
    "\n",
    "    ### Calculate binary crossentropy by hand\n",
    "    BCE = [-(y_test[i] * math.log(y_prob[i]) + (1 - y_test[i]) * math.log(1 - y_prob[i])) for i in range (10)]\n",
    "    Class_Weights = [1.0,1.0]\n",
    "    Weights = [Class_Weights[y_test_binary[i]] for i in range(10)]\n",
    "    Product = [BCE[i] * Weights[i] for i in range (10)]\n",
    "    loss = sum(Product)/len(Product)\n",
    "    print (loss, \"  Hand-calculated BCE loss\")\n",
    "    \n",
    "    ### Calculate binary crossentropy like I did in my custom loss functions\n",
    "    binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "#    display(binary_crossentropy.numpy())\n",
    "    loss = keras.backend.mean(binary_crossentropy).numpy()\n",
    "    print (loss, \"  My custom AWBCE function's no-alpha backend\")\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_parameter(alpha = 0.5)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom one-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    # Weights for each class = (nSamples Total)/(2* (nSamples in Class))\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_class_weight_parameters(weight_0 = 1.0, weight_1 = 1.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom two-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate binary crossentropy using Keras's loss function\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    loss = bce(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BCE function\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.5,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.5, 0.0, 0.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=0.0')\n",
    "    \n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with p = 3.0, alpha = 0.75, gamma = 0.0')\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_parameter(alpha = 0.75)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom one-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    # Weights for each class = (nSamples Total)/(2* (nSamples in Class))\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_class_weight_parameters(weight_0 = 2.0/3.0, weight_1 = 2.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom two-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.75,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.75, 0.0, 0.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=0.0')\n",
    "    \n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with alpha = 0.8, gamma = 0.0')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.8,\n",
    "        gamma = 0.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.8, 0.0, 0.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function')\n",
    "\n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with alpha = 0.8, gamma = 2.0')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.8,\n",
    "        gamma = 2.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.8, 2.0, 2.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=2.0')\n",
    "\n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with p = 1.0, alpha = 0.5, gamma = 2.0')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.5,\n",
    "        gamma = 2.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.5, 2.0, 2.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=2.0')\n",
    "\n",
    "    ##################################################################\n",
    "    print ()\n",
    "    print (\"Test Keras's BFC Function with different values of alpha\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.1,\n",
    "        gamma = 0.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.5,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.9,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "Test_Loss_Functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c65a77",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a36e3f",
   "metadata": {},
   "source": [
    "## Simple Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Simple_Model(X_train):\n",
    "    print ('Make_Model()')\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Dense(\n",
    "                256, activation=\"relu\", input_shape=(X_train.shape[-1],)\n",
    "            ),\n",
    "            keras.layers.Dense(256, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(256, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "#    display(model.summary())\n",
    "    print ()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Simple_Model (X_train, X_test, y_train, y_test, model, loss_function, r_target, filename):\n",
    "    print ('Train_Model()')\n",
    "    metrics = [\n",
    "#        keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "#        keras.metrics.FalsePositives(name=\"fp\"),\n",
    "#        keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "#        keras.metrics.TruePositives(name=\"tp\"),\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "#        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        Balanced_Accuracy_Metric,\n",
    "#        Gmean_Metric,\n",
    "        Balanced_Precision_Metric,\n",
    "#        F1_Metric,\n",
    "        Balanced_F1_Metric,\n",
    "    ]\n",
    "\n",
    "    model.compile(\n",
    "#        optimizer=keras.optimizers.Adam(), \n",
    "        optimizer=keras.optimizers.Adam(1e-7, clipnorm=0.99999), \n",
    "        loss=loss_function, \n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "#    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "#    class_weight = {0: 1, 1: 1}\n",
    "    class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)}\n",
    "    \n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=128,\n",
    "        epochs=30,\n",
    "        verbose=0,\n",
    "#        callbacks=callbacks,\n",
    "        validation_data=(X_test, y_test),\n",
    "        class_weight=class_weight,\n",
    "    )\n",
    "    \n",
    "    # Make everything a numpy array\n",
    "    y_proba = model.predict(X_test)\n",
    "    # y_proba is a numpy array\n",
    "    y_pred = np.around(y_proba)\n",
    "    # y_test is a Pandas dataframe\n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    Balance_Proba(y_test, y_proba, y_pred, r_target, filename)    \n",
    "    \n",
    "    print ()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f61f36",
   "metadata": {},
   "source": [
    "## Another Keras Binary Classification Model\n",
    "https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, alpha, gamma, epochs, filename, title):\n",
    "    print ('Keras_Binary_Focal_Crossentropy')\n",
    "    print ('alpha = ', alpha, ', gamma = ', gamma)\n",
    "    print ()\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing=True,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "#        from_logits=False,\n",
    "#        label_smoothing=0.0,\n",
    "#        axis=-1,\n",
    "#        reduction=losses_utils.ReductionV2.AUTO,\n",
    "#        name='binary_focal_crossentropy'\n",
    "    )   \n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "#    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    # Compile model\n",
    "    metrics = [\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "        F1_Metric,\n",
    "    ]\n",
    "    model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\n",
    "    estimator = KerasClassifier(\n",
    "        model=model, \n",
    "#        random_state=42,\n",
    "        metrics=metrics,\n",
    "        batch_size=128, \n",
    "        verbose=0,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    # What does this do?\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "    # Fit model\n",
    "    estimator.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )\n",
    "    \n",
    "    # Test for overfit\n",
    "    y_proba = estimator.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    print ('y_proba unique')\n",
    "    print (np.unique(y_proba))\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_train, y_proba, y_pred, r_target, filename + '_Train', title)\n",
    "\n",
    "    # Test model on test set\n",
    "    y_proba = estimator.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    print ('y_proba unique')\n",
    "    print (np.unique(y_proba))\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_test, y_proba, y_pred, r_target, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d06a6c",
   "metadata": {},
   "source": [
    "## Our Binary Focal Crossentropy Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, alpha, gamma_0, gamma_1, epochs, filename, title):\n",
    "    print ('Our_Binary_Focal_Crossentropy')\n",
    "    print ('alpha = ', alpha, ' gamma_0 = ', gamma_0, ', gamma_1 = ', gamma_1)\n",
    "\n",
    "#    alpha_target = r_target/(r_target+1)\n",
    "    loss_function = focal_loss_with_parameters(alpha, gamma_0, gamma_1)\n",
    "#    loss_function = focal_loss_with_parameters_2(alpha_target, gamma)\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "#    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    metrics = [\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "        F1_Metric,\n",
    "    ]\n",
    "    estimator = KerasClassifier(\n",
    "        model=model, \n",
    "#        random_state=42,\n",
    "        metrics=metrics,\n",
    "        batch_size=128, \n",
    "        verbose=0,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    # What does this do?\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "    # Fit model\n",
    "    estimator.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )\n",
    "    y_proba = estimator.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "\n",
    "    Balance_Proba(y_test, y_proba, y_pred, r_target, filename, title)\n",
    "    print ()\n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91de86f",
   "metadata": {},
   "source": [
    "## AdaBoost Model\n",
    "https://stackoverflow.com/questions/39063676/how-to-boost-a-keras-based-neural-network-using-adaboost\n",
    "- model.predict_proba(X_test) returns two columns, \n",
    "    - the first the probability that the sample is in class 0, \n",
    "    - and the second the probability that the sample is in class 1.\n",
    "    - We just want the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764615d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost(X_train, X_test, y_train, y_test, r_target, filename, title):\n",
    "    print ('AdaBoost() ', filename)\n",
    "    model = AdaBoostClassifier(n_estimators=100)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_train, y_proba, y_pred, r_target, filename + '_Train', title)\n",
    "    \n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_test, y_proba, y_pred, r_target, filename + '_Test', title)\n",
    "    print ()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afd759",
   "metadata": {},
   "source": [
    "### Ensembles of Classifiers\n",
    "https://imbalanced-learn.org/stable/ensemble.html#bagging-classifier\n",
    "\n",
    "with arguments based on the documentation examples\n",
    "\n",
    "https://imbalanced-learn.org/stable/auto_examples/ensemble/plot_comparison_ensemble_classifier.html#sphx-glr-auto-examples-ensemble-plot-comparison-ensemble-classifier-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cffa9",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe617891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging(X_train, X_test, y_train, y_test, r_target, filename, title):\n",
    "    print ('Bagging() ', filename)\n",
    "    model = BalancedBaggingClassifier(\n",
    "#        random_state=42\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    \n",
    "    # Check for overfitting on Test set\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_train, y_proba, y_pred, r_target, filename + '_Train', title)\n",
    "\n",
    "    # Test\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_test, y_proba, y_pred, r_target, filename + '_Test', title)\n",
    "\n",
    "    print ()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421ec7b",
   "metadata": {},
   "source": [
    "## Balanced Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, r_target, alpha, filename, title):\n",
    "    print ('Balanced Random Forest Classifier ', filename)\n",
    "    print ('alpha = ', alpha)\n",
    "    print ()\n",
    "    model = BalancedRandomForestClassifier(\n",
    "        bootstrap = True, ccp_alpha = 0.0, criterion = 'gini', \n",
    "        max_depth = None,\n",
    "#        max_depth = 40, \n",
    "        max_features = 'sqrt', \n",
    "        max_leaf_nodes = None,\n",
    "#        max_leaf_nodes = 10000,  \n",
    "        max_samples = None, \n",
    "        min_impurity_decrease = 0.0, \n",
    "        min_samples_leaf = 1, \n",
    "        min_samples_split = 2, \n",
    "        min_weight_fraction_leaf = 0.0, \n",
    "        n_estimators = 100, \n",
    "#        n_estimators = 1000, \n",
    "        n_jobs = None, \n",
    "        oob_score = False, \n",
    "        random_state = None, \n",
    "        replacement = False, \n",
    "        sampling_strategy = 'auto', \n",
    "        verbose = 0, \n",
    "        warm_start = False,\n",
    "        class_weight = {0:1-alpha, 1:alpha}\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    print ()\n",
    "    print ('model.get_params()')\n",
    "    print (model.get_params())\n",
    "    print ()\n",
    "    \n",
    "    print ('[estimator.get_depth() for estimator in model.estimators_]')\n",
    "    print ([estimator.get_depth() for estimator in model.estimators_])\n",
    "    print ()\n",
    "    print ('[estimator.get_n_leaves() for estimator in model.estimators_]')\n",
    "    print ([estimator.get_n_leaves() for estimator in model.estimators_])\n",
    "    print ()\n",
    "    \n",
    "    # Test fit on training data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_train, y_proba, y_pred, r_target, filename + '_Train', title)\n",
    "    \n",
    "    # Test fit on test data, to test for overfit.\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_test, y_proba, y_pred, r_target, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a4b47",
   "metadata": {},
   "source": [
    "## RUSBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240eb43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUSBoost_Classifier(X_train, X_test, y_train, y_test, estimator, r_target, filename, title):\n",
    "    print ('RUSBoost Classifier ', filename)\n",
    "    model = RUSBoostClassifier(\n",
    "        n_estimators=1000, \n",
    "        estimator=estimator,\n",
    "        algorithm='SAMME.R', \n",
    "#        random_state=42\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    \n",
    "    # Test fit on training data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_train, y_proba, y_pred, r_target, filename + '_Train', title)\n",
    "    \n",
    "    # Test fit on test data, to test for overfit.\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_test, y_proba, y_pred, r_target, filename + '_Test', title)\n",
    "    print ()\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb5091",
   "metadata": {},
   "source": [
    "## Easy Ensemble Classifier (Adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db654cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Easy_Ensemble_Classifier(X_train, X_test, y_train, y_test, r_target, filename, title):\n",
    "    print ('Easy Ensemble Classifier ', filename)\n",
    "    estimator = AdaBoostClassifier(n_estimators=10)\n",
    "    model = EasyEnsembleClassifier(n_estimators=10, estimator=estimator)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "\n",
    "    # Test fit on training data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_train, y_proba, y_pred, r_target, filename + '_Train', title)\n",
    "    \n",
    "    # Test fit on test data, to test for overfit.\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_test, y_proba, y_pred, r_target, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592a6f5",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression_Classifier(X_train, X_test, y_train, y_test, r_target, alpha, filename, title):\n",
    "    print ('Logistic Regression Classifier ', filename)\n",
    "    model = LogisticRegression(\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)}\n",
    "        class_weight = {0:1-alpha, 1:alpha},\n",
    "        max_iter=1000,\n",
    "#        random_state=42,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "\n",
    "    # Test fit on training data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_train, y_proba, y_pred, r_target, filename + '_Train', title)\n",
    "    \n",
    "    # Test fit on test data, to test for overfit.\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_proba = np.array(y_proba)\n",
    "    Balance_Proba(y_test, y_proba, y_pred, r_target, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44982302",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6c830",
   "metadata": {},
   "source": [
    "## Adjust Center of Probability Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shift_y_proba(y_test, y_proba, y_pred, filename):\n",
    "    print ('Shift_y_proba()')\n",
    "    print ()\n",
    "    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "    center = (N_median + P_median)/2\n",
    "    print ('N_median = %.3f, P_median = %.3f, center = %.3f' % (N_median, P_median, center))\n",
    "\n",
    "    y_proba = y_proba - center + 0.5\n",
    "    y_proba = np.where (y_proba < 0.0, 0.0, y_proba)\n",
    "    y_proba = np.where (y_proba > 1.0, 1.0, y_proba)\n",
    "    y_pred = K.round(y_proba)\n",
    "\n",
    "    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "    center = (N_median + P_median)/2\n",
    "    print ('N_median = %.3f, P_median = %.3f, center = %.3f' % (N_median, P_median, center))\n",
    "    \n",
    "    print ()\n",
    "    \n",
    "    return y_test, y_proba, y_pred, filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de890369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Transform_y_proba(y_test, y_proba, y_pred, filename):\n",
    "    print ('Linear_Transform_y_proba()')\n",
    "    print ()\n",
    "    \n",
    "    # I considered two methods.  \n",
    "    # One was to take the medians of the negative and positive classes and transform them to 0.25 and 0.75.\n",
    "    # That didn't always work the way I wanted.  \n",
    "    # Then I tried taking the 0.05 quantile to 0.05 and the 0.95 quantile to 0.95.\n",
    "    \n",
    "#    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "#    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    center = (N_median + P_median)/2\n",
    "#    print ('N_median = %.3f, P_median = %.3f, center = %.3f' % (N_median, P_median, center))\n",
    "#    y_proba = 0.25/(center - N_median) * (y_proba - center) + 0.5\n",
    "\n",
    "    \n",
    "    a = np.quantile(y_proba[np.array(y_test)==0],0.05)\n",
    "    b = np.quantile(y_proba[np.array(y_test)==1],0.95)\n",
    "    print ('a = %.3f, b = %.3f' % (a, b))\n",
    "    y_proba = 0.9/(b-a) * (y_proba - a) + 0.05\n",
    "    \n",
    "    y_proba = np.where (y_proba < 0.0, 0.0, y_proba)\n",
    "    y_proba = np.where (y_proba > 1.0, 1.0, y_proba)\n",
    "    y_pred = K.round(y_proba)\n",
    "\n",
    "    print ()\n",
    "    \n",
    "    return y_test, y_proba, y_pred, filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shift_y_proba_to_FP_equals_r_TP(y_test, y_proba, r_target, filename):\n",
    "    print ('Shift_y_proba_to_FP_equals_r_TP()')\n",
    "    print ('y_test is a ', type(y_test))\n",
    "    print ('y_proba is a ', type(y_proba))\n",
    "    print ('r_target = ', r_target)\n",
    "    print ()\n",
    "    \n",
    "    \n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "    n = 100\n",
    "    m = np.quantile(C, 0.005)\n",
    "    M = np.quantile(D,0.995)\n",
    "    print ('Quantiles ', m, M)\n",
    "    bins = [(M-m) * (x/n) + m for x in range (0, n+1)]\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['TP'] = G\n",
    "    df['FP'] = H\n",
    "    roll = 10\n",
    "    df['TP_RA'] = G.rolling(roll, center=True, min_periods=1).mean()\n",
    "    df['FP_RA'] = H.rolling(roll, center=True, min_periods=1).mean()\n",
    "    df['TP/FP'] = df['TP_RA']/df['FP_RA']\n",
    "    df['TP/FP_RA'] = df['TP/FP'].rolling(roll, center=True, min_periods=1).mean()\n",
    "    df['Truncate'] = np.where(df['TP/FP_RA']>2,2,df['TP/FP_RA'])\n",
    "    df['bins'] = bins[:-1]\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    #    display(df)\n",
    "    df_closest = df.iloc[(df['TP/FP_RA'] - r_target).abs().argsort()[:1]]\n",
    "    center = df_closest['bins'].to_numpy()\n",
    "    center = center[0]\n",
    "    center_index = df.index[df['bins'] == center].tolist()\n",
    "#    print (df_closest)\n",
    "    print ('center = ', center)\n",
    "#    print (center_index)\n",
    "    print ()\n",
    "    \n",
    "    print ('Plot TP/FP')\n",
    "    x = df['bins'].to_numpy()\n",
    "    y = df['TP/FP_RA'].to_numpy()\n",
    "    fig = plt.figure(figsize=(2.4,1.8)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    plt.plot(x,y, label='TP/FP', color='black')\n",
    "    plt.axhline(y=2.0, color='black', linestyle='--', label='2.0')  \n",
    "    plt.plot([center], [2.0], marker=\"o\", markersize=6, markerfacecolor='black', markeredgecolor='black')\n",
    "    plt.xticks(\n",
    "        ticks = [m, center, M], \n",
    "        labels = [round(m,3), round(center, 3), round(M,3)],\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend()\n",
    "    plt.title('$\\Delta$TP/$\\Delta$FP')\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('$\\Delta$TP/$\\Delta$FP')\n",
    "    plt.savefig('./Images/' + filename + '_TP_FP.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_TP_FP.pgf', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['FP'] = G\n",
    "    df['TP'] = H\n",
    "    roll = 10\n",
    "    df['FP_RA'] = G.rolling(roll, center=True, min_periods=1).mean()\n",
    "    df['TP_RA'] = H.rolling(roll, center=True, min_periods=1).mean()\n",
    "    df['FP/TP'] = df['FP_RA']/df['TP_RA']\n",
    "    df['FP/TP_RA'] = df['FP/TP'].rolling(roll, center=True, min_periods=1).mean()\n",
    "    df['Truncate'] = np.where(df['FP/TP_RA']>2,2,df['FP/TP_RA'])\n",
    "    df['bins'] = bins[:-1]\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    #    display(df)\n",
    "    df_closest = df.iloc[(df['FP/TP_RA'] - r_target).abs().argsort()[:1]]\n",
    "    center = df_closest['bins'].to_numpy()\n",
    "    center = center[0]\n",
    "    center_index = df.index[df['bins'] == center].tolist()\n",
    "#    print (df_closest)\n",
    "    print ('center = ', center)\n",
    "#    print (center_index)\n",
    "    print ()\n",
    "    \n",
    "    print ('Plot FP/TP')\n",
    "    x = df['bins'].to_numpy()\n",
    "    y = df['FP/TP_RA'].to_numpy()\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    plt.plot(x,y, label='$\\Delta FP/\\Delta TP$', color='black')\n",
    "    plt.axhline(y = r_target, color='black', linestyle='--')  \n",
    "    \n",
    "    # Update r_target here\n",
    "    plt.plot([center], [r_target], label='(%.3f,%d)' % (center, r_target), marker=\"o\", markersize=6, markerfacecolor='black', markeredgecolor='black')\n",
    "    plt.xticks(\n",
    "        ticks = [m, M], \n",
    "#        labels = [round(m,3), round(center, 3), round(M,3)],\n",
    "        labels = [round(m,3), round(M,3)],\n",
    "        rotation=0\n",
    "    )\n",
    "#    ax.annotate('data = (%.3f, %.1f)'%(center, 2.0),(center, 2.0), textcoords='data')\n",
    "#    plt.text(center,2.0,'(%.3f,%.1f)' % (center, 2.0),horizontalalignment='left', verticalalignment='bottom')\n",
    "#    plt.title('$\\Delta FP/\\Delta TP$')\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('$\\Delta$FP/$\\Delta$TP')\n",
    "    ax.legend()\n",
    "    plt.savefig('./Images/' + filename + '_FP_TP.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_FP_TP.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_FP_TP.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Re-center the distribution\n",
    "    y_proba = y_proba - center + 0.5\n",
    "\n",
    "    # Decide which direction to dilate the distribution\n",
    "    M = np.quantile(y_proba,0.995)\n",
    "    m = np.quantile(y_proba,0.005)\n",
    "    right = M-0.5\n",
    "    left = 0.5-m\n",
    "    # If the tail to the right is longer, map M to 1 and 0.5 to itself.\n",
    "    if left < right:\n",
    "        y_proba = (1/(2*M-1))*(y_proba - 0.5) + 0.5\n",
    "    # If the tail to the left is longer, map m to 0 and 0.5 to itself.\n",
    "    if left > right:\n",
    "        y_proba = (1/(1-2*m))*(y_proba - 0.5) + 0.5\n",
    "    print ('y_proba unique = ', np.unique(y_proba))\n",
    "    y_proba = np.clip(y_proba,0,1)\n",
    "    print ('y_proba unique = ', np.unique(y_proba))\n",
    "    print ('M, m, left, right = ', M, m, left, right)\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    \n",
    "    return y_test, y_proba, y_pred, center, filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balance_Proba(y_test, y_proba, y_pred, r_target, filename, title):\n",
    "    print ('Balance_Proba')\n",
    "    print (filename)\n",
    "\n",
    "    Analyze_Prediction(y_test, y_proba, filename, title)\n",
    "    y_test, y_proba_New, y_pred_New, center, filename = Shift_y_proba_to_FP_equals_r_TP(y_test, y_proba, r_target, filename)\n",
    "\n",
    "    N = y_proba[np.array(y_test)==0]\n",
    "    P = y_proba[np.array(y_test)==1]\n",
    "    N_median = np.median(N)\n",
    "    P_median = np.median(P)\n",
    "\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba, filename, title)\n",
    "#    ROC(y_test, y_proba, [center, N_median, P_median], filename)    \n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, center, filename)    \n",
    "    print ()\n",
    "\n",
    "    filename = filename + '_Linear_Transform'\n",
    "    title = title + ' Trans'\n",
    "\n",
    "    N = y_proba_New[np.array(y_test)==0]\n",
    "    P = y_proba_New[np.array(y_test)==1]\n",
    "    N_median = np.median(N)\n",
    "    P_median = np.median(P)\n",
    "    \n",
    "    Plot_Prediction(y_test, y_proba_New, filename, title)\n",
    "\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, filename, title)\n",
    "\n",
    "    Plot_Prediction_Zoom(y_test, y_proba_New, filename, title, 0.4, 0.6)\n",
    "#    ROC(y_test, y_proba, [center, N_median, P_median], filename)    \n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba_New, y_pred_New, center, filename)    \n",
    "    print ()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085583e",
   "metadata": {},
   "source": [
    "## Evaluate_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97848247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_Model(y_test, y_proba, y_pred, center, filename):\n",
    "    print ('Evaluate_Model()')\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = [round(x) for x in y_proba]\n",
    "    y_pred = np.array(y_pred)\n",
    "    print ('np.unique(y_proba) = ', np.unique(y_proba))\n",
    "    print ('np.unique(y_pred) = ', np.unique(y_pred))\n",
    "    CM = confusion_matrix(y_test, y_pred)\n",
    "    print(CM)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    \n",
    "    CSV = [[filename, CM[0][0], CM[0][1], CM[1][0], CM[1][1], center, auc_value]]\n",
    "    np.savetxt('./Confusion_Matrices/' + filename + '.csv', \n",
    "        CSV,\n",
    "        delimiter =\", \", \n",
    "        fmt ='% s'\n",
    "              )\n",
    "    print ()\n",
    "    CM = confusion_matrix(y_test, y_pred, normalize='all')\n",
    "    print(CM)\n",
    "    print ()\n",
    "\n",
    "    y_pred = y_pred.ravel()\n",
    "    y_test = tf.convert_to_tensor(y_test)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "    print ('%.3f & Precision \\cr ' %  Precision_Metric(y_test, y_pred).numpy())\n",
    "    print ('%.3f & Recall \\cr ' %  Recall_Metric(y_test, y_pred).numpy())\n",
    "    print ('%.3f & F1 \\cr ' %  F1_Metric(y_test, y_pred).numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf928217",
   "metadata": {},
   "source": [
    "## Plot Prediction\n",
    "\n",
    "How to insert a .pgf plot into a \\LaTeX document:\n",
    "\n",
    "\\begin{figure}\n",
    "    \\begin{center}\n",
    "        \\input{Plot.pgf}\n",
    "    \\end{center}\n",
    "    \\caption{A PGF histogram from \\texttt{matplotlib}.}\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3496ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction(y_test, y_proba, filename, title):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 10\n",
    "    bins= [x/n for x in range (0, n+1)]\n",
    "    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "    \n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "    plt.xticks(\n",
    "        ticks = [0, 2.5, 5, 7.5, 10], \n",
    "        labels = ['0.0', '0.25', '0.5', '0.75', '1.0'],\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_Pred.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_Pred.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf5823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b634349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction_Zoom(y_test, y_proba, filename, title, left, right):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    B = B[A['HOSPITAL'] > left]\n",
    "    B = B[A['HOSPITAL'] < right]\n",
    "    A = A[A['HOSPITAL'] > left]\n",
    "    A = A[A['HOSPITAL'] < right]\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 10\n",
    "    bins= [left + (right-left)*x/n for x in range (0, n+1)]\n",
    "    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "\n",
    "    ticks = [0, 2.5, 5, 7.5, 10]\n",
    "    labels = [str(round(left + (right-left) * t/10,2)) for t in ticks]\n",
    "    plt.xticks(\n",
    "        ticks = ticks, \n",
    "        labels = labels,\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_Pred_Zoom.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eda862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction_Wide(y_test, y_proba, filename, title):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 25\n",
    "    bins= [x/n for x in range (0, n+1)]\n",
    "    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(5.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "    plt.xticks(\n",
    "        ticks = [0, 2.5, 5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5, 25], \n",
    "        labels = ['0.0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.0'],\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Wide.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Wide.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_Pred_Wide.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de839f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_Prediction(y_test, y_proba, filename, title):\n",
    "    print ('Analyze_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "    print ('print (len(A), len(C), len(D), len(C) + len(D))')\n",
    "    print (len(A), len(C), len(D), len(C) + len(D))\n",
    "\n",
    "    N = len(C)\n",
    "    P = len(D)\n",
    "    \n",
    "    ##### 20 bins\n",
    "    n = 20\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec.'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec.'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\hat{p}$'] = (Analyze['TP'] + Analyze['FP'])/len(y_proba)\n",
    "\n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec.']=Analyze['Prec.'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec.']=Analyze['Rec.'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\hat{p}$']=Analyze['$\\hat{p}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "    print ('./Analyze_Proba/' + filename + '_20.tex')\n",
    "    print (len(y_proba))\n",
    "    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_20.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_20.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "    ##### 100 bins\n",
    "    n = 100\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec.'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec.'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\hat{p}$'] = (Analyze['TP'] + Analyze['FP'])/len(y_proba)\n",
    "\n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec.']=Analyze['Prec.'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec.']=Analyze['Rec.'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\hat{p}$']=Analyze['$\\hat{p}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "#    print ('./Analyze_Proba/' + filename + '_100.tex')\n",
    "#    print (len(y_proba))\n",
    "#    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_100.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_100.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e092785",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Test_Plot_Prediction_Zoom():\n",
    "    print ('Idealized_Results()')\n",
    "    # Set randomness\n",
    "    np.random.seed(42) # NumPy\n",
    "    random.seed(42) # Python\n",
    "    tf.random.set_seed(42) # Tensorflow    \n",
    "\n",
    "    shape, scale = 3.7, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    a = np.random.gamma(shape, scale, 150771)\n",
    "    a = np.where(a>1.0, random.random(), a)\n",
    "    \n",
    "    shape, scale = 3.8, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    b = np.random.gamma(shape, scale, 26621)    \n",
    "    b = np.where(b>1.0, random.random(), b)\n",
    "    b = 1-b\n",
    "    \n",
    "    y_proba = np.concatenate((a,b),axis=0)\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)  \n",
    "    \n",
    "    display(y_proba[:20])\n",
    "    display(y_pred[:20])\n",
    "    \n",
    "    Plot_Prediction(y_test, y_proba, 'Test', 'Test')    \n",
    "    Plot_Prediction_Wide(y_test, y_proba, 'Test', 'Test')    \n",
    "    Plot_Prediction_Zoom(y_test, y_proba, 'Test', 'Test', 0.45, 0.55)\n",
    "    Analyze_Prediction(y_test, y_proba, 'Test', 'Test')    \n",
    "    \n",
    "Test_Plot_Prediction_Zoom()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a04113",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e088e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(y_test, y_proba, p_values, filename):\n",
    "    print ('ROC()')\n",
    "    print (filename)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    \n",
    "    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    print ('N_median, P_median = ', N_median, P_median)\n",
    "\n",
    "    m = np.quantile(y_proba,0.50)\n",
    "    p = np.quantile(y_proba,0.25)\n",
    "    q = np.quantile(y_proba,0.75)\n",
    "    \n",
    "    Y = []\n",
    "    print ('p_values = ', p_values)\n",
    "    for X in p_values:\n",
    "        difference_array = np.absolute(thresholds-X)\n",
    "        index = difference_array.argmin()\n",
    "        F = fpr[index]\n",
    "        T = tpr[index]\n",
    "        Y.append([X,str(round(X,3)),F,T])\n",
    "    \n",
    "    auc_value = auc(fpr, tpr)\n",
    "    auc_value = round(auc_value,3)\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, color='black', label='AUC={:.3f}'.format(auc_value))\n",
    "    \n",
    "    for y in Y:\n",
    "#        plt.plot([y[2]], [y[3]], marker=\"o\", markersize=20, markeredgecolor=\"white\", markerfacecolor=\"white\")\n",
    "#        plt.annotate(\n",
    "#            y[1], # this is the text\n",
    "#            (y[2], y[3]), # these are the coordinates to position the label\n",
    "#            ha='center' # horizontal alignment can be left, right or center\n",
    "#        )\n",
    "        plt.text(\n",
    "            y[2], y[3], # these are the coordinates to position the label\n",
    "            y[1], # this is the text\n",
    "            backgroundcolor='white', # horizontal alignment can be left, right or center\n",
    "            bbox=dict(facecolor='white', edgecolor='none', boxstyle='square,pad=0.3')\n",
    "        )\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "#    plt.title('ROC with AUC {:.3f}'.format(auc_value))\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('./Images/' + filename + '_ROC.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_ROC.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_ROC.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n",
    "    return 0\n",
    "\n",
    "def Test_ROC():\n",
    "    y_test = [0,0,0,0,0,1]*10000\n",
    "#    y_proba = [abs(0.45 - y)+round(0.45*random.random(),2) for y in y_test]\n",
    "    y_proba = [abs(0.45 - y)+round(0.45*random.normalvariate(mu=0.2, sigma=0.2),3) for y in y_test]\n",
    "#    random.normalvariate(mu=0.0, sigma=1.0)\n",
    "    y_test = np.array(y_test)\n",
    "    y_proba = np.array(y_proba)\n",
    "    print (y_test)\n",
    "    print (y_proba)\n",
    "    ROC(y_test, y_proba, [0.5], \"tmp\")\n",
    "    \n",
    "Test_ROC()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb935a",
   "metadata": {},
   "source": [
    "## Build Idealized Results Plots\n",
    "- The Plot_Prediciton and ROC_Curves functions take two lists (or np arrays) and a filename for saving the plots:\n",
    "    - ROC(y_test, y_proba, filename):\n",
    "    - Plot_Prediction(y_test, y_proba, filename):\n",
    "    - y_test is the {0,1} binary and \n",
    "    - y_proba is the (0,1) continuous\n",
    "- The Evaluate_Model(y_test, y_proba, y_pred, filename) takes three lists (or np arrays)\n",
    "    - y_test is the {0,1} binary ground truth,\n",
    "    - y_proba is the (0,1) continuous prediction, and\n",
    "    - y_pred is the discrete {0,1} binary version of y_proba\n",
    "- We want a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66bb65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Move_Threshold(y_proba, y_test):\n",
    "    print ('Move_Threshold()')\n",
    "    n = 10\n",
    "    T = [x/n for x in range (n+1)]\n",
    "    \n",
    "    print (type(y_proba))\n",
    "    print (type(y_test))\n",
    "    y_test = np.array(y_test)\n",
    "    print (type(y_test))\n",
    "    N = y_proba[y_test==0]\n",
    "    P = y_proba[y_test==1]\n",
    "    print (len(N), len(P))\n",
    "\n",
    "    A = [['t', 'TN', 'FP', 'FN', 'TP', 'TPR', 'FPR']]\n",
    "    for t in T:\n",
    "        TN = len(N[N<t])\n",
    "        FP = len(N[N>t])\n",
    "        FN = len(P[P<t])\n",
    "        TP = len(P[P>t])\n",
    "        TPR = TP/len(P)\n",
    "        FPR = FP/len(N)\n",
    "        A.append([t, TN, FP, FN, TP, TPR, FPR])\n",
    "    display(pd.DataFrame(A))\n",
    "    \n",
    "    print ()\n",
    "\n",
    "\n",
    "def Idealized_Results():\n",
    "    print ('Idealized_Results()')\n",
    "    # Set randomness\n",
    "    np.random.seed(0) # NumPy\n",
    "    random.seed(0) # Python\n",
    "    tf.random.set_seed(0) # Tensorflow    \n",
    "\n",
    "    shape, scale = 3.7, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    a = np.random.gamma(shape, scale, 150771)\n",
    "    a = np.where(a>1.0, random.random(), a)\n",
    "    \n",
    "    shape, scale = 3.8, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    b = np.random.gamma(shape, scale, 26621)    \n",
    "    b = np.where(b>1.0, random.random(), b)\n",
    "    b = 1-b\n",
    "    \n",
    "    y_proba = np.concatenate((a,b),axis=0)\n",
    "    Y_PROBA = np.concatenate((a,b),axis=0)\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)\n",
    "    N_median = np.median(a)\n",
    "    P_median = np.median(b)\n",
    "    \n",
    "    Move_Threshold(y_proba, y_test)\n",
    "    \n",
    "    filename = 'Ideal'\n",
    "    print (filename)\n",
    "    title = 'Original Example'\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [N_median, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    filename = 'Ideal_Left'\n",
    "    title = 'Never Ambulance'\n",
    "    print (filename)\n",
    "    y_proba = 0.5 * y_proba\n",
    "    y_pred = K.round(y_proba)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [N_median, 0.5, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "\n",
    "    filename = 'Ideal_Left_Shifted'\n",
    "    title = ''\n",
    "    print (filename)\n",
    "    y_test, y_proba, y_pred, filename = Shift_y_proba(y_test, y_proba, y_pred, filename)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    filename = 'Ideal_Left_Linear_Transform'\n",
    "    title = ''\n",
    "    print (filename)\n",
    "    y_test, y_proba, y_pred, filename = Linear_Transform_y_proba(y_test, y_proba, y_pred, filename)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    filename = 'Ideal_Right'\n",
    "    title = 'Always Ambulance'\n",
    "    print (filename)\n",
    "    y_proba = 0.5 * Y_PROBA + 0.5\n",
    "    y_pred = K.round(y_proba)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [N_median, 0.5, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "\n",
    "    filename = 'Ideal_Right_Shifted'\n",
    "    title = ''\n",
    "    print (filename)\n",
    "    y_test, y_proba, y_pred, filename = Shift_y_proba(y_test, y_proba, y_pred, filename)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    filename = 'Ideal_Right_Linear_Transform'\n",
    "    title = ''\n",
    "    print (filename)\n",
    "    y_test, y_proba, y_pred, filename = Linear_Transform_y_proba(y_test, y_proba, y_pred, filename)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    y_proba = Y_PROBA\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)\n",
    "    \n",
    "    filename = 'Ideal_Tight'\n",
    "    title = 'Tight'\n",
    "    print (filename)\n",
    "    y_proba = 0.2 * Y_PROBA + 0.4\n",
    "    y_pred = K.round(y_proba)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [N_median, 0.5, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "   \n",
    "    y_proba = Y_PROBA\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)\n",
    "    \n",
    "    filename = 'Ideal_Shift_to_FP_equals_r_TP'\n",
    "    title = 'Transformed'\n",
    "    print (filename)\n",
    "    Plot_Prediction_Zoom(y_test, y_proba, 'Test', 'Test', 0.53, 0.73)    \n",
    "    y_test, y_proba, y_pred, p_target, filename_tmp = Shift_y_proba_to_FP_equals_r_TP(y_test, y_proba, 2.0, filename)\n",
    "    print ('type(y_test) = ', type(y_test))\n",
    "    N = y_proba[np.array(y_test)==0]\n",
    "    P = y_proba[np.array(y_test)==1]\n",
    "    display(N)\n",
    "    N_median = np.median(N)\n",
    "    P_median = np.median(P)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction_Zoom(y_test, y_proba, filename, title, 0.4, 0.6)\n",
    "    ROC(y_test, y_proba, [N_median, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "Idealized_Results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a285425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Awful_Results():\n",
    "    # Set randomness\n",
    "    np.random.seed(42) # NumPy\n",
    "    random.seed(42) # Python\n",
    "    tf.random.set_seed(42) # Tensorflow    \n",
    "    \n",
    "    \n",
    "    shape, scale = 1.0, 0.5 # mean=4, std=2*sqrt(2)\n",
    "    a = np.random.random(600)   \n",
    "    \n",
    "    b = np.random.random(100)    \n",
    "    b = np.where(b>1.0, random.random(), b)\n",
    "    b = 1-b\n",
    "    \n",
    "    y_proba = np.concatenate((a,b),axis=0)\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)\n",
    "    \n",
    "    filename = 'Awful'\n",
    "    title = 'Awful'\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    \n",
    "Awful_Results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4a296",
   "metadata": {},
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Models(Features = 'Hard', Tomek = 0, Version = 1):\n",
    "    r_target = 2.0\n",
    "    alpha = r_target/(1+r_target)\n",
    "\n",
    "    \n",
    "    if Features == 'Hard':\n",
    "        read_filename_features = '_Thin'\n",
    "        write_filename_features = '_Hard'\n",
    "    if Features == 'Medium':\n",
    "        read_filename_features = '_Really_Thin'\n",
    "        write_filename_features = '_Medium'\n",
    "    if Features == 'Easy':\n",
    "        read_filename_features = '_Thin_to_Minimal'\n",
    "        write_filename_features = '_Easy'\n",
    "    if Tomek==0:\n",
    "        read_filename_tomek = '_before_Tomek'\n",
    "        write_filename_tomek = '_Tomek_0'\n",
    "    if Tomek==1:\n",
    "        read_filename_tomek = '_after_Tomek'\n",
    "        write_filename_tomek = '_Tomek_1'\n",
    "    if Tomek==2:\n",
    "        read_filename_tomek = '_after_Tomek_Twice'\n",
    "        write_filename_tomek = '_Tomek_2'\n",
    "    if Version==1:\n",
    "        filename_version = '_v1'\n",
    "        random_seed = 0\n",
    "    if Version==2:\n",
    "        filename_version = '_v2'\n",
    "        random_seed = 42\n",
    "\n",
    "    X_train = pd.read_csv('../../Big_Files/X_train' + read_filename_features + read_filename_tomek + filename_version + '.csv')\n",
    "    y_train = pd.read_csv('../../Big_Files/y_train' + read_filename_features + read_filename_tomek + filename_version + '.csv').squeeze()\n",
    "    X_test = pd.read_csv('../../Big_Files/X_test' + read_filename_features + read_filename_tomek + filename_version + '.csv')\n",
    "    y_test = pd.read_csv('../../Big_Files/y_test' + read_filename_features + read_filename_tomek + filename_version + '.csv').squeeze()\n",
    "\n",
    "    N = len(y_train)\n",
    "    n = len(y_train[y_train==1])\n",
    "    p = (N-n)/n\n",
    "    alpha_balanced = p/(p+1)\n",
    "    print ('p = ', p)\n",
    "    print ('alpha_balanced = ', alpha_balanced)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    " \n",
    "\n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'KBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced_gamma_0_0' + filename_version\n",
    "    title = 'Focal $\\gamma=0.0$'\n",
    "    print (filename)\n",
    "    print ('alpha_balanced = ', alpha_balanced)\n",
    "    gamma = 0.0\n",
    "    epochs=20\n",
    "    Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, alpha_balanced, gamma, epochs, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'KBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_target_gamma_0_0' + filename_version\n",
    "    title = 'Focal $\\gamma=0.0$'\n",
    "    print (filename)\n",
    "    gamma = 0.0\n",
    "    epochs=20\n",
    "    Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, alpha, gamma, epochs, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'KBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_0_5_gamma_0_0' + filename_version\n",
    "    title = 'Focal $\\gamma=0.0$'\n",
    "    print (filename)\n",
    "    gamma = 0.0\n",
    "    epochs=20\n",
    "    Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, 0.5, gamma, epochs, filename, title)\n",
    "\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'KBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_target_gamma_0_5' + filename_version\n",
    "    title = 'Focal $\\gamma=0.5$'\n",
    "    print (filename)\n",
    "    gamma = 0.5\n",
    "    epochs=20\n",
    "    Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, alpha, gamma, epochs, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'KBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_target_gamma_1_0' + filename_version\n",
    "    title = 'Focal $\\gamma=1.0$'\n",
    "    print (filename)\n",
    "    gamma = 1.0\n",
    "    epochs=20\n",
    "    Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, alpha, gamma, epochs, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'KBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_target_gamma_2_0' + filename_version\n",
    "    title = 'Focal $\\gamma=2.0$'\n",
    "    print (filename)\n",
    "    gamma = 2.0\n",
    "    epochs=20\n",
    "    Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, alpha, gamma, epochs, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'KBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_target_gamma_5_0' + filename_version\n",
    "    title = 'Focal $\\gamma=5.0$'\n",
    "    print (filename)\n",
    "    gamma = 5.0\n",
    "    epochs=20\n",
    "    Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, alpha, gamma, epochs, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    " \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'OBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_target_gamma_0_0_5_gamma_1_2_0' + filename_version\n",
    "#    alpha = alpha_target\n",
    "    gamma_0 = 0.5\n",
    "    gamma_1 = 2.0\n",
    "    epochs = 20\n",
    "    title = 'OBFC $\\alpha = ' + \"{:.2f}\".format(alpha) + '\\gamma_0 = ' + \"{:.2f}\".format(gamma_0) + ' \\gamma_1 = ' + \"{:.2f}\".format(gamma_1) + '$'\n",
    "    Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, r_target, alpha, gamma_0, gamma_1, epochs, filename, title)\n",
    "    \n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'LRC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_0_5' + filename_version\n",
    "    title = 'LogReg'\n",
    "    Logistic_Regression_Classifier(X_train, X_test, y_train, y_test, r_target, 0.5, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "\n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'LRC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_target' + filename_version\n",
    "    title = 'LogReg'\n",
    "    Logistic_Regression_Classifier(X_train, X_test, y_train, y_test, r_target, alpha, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'LRC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced' + filename_version\n",
    "    title = 'LogReg'\n",
    "    Logistic_Regression_Classifier(X_train, X_test, y_train, y_test, r_target, alpha_balanced, filename, title)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'BRFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_0_5' + filename_version\n",
    "    title = 'BRForest'\n",
    "    Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, r_target, 0.5, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'BRFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_target' + filename_version\n",
    "    title = 'BRForest'\n",
    "    Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, r_target, alpha, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'BRFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced' + filename_version\n",
    "    title = 'BRForest'\n",
    "    Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, r_target, alpha_balanced, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'AdaBoost'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '' + filename_version\n",
    "    title = 'AdaBoost'\n",
    "    AdaBoost(X_train, X_test, y_train, y_test, r_target, filename, title)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'Bagging'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '' + filename_version\n",
    "    title = 'BalBag'\n",
    "    Bagging(X_train, X_test, y_train, y_test, r_target, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'RUSBoost'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '' + filename_version\n",
    "    title = 'RUSBoost'\n",
    "    estimator = DecisionTreeClassifier(\n",
    "        max_depth=1,\n",
    "        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )\n",
    "    RUSBoost_Classifier(X_train, X_test, y_train, y_test, estimator, r_target, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'EEC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '' + filename_version\n",
    "    title = 'EasyEns'\n",
    "    Easy_Ensemble_Classifier(X_train, X_test, y_train, y_test, r_target, filename, title)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e482ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Run_Models(Features = 'Hard', Tomek = 0, Version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45295c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run_Models(Features = 'Hard', Tomek = 1, Version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c126a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run_Models(Features = 'Hard', Tomek = 2, Version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd292d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run_Models(Features = 'Medium', Tomek = 0, Version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run_Models(Features = 'Medium', Tomek = 1, Version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run_Models(Features = 'Medium', Tomek = 2, Version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e41e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Run_Models(Features = 'Easy', Tomek = 0, Version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b79ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Run_Models(Features = 'Hard', Tomek = 0, Version = 2)\n",
    "#Run_Models(Features = 'Hard', Tomek = 1, Version = 2)\n",
    "#Run_Models(Features = 'Hard', Tomek = 2, Version = 2)\n",
    "#Run_Models(Features = 'Medium', Tomek = 0, Version = 2)\n",
    "#Run_Models(Features = 'Medium', Tomek = 1, Version = 2)\n",
    "#Run_Models(Features = 'Medium', Tomek = 2, Version = 2)\n",
    "#Run_Models(Features = 'Easy', Tomek = 0, Version = 2)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2658775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d03591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Henry_Run_Models(Features = 'Hard', Tomek = 0, Version = 1):\n",
    "    r_target = 3.0\n",
    "    alpha_target = r_target/(1+r_target)\n",
    "\n",
    "    if Features == 'Hard':\n",
    "        read_filename_features = '_Thin'\n",
    "        write_filename_features = '_Hard'\n",
    "    if Features == 'Medium':\n",
    "        read_filename_features = '_Really_Thin'\n",
    "        write_filename_features = '_Medium'\n",
    "    if Features == 'Easy':\n",
    "        read_filename_features = '_Thin_to_Minimal'\n",
    "        write_filename_features = '_Easy'\n",
    "    if Tomek==0:\n",
    "        read_filename_tomek = '_before_Tomek'\n",
    "        write_filename_tomek = '_Tomek_0'\n",
    "    if Tomek==1:\n",
    "        read_filename_tomek = '_after_Tomek'\n",
    "        write_filename_tomek = '_Tomek_1'\n",
    "    if Tomek==2:\n",
    "        read_filename_tomek = '_after_Tomek_Twice'\n",
    "        write_filename_tomek = '_Tomek_2'\n",
    "    if Version==1:\n",
    "        filename_version = '_v1'\n",
    "        random_seed = 0\n",
    "    if Version==2:\n",
    "        filename_version = '_v2'\n",
    "        random_seed = 42\n",
    "\n",
    "    X_train = pd.read_csv('../../Big_Files/X_train' + read_filename_features + read_filename_tomek + filename_version + '.csv')\n",
    "    y_train = pd.read_csv('../../Big_Files/y_train' + read_filename_features + read_filename_tomek + filename_version + '.csv').squeeze()\n",
    "    X_test = pd.read_csv('../../Big_Files/X_test' + read_filename_features + read_filename_tomek + filename_version + '.csv')\n",
    "    y_test = pd.read_csv('../../Big_Files/y_test' + read_filename_features + read_filename_tomek + filename_version + '.csv').squeeze()\n",
    "\n",
    "    N = len(y_train)\n",
    "    n = len(y_train[y_train==1])\n",
    "    p = (N-n)/n\n",
    "    alpha_balanced = p/(p+1)\n",
    "    print ('p = ', p)\n",
    "    print ('alpha_balanced = ', alpha_balanced)\n",
    "    \n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "\n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'BRFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_0_5' + filename_version + '_r_target_3'\n",
    "    title = 'BRForest'\n",
    "    Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, r_target, 0.5, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'BRFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_target' + filename_version + '_r_target_3'\n",
    "    title = 'BRForest'\n",
    "    Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, r_target, alpha_target, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'BRFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced' + filename_version + '_r_target_3'\n",
    "    title = 'BRForest'\n",
    "    Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, r_target, alpha_balanced, filename, title)\n",
    "\n",
    "\n",
    "\n",
    "#Henry_Run_Models(Features = 'Hard', Tomek = 0, Version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d99a00b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499496, 174)\n",
      "(30000, 174)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:70\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:51\u001b[0m, in \u001b[0;36mFeature_Selection\u001b[0;34m(Features, Tomek, Version)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/feature_selection/_rfe.py:251\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/feature_selection/_rfe.py:299\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[0;32m--> 299\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m    302\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    303\u001b[0m     estimator,\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[1;32m    305\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    306\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/svm/_base.py:252\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m--> 252\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/svm/_base.py:331\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    317\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    321\u001b[0m (\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[0;32m--> 331\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "File \u001b[0;32msklearn/svm/_libsvm.pyx:236\u001b[0m, in \u001b[0;36msklearn.svm._libsvm.fit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Feature_Selection(Features = 'Hard', Tomek = 0, Version = 1):\n",
    "\n",
    "    if Features == 'Hard':\n",
    "        read_filename_features = '_Thin'\n",
    "        write_filename_features = '_Hard'\n",
    "    if Features == 'Medium':\n",
    "        read_filename_features = '_Really_Thin'\n",
    "        write_filename_features = '_Medium'\n",
    "    if Features == 'Easy':\n",
    "        read_filename_features = '_Thin_to_Minimal'\n",
    "        write_filename_features = '_Easy'\n",
    "    if Tomek==0:\n",
    "        read_filename_tomek = '_before_Tomek'\n",
    "        write_filename_tomek = '_Tomek_0'\n",
    "    if Tomek==1:\n",
    "        read_filename_tomek = '_after_Tomek'\n",
    "        write_filename_tomek = '_Tomek_1'\n",
    "    if Tomek==2:\n",
    "        read_filename_tomek = '_after_Tomek_Twice'\n",
    "        write_filename_tomek = '_Tomek_2'\n",
    "    if Version==1:\n",
    "        filename_version = '_v1'\n",
    "        random_seed = 0\n",
    "    if Version==2:\n",
    "        filename_version = '_v2'\n",
    "        random_seed = 42\n",
    "    \n",
    "    X_train = pd.read_csv('../../Big_Files/X_train' + read_filename_features + read_filename_tomek + filename_version + '.csv')\n",
    "    y_train = pd.read_csv('../../Big_Files/y_train' + read_filename_features + read_filename_tomek + filename_version + '.csv').squeeze()\n",
    "    X_test = pd.read_csv('../../Big_Files/X_test' + read_filename_features + read_filename_tomek + filename_version + '.csv')\n",
    "    y_test = pd.read_csv('../../Big_Files/y_test' + read_filename_features + read_filename_tomek + filename_version + '.csv').squeeze()\n",
    "\n",
    "    X = X_train.head(30000)\n",
    "    y = y_train.head(30000)\n",
    "    \n",
    "    # 10,000: 10 min\n",
    "    # 20,000:  1 hour\n",
    "    \n",
    "#    X = X_train\n",
    "#    y = y_train\n",
    "    print (X_train.shape)\n",
    "    print (X.shape)\n",
    "#    print (list(X.columns))\n",
    "    \n",
    "    \n",
    "    from sklearn.datasets import make_friedman1\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.svm import SVR\n",
    "    estimator = SVR(kernel=\"linear\")\n",
    "    selector = RFE(estimator, n_features_to_select=100, step=1)\n",
    "    selector = selector.fit(X, y)\n",
    "#    print (selector.support_)\n",
    "#    print (selector.ranking_)\n",
    "    \n",
    "    A = list(X.columns)\n",
    "    B = selector.ranking_\n",
    "    C = [[B[i], A[i]] for i in range (len(A))]\n",
    "    C = sorted(C, key=lambda x: x[0])\n",
    "\n",
    "    for c in C:\n",
    "        print (c[0], c[1])\n",
    "    \n",
    "    print ()\n",
    "    print ()\n",
    "    C = sorted(C, key=lambda x: x[1])\n",
    "\n",
    "    for c in C:\n",
    "        print (c[0], c[1])\n",
    "    \n",
    "Feature_Selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72178501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
