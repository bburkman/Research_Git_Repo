{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767154e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e1d2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7d47d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b4fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('Install Packages')\n",
    "\n",
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "#from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import scipy as sc\n",
    "print ('SciPy version:  {}'.format(sc.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print ('TensorFlow version:  {}'.format(tf.__version__))\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "from tensorflow import keras\n",
    "print ('Keras version:  {}'.format(keras.__version__))\n",
    "\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "from keras.layers import IntegerLookup\n",
    "from keras.layers import Normalization\n",
    "from keras.layers import StringLookup\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.utils import tf_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "#    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Library for reading Microsoft Access files\n",
    "#import pandas_access as mdb\n",
    "\n",
    "import sklearn\n",
    "print ('SciKit-Learn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import imblearn\n",
    "print ('Imbalanced-Learn version: {}'.format(imblearn.__version__))\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "#!pip install pydot\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "#np.random.seed(42) # NumPy\n",
    "#random.seed(42) # Python\n",
    "#tf.random.set_seed(42) # Tensorflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Installing Packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437f109",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919fb2db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Get_Data():\n",
    "    print ('Get_Data()')\n",
    "    data = pd.read_csv(\n",
    "        '../../Big_Files/CRSS_Imputed_All_05_19_23.csv',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Get_Data()')\n",
    "    print ()\n",
    "    return data\n",
    "\n",
    "def Test_Get_Data():\n",
    "    data = Get_Data()\n",
    "    display (data.head())\n",
    "    \n",
    "#Test_Get_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d540640",
   "metadata": {},
   "source": [
    "# Remove_Pedestrian_Crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Pedestrian_Crashes(data):\n",
    "    print ('Remove_Pedestrian_Crashes()')\n",
    "    display(data.PEDS.value_counts())\n",
    "    n = len(data[data.PEDS>0])\n",
    "    print ('Removing %d crashes that involve a pedestrian.' % n)\n",
    "    data = data[data.PEDS==0]\n",
    "    return data\n",
    "\n",
    "def Test_Remove_Pedestrian_Crashes():\n",
    "    data = Get_Data()\n",
    "    print (len(data))\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    print (len(data))\n",
    "\n",
    "#Test_Remove_Pedestrian_Crashes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb9dce",
   "metadata": {},
   "source": [
    "## Engineer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering_Cross_Two(data):\n",
    "    print ('Feature_Engineering_Cross_Two')\n",
    "    Pairs = [\n",
    "        ['AGE', 'SEX', 'AGE_x_SEX'],\n",
    "        ['AGE', 'SCH_BUS', 'AGE_x_SCH_BUS']\n",
    "    ]\n",
    "    for P in Pairs:\n",
    "        data[P[2]] = data[P[0]].map(str) + '_x_' + data[P[1]].map(str)\n",
    "    \n",
    "    print ()\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b900000",
   "metadata": {},
   "source": [
    "## Thin Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_Features(data):\n",
    "    print ('Thin_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "        'PERMVIT',\n",
    "        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "        'RELJCT2',\n",
    "        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "        'VE_FORMS',\n",
    "        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "        'BODY_TYP',\n",
    "        'BUS_USE',\n",
    "        'EMER_USE',\n",
    "        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "        'MODEL',\n",
    "        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "        'LOCATION',\n",
    "        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "        'VEH_AGE',\n",
    "        'AGE_x_SEX',\n",
    "        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e759eda",
   "metadata": {},
   "source": [
    "## Really Thin Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4202cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Really_Thin_Features(data):\n",
    "    print ('Really_Thin_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "        'AGE_x_SEX',\n",
    "#        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Really_Thin_Features():\n",
    "    data = Get_Data()\n",
    "    data = Really_Thin_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Really_Thin_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11264ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_to_Minimal_Features(data):\n",
    "    print ('Thin_to_Minimal_Features()')\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "#        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "#        'REL_ROAD',\n",
    "#        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "#        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "#        'VALIGN',\n",
    "#        'VNUM_LAN',\n",
    "#        'VPROFILE',\n",
    "#        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "#        'VTRAFCON',\n",
    "#        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "#        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "#        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "#        'AGE_x_SEX',\n",
    "#        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_to_Minimal_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_to_Minimal_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_to_Minimal_Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52366e1a",
   "metadata": {},
   "source": [
    "## Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cefa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Dummies(data, target):\n",
    "    print ('Get_Dummies')\n",
    "    data = data.astype('category')\n",
    "    Target = data.pop(target)\n",
    "    data_Dummies = pd.get_dummies(data, prefix = data.columns)\n",
    "    data_Dummies = data_Dummies.join(Target)\n",
    "#    for feature in data_Dummies:\n",
    "#        print (feature)\n",
    "    print ()\n",
    "\n",
    "    return data_Dummies\n",
    "\n",
    "def Test_Get_Dummies():\n",
    "    print ('Test_Get_Dummies')\n",
    "    A = pd.DataFrame({\n",
    "        'A': ['a', 'b', 'a'], \n",
    "        'B': ['b', 'a', 'c'], \n",
    "        'C': [1, 2, 3]})\n",
    "    C = Get_Dummies(A, 'C')\n",
    "    display(C)\n",
    "    print ()\n",
    "\n",
    "#Test_Get_Dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d43d6e",
   "metadata": {},
   "source": [
    "## Test-Train Split\n",
    "- We're using sklearn's train_test_split rather than Pandas's sample because the former has a 'stratify' option that will put the same proportion of HOSPITAL==1 into each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Data(data, target, test_size):\n",
    "    print ('Split_Data()')\n",
    "    X = data.drop(columns=[target])\n",
    "    y = data[target]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=test_size, \n",
    "        #random_state=42\n",
    "    )\n",
    "    \n",
    "    a = y_train[y_train==1].shape[0]\n",
    "    b = y_test[y_test==1].shape[0]\n",
    "    print (\n",
    "        x_train.shape, \n",
    "        y_train.shape, a, round((a/(a+b)*100),2), '%')\n",
    "    print (\n",
    "        x_test.shape, \n",
    "        y_test.shape, b, round((b/(a+b)*100),2), '%'\n",
    "    )\n",
    "    print ()\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea65f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43172997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c7b22f0",
   "metadata": {},
   "source": [
    "# Imbalanced Data Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984f24a",
   "metadata": {},
   "source": [
    "## Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a61c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tomek_Links(X_train, y_train):\n",
    "    print ('Tomek_Links()')\n",
    "    M = len(y_train)\n",
    "    N = len(y_train)\n",
    "    n = len(y_train[y_train==1])\n",
    "    p = (N-n)/n\n",
    "    print ('Before Tomek Links:')\n",
    "    print ('%d samples, %d hospitalized, %d not hospitalized' % (N, n, N-n))\n",
    "    print ('%f percent of samples hospitalized' % (n/N*100))\n",
    "    print ('There are %f negative samples for each positive.' % ((N-n)/n))\n",
    "    print ()\n",
    "\n",
    "    X_train, y_train = TomekLinks().fit_resample(X_train, y_train)\n",
    "    N = len(y_train)\n",
    "    n = len(y_train[y_train==1])\n",
    "    p = (N-n)/n\n",
    "    print ('After Tomek Links:')\n",
    "    print ('%d samples, %d hospitalized, %d not hospitalized' % (N, n, N-n))\n",
    "    print ('%f percent of samples hospitalized' % (n/N*100))\n",
    "    print ('There are %f negative samples for each positive.' % ((N-n)/n))\n",
    "    print ('Removed %d samples, or %.2f%% of the set.' % (M-N, (M-N)/M*100))\n",
    "    print ()\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce9a6e",
   "metadata": {},
   "source": [
    "## Condensed Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Condensed_Nearest_Neighbour(X_train, y_train):\n",
    "    print ('Condensed_Nearest_Neighbour()')\n",
    "    N = X_train.shape[0]\n",
    "    print ('X_train.shape before = ', X_train.shape)\n",
    "    print ('y_train.shape before = ', y_train.shape)\n",
    "    print ()\n",
    "    cnn = CondensedNearestNeighbour(n_neighbors=None)\n",
    "    X_train, y_train = cnn.fit_resample(X_train, y_train)\n",
    "    n = X_train.shape[0]\n",
    "    print ('X_train.shape after = ', X_train.shape)\n",
    "    print ('y_train.shape after = ', y_train.shape)\n",
    "    print ()\n",
    "    print ('Removed %d samples, or %.2f%% of the set.' % (N-n, (N-n)/N*100))\n",
    "    print ()\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bdd5a1",
   "metadata": {},
   "source": [
    "# Undersample Data\n",
    "- These functions take the three versions of the dataset, which correspond to these names in the paper:\n",
    "    - Thin (Hard)\n",
    "    - Really_Thin (Medium)\n",
    "    - Thin_to_Minimum (Easy)\n",
    "- runs Tomek Links on them once, then again, and saves the results to file.\n",
    "- Each of the three sets takes about 90 minutes to run on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8628b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def Undersample_Data_Thin(round_text):\n",
    "    print ('Undersample_Data_Thin()')\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_Features(data)\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.80)\n",
    "#    data = X_train\n",
    "#    data[target] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "\n",
    "    # CNN took 6 minutes at 99% decreased set size.  \n",
    "#    X_train, y_train = Condensed_Nearest_Neighbour(X_train, y_train)\n",
    "\n",
    "    # 413,913 samples before Tomek\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "    # Two rounds of Tomek took one hour 30 minutes\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # Write to csv and read back in, \n",
    "    #    so we can play with the stuff later without having to redo the Tomek Links, \n",
    "    #    which can take a long time.\n",
    "    \n",
    "    # 399,515 samples after Tomek, v1\n",
    "    # 399,714  v2\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "    \n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # 396,511 after Tomek twice v1\n",
    "    # 396,718 v2\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    print ()\n",
    "    \n",
    "def Undersample_Data_Really_Thin(round_text):\n",
    "    print ('Undersample_Data_Really_Thin()')\n",
    "\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Really_Thin_Features(data)\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.80)\n",
    "#    data = X_train\n",
    "#    data[target] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "\n",
    "    # CNN took 6 minutes at 99% decreased set size.  \n",
    "#    X_train, y_train = Condensed_Nearest_Neighbour(X_train, y_train)\n",
    "\n",
    "    # 413,913 Samples\n",
    "\n",
    "    X_train.to_csv('../../Big_Files/X_train_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "    # Two rounds of Tomek took one hour 30 minutes\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # Write to csv and read back in, \n",
    "    #    so we can play with the stuff later without having to redo the Tomek Links, \n",
    "    #    which can take a long time.\n",
    "    \n",
    "    # 406,691 Samples v1\n",
    "    # 406,781 v2\n",
    "    X_train.to_csv('../../Big_Files/X_train_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    \n",
    "    # 405,288 Samples v1\n",
    "    # 405,368 v2\n",
    "\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    X_train.to_csv('../../Big_Files/X_train_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    print ()\n",
    "    \n",
    "def Undersample_Data_Thin_to_Minimal(round_text):\n",
    "    print ('Undersample_Data_Thin_to_Minimal()')\n",
    "\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_to_Minimal_Features(data)\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.80)\n",
    "#    data = X_train\n",
    "#    data[target] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "\n",
    "    # CNN took 6 minutes at 99% decreased set size.  \n",
    "#    X_train, y_train = Condensed_Nearest_Neighbour(X_train, y_train)\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "\n",
    "    # Two rounds of Tomek took one hour 30 minutes\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # Write to csv and read back in, \n",
    "    #    so we can play with the stuff later without having to redo the Tomek Links, \n",
    "    #    which can take a long time.\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    print ()\n",
    "    \n",
    "#Undersample_Data_Thin('_v1')\n",
    "#Undersample_Data_Really_Thin('_v1')\n",
    "#Undersample_Data_Thin_to_Minimal('_v1')\n",
    "\n",
    "#Undersample_Data_Thin('_v2')\n",
    "#Undersample_Data_Really_Thin('_v2')\n",
    "#Undersample_Data_Thin_to_Minimal('_v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577bab5",
   "metadata": {},
   "source": [
    "## Undersampling Results\n",
    "- Start with 747,342 samples\n",
    "- Remove 33,776 samples iwth pedestrians to get 713,566 samples\n",
    "- Split 70/30 to have 499,496 samples in training set, 214,070 in test set\n",
    "- In training set, 499,496 samples, 78,926 hospitalized, 420,570 not hospitalized\n",
    "\n",
    "\n",
    "| Feature Set | Random Seed | Tomek Round | # Samples Removed | % Samples Removed |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Hard | 1 | 1 | 17,851 | 3.57 |\n",
    "| Hard | 2 | 1 | 7,794 | 3.56 |\n",
    "| Hard | 1 | 2 | 3,664 | 0.76 |\n",
    "| Hard | 2 | 2 | 3,751 | 0.78 |\n",
    "| Medium | 1 | 1 | 8,839 | 1.77 |\n",
    "| Medium | 2 | 1 | 8.825 | 1.77 |\n",
    "| Medium | 1 | 2 | 1,736 | 0.35 |\n",
    "| Medium | 2 | 2 | 1,656 | 0.34 |\n",
    "| Easy | 1 | 1 | 6 | 0.00 |\n",
    "| Easy | 2 | 1 | 3 | 0.00 |\n",
    "| Easy | 1 | 2 | 0 | 0.00 |\n",
    "| Easy | 2 | 2 | 0 | 0.00 |\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dcfd4d",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf4a61",
   "metadata": {},
   "source": [
    "## Alpha Weighted Binary Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_weighted_binary_crossentropy_with_parameter(alpha = 0.5):\n",
    "    def alpha_weighted_binary_crossentropy(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        weights = tf.where(tf.equal(y_true,1),alpha, 1-alpha)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        loss = keras.backend.mean(product)\n",
    "        return loss\n",
    "    return alpha_weighted_binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_weighted_binary_crossentropy_with_class_weight_parameters(weight_0 = 1.0, weight_1 = 1.0):\n",
    "    # Weights for each class = (nSamples Total)/(2* (nSamples in Class))\n",
    "    def alpha_weighted_binary_crossentropy(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "        weights = tf.where(tf.equal(y_true,1),weight_1, weight_0)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        loss = keras.backend.mean(product)\n",
    "        return loss\n",
    "    return alpha_weighted_binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74dedab",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    # The dataset has  259077  elements.\n",
    "    # The target group has  31891  elements.\n",
    "    # Our target is  12.3095 % of the dataset.\n",
    "    # There are  8.12  negative elements for each positive.    \n",
    "#    p = 8.12\n",
    "    p = 5.94\n",
    "\n",
    "    alpha = (p/(p+1))*1.0\n",
    "\n",
    "    gamma_1 = 0.0 # Must be float for the tf.math.pow() function to work.\n",
    "    gamma_2 = 0.0\n",
    "    y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "    binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "#    print (binary_crossentropy.numpy())\n",
    "    weights = tf.where(tf.equal(y_true,1),alpha, 1-alpha)\n",
    "#    print (weights.numpy())\n",
    "    focal = tf.where(tf.equal(y_true,1), (1.0-y_pred), (y_pred))\n",
    "    power = tf.where(tf.equal(y_true,1), gamma_1, gamma_2)\n",
    "    focal_power = tf.math.pow(focal,power)\n",
    "#    print (focal.numpy())\n",
    "#    print (power.numpy())\n",
    "#    print (focal_power.numpy())\n",
    "    product = tf.multiply(binary_crossentropy, weights)\n",
    "    focal_power_product = tf.multiply(product, focal_power)\n",
    "#    print (focal_power_product.numpy())\n",
    "    loss = keras.backend.mean(focal_power_product)\n",
    "#    print (loss.numpy())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaef565",
   "metadata": {},
   "source": [
    "## Focal Loss with Parameters\n",
    "- Adapted from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_with_parameters(alpha = 0.5, gamma_0=0.0, gamma_1=0.0):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "#        tf.clip_by_value(y_pred, 0.00001, 0.99999) # Make sure we don't blow up the logarithm\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        weights = tf.where(tf.equal(y_true,1),alpha, 1.0-alpha)\n",
    "        focal = tf.where(tf.equal(y_true,1), (1.0-y_pred), (y_pred))\n",
    "        power = tf.where(tf.equal(y_true,0), gamma_0, gamma_1)\n",
    "        focal_power = tf.math.pow(focal,power)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        focal_power_product = tf.multiply(product, focal_power)\n",
    "#        tf.clip_by_value(focal_power_product, 0.00001, 0.99999)\n",
    "        loss = keras.backend.mean(focal_power_product)\n",
    "        if math.isnan(loss):\n",
    "            print ('loss is nan')\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "get_custom_objects().update({'focal_loss_with_parameters': focal_loss_with_parameters()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_with_parameters_2(alpha=.25, gamma=2.0):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c9b6f",
   "metadata": {},
   "source": [
    "## Test Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b909c30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Test_Loss_Functions():\n",
    "    \n",
    "    ### Data as list y_test and y_prob\n",
    "    y_test = [0.0]*500 + [1.0]*500\n",
    "    y_test_binary = [0]*500 + [1]*500\n",
    "#    y_test = [0.0, 1.0]*5\n",
    "#    y_test_binary = [0,1]*5\n",
    "#    y_prob = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 0.999]\n",
    "    y_prob = [random.random() for x in range (1000)]\n",
    "#    print (y_prob)\n",
    "    \n",
    "    ### Data as tensors y_true and y_pred\n",
    "    y_true = np.array(y_test, dtype=np.float32)\n",
    "    y_true = tf.convert_to_tensor(y_true)\n",
    "    y_pred = np.array(y_prob, dtype=np.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "    ####################################################\n",
    "    print ('Test with p==1.0, alpha = 0.5, gamma = 0.0')\n",
    "\n",
    "    ### Calculate binary crossentropy by hand\n",
    "    BCE = [-(y_test[i] * math.log(y_prob[i]) + (1 - y_test[i]) * math.log(1 - y_prob[i])) for i in range (10)]\n",
    "    Class_Weights = [1.0,1.0]\n",
    "    Weights = [Class_Weights[y_test_binary[i]] for i in range(10)]\n",
    "    Product = [BCE[i] * Weights[i] for i in range (10)]\n",
    "    loss = sum(Product)/len(Product)\n",
    "    print (loss, \"  Hand-calculated BCE loss\")\n",
    "    \n",
    "    ### Calculate binary crossentropy like I did in my custom loss functions\n",
    "    binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "#    display(binary_crossentropy.numpy())\n",
    "    loss = keras.backend.mean(binary_crossentropy).numpy()\n",
    "    print (loss, \"  My custom AWBCE function's no-alpha backend\")\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_parameter(alpha = 0.5)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom one-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    # Weights for each class = (nSamples Total)/(2* (nSamples in Class))\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_class_weight_parameters(weight_0 = 1.0, weight_1 = 1.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom two-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate binary crossentropy using Keras's loss function\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    loss = bce(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BCE function\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.5,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.5, 0.0, 0.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=0.0')\n",
    "    \n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with p = 3.0, alpha = 0.75, gamma = 0.0')\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_parameter(alpha = 0.75)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom one-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    # Weights for each class = (nSamples Total)/(2* (nSamples in Class))\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_class_weight_parameters(weight_0 = 2.0/3.0, weight_1 = 2.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom two-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.75,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.75, 0.0, 0.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=0.0')\n",
    "    \n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with alpha = 0.8, gamma = 0.0')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.8,\n",
    "        gamma = 0.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.8, 0.0, 0.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function')\n",
    "\n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with alpha = 0.8, gamma = 2.0')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.8,\n",
    "        gamma = 2.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.8, 2.0, 2.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=2.0')\n",
    "\n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with p = 1.0, alpha = 0.5, gamma = 2.0')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.5,\n",
    "        gamma = 2.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.5, 2.0, 2.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=2.0')\n",
    "\n",
    "    ##################################################################\n",
    "    print ()\n",
    "    print (\"Test Keras's BFC Function with different values of alpha\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.1,\n",
    "        gamma = 0.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.5,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.9,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#Test_Loss_Functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c65a77",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f61f36",
   "metadata": {},
   "source": [
    "## Another Keras Binary Classification Model\n",
    "https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, alpha, gamma, epochs, filename, title):\n",
    "    print ('Keras_Binary_Focal_Crossentropy')\n",
    "    print ('alpha = ', alpha, ', gamma = ', gamma)\n",
    "    print ()\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing=True,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "#        from_logits=False,\n",
    "#        label_smoothing=0.0,\n",
    "#        axis=-1,\n",
    "#        reduction=losses_utils.ReductionV2.AUTO,\n",
    "#        name='binary_focal_crossentropy'\n",
    "    )   \n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "#    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    # Compile model\n",
    "    metrics = [\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "#        F1_Metric,\n",
    "    ]\n",
    "    model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\n",
    "    estimator = KerasClassifier(\n",
    "        model=model, \n",
    "#        random_state=42,\n",
    "        metrics=metrics,\n",
    "        batch_size=128, \n",
    "        verbose=0,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    estimator.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = estimator.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on Test Set\n",
    "    y_proba = estimator.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d06a6c",
   "metadata": {},
   "source": [
    "## Our Binary Focal Crossentropy Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, alpha, gamma_0, gamma_1, epochs, filename, title):\n",
    "    print ('Our_Binary_Focal_Crossentropy')\n",
    "    print ('alpha = ', alpha, ' gamma_0 = ', gamma_0, ', gamma_1 = ', gamma_1)\n",
    "\n",
    "#    alpha_target = r_target/(r_target+1)\n",
    "    loss_function = focal_loss_with_parameters(alpha, gamma_0, gamma_1)\n",
    "#    loss_function = focal_loss_with_parameters_2(alpha_target, gamma)\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "#    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    metrics = [\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "#        F1_Metric,\n",
    "    ]\n",
    "    estimator = KerasClassifier(\n",
    "        model=model, \n",
    "#        random_state=42,\n",
    "        metrics=metrics,\n",
    "        batch_size=128, \n",
    "        verbose=0,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    estimator.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )\n",
    "\n",
    "    # Test on training set for overfit\n",
    "    y_proba = estimator.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    \n",
    "    # Test on Test Set\n",
    "    y_proba = estimator.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91de86f",
   "metadata": {},
   "source": [
    "## AdaBoost Model\n",
    "https://stackoverflow.com/questions/39063676/how-to-boost-a-keras-based-neural-network-using-adaboost\n",
    "- model.predict_proba(X_test) returns two columns, \n",
    "    - the first the probability that the sample is in class 0, \n",
    "    - and the second the probability that the sample is in class 1.\n",
    "    - We just want the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764615d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost(X_train, X_test, y_train, y_test, filename, title):\n",
    "    print ('AdaBoost() ', filename)\n",
    "    model = AdaBoostClassifier(n_estimators=100)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on Test Set\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afd759",
   "metadata": {},
   "source": [
    "### Ensembles of Classifiers\n",
    "https://imbalanced-learn.org/stable/ensemble.html#bagging-classifier\n",
    "\n",
    "with arguments based on the documentation examples\n",
    "\n",
    "https://imbalanced-learn.org/stable/auto_examples/ensemble/plot_comparison_ensemble_classifier.html#sphx-glr-auto-examples-ensemble-plot-comparison-ensemble-classifier-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cffa9",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe617891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging(X_train, X_test, y_train, y_test, filename, title):\n",
    "    print ('Bagging() ', filename)\n",
    "    model = BalancedBaggingClassifier(\n",
    "#        random_state=42\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on Test Set\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "\n",
    "    print ()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421ec7b",
   "metadata": {},
   "source": [
    "## Balanced Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, alpha, filename, title):\n",
    "    print ('Balanced Random Forest Classifier ', filename)\n",
    "    print ('alpha = ', alpha)\n",
    "    print ()\n",
    "    model = BalancedRandomForestClassifier(\n",
    "        bootstrap = True, ccp_alpha = 0.0, criterion = 'gini', \n",
    "        max_depth = None,\n",
    "#        max_depth = 40, \n",
    "        max_features = 'sqrt', \n",
    "        max_leaf_nodes = None,\n",
    "#        max_leaf_nodes = 10000,  \n",
    "        max_samples = None, \n",
    "        min_impurity_decrease = 0.0, \n",
    "        min_samples_leaf = 1, \n",
    "        min_samples_split = 2, \n",
    "        min_weight_fraction_leaf = 0.0, \n",
    "        n_estimators = 100, \n",
    "#        n_estimators = 1000, \n",
    "        n_jobs = None, \n",
    "        oob_score = False, \n",
    "        random_state = None, \n",
    "        replacement = False, \n",
    "        sampling_strategy = 'auto', \n",
    "        verbose = 0, \n",
    "        warm_start = False,\n",
    "        class_weight = {0:1-alpha, 1:alpha}\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    print ()\n",
    "    print ('model.get_params()')\n",
    "    print (model.get_params())\n",
    "    print ()\n",
    "    \n",
    "    print ('[estimator.get_depth() for estimator in model.estimators_]')\n",
    "    print ([estimator.get_depth() for estimator in model.estimators_])\n",
    "    print ()\n",
    "    print ('[estimator.get_n_leaves() for estimator in model.estimators_]')\n",
    "    print ([estimator.get_n_leaves() for estimator in model.estimators_])\n",
    "    print ()\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print ()\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e244a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf675fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8a4b47",
   "metadata": {},
   "source": [
    "## RUSBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240eb43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUSBoost_Classifier(X_train, X_test, y_train, y_test, estimator, filename, title):\n",
    "    print ('RUSBoost Classifier ', filename)\n",
    "    model = RUSBoostClassifier(\n",
    "        n_estimators=1000, \n",
    "        estimator=estimator,\n",
    "        algorithm='SAMME.R', \n",
    "#        random_state=42\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on test data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "\n",
    "    print ()\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb5091",
   "metadata": {},
   "source": [
    "## Easy Ensemble Classifier (Adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db654cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Easy_Ensemble_Classifier(X_train, X_test, y_train, y_test, filename, title):\n",
    "    print ('Easy Ensemble Classifier ', filename)\n",
    "    estimator = AdaBoostClassifier(n_estimators=10)\n",
    "    model = EasyEnsembleClassifier(n_estimators=10, estimator=estimator)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "\n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on test data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592a6f5",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression_Classifier(X_train, X_test, y_train, y_test, alpha, filename, title):\n",
    "    print ('Logistic Regression Classifier ', filename)\n",
    "    model = LogisticRegression(\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)}\n",
    "        class_weight = {0:1-alpha, 1:alpha},\n",
    "        max_iter=1000,\n",
    "#        random_state=42,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "\n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on test data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44982302",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d83bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chart_and_Plots(y_test, y_proba, y_pred, filename, title):\n",
    "    \n",
    "    Analyze_Prediction(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)\n",
    "    \n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.0, 1.0)\n",
    "    Filename = filename + '_Transformed_100'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "\n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.01,0.99)\n",
    "    Filename = filename + '_Transformed_98'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "\n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.025,0.975)\n",
    "    Filename = filename + '_Transformed_95'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "\n",
    "    \n",
    "    print ()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ae0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Transform_y_proba(y_test, y_proba):\n",
    "    print ('Linear_Transform_y_proba()')\n",
    "    print ()\n",
    "    \n",
    "    # I considered two methods.  \n",
    "    # One was to take the medians of the negative and positive classes and transform them to 0.25 and 0.75.\n",
    "    # That didn't always work the way I wanted.  \n",
    "    # Then I tried taking the 0.05 quantile to 0.05 and the 0.95 quantile to 0.95.\n",
    "    \n",
    "#    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "#    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    center = (N_median + P_median)/2\n",
    "#    print ('N_median = %.3f, P_median = %.3f, center = %.3f' % (N_median, P_median, center))\n",
    "#    y_proba = 0.25/(center - N_median) * (y_proba - center) + 0.5\n",
    "\n",
    "    \n",
    "    a = np.quantile(y_proba[np.array(y_test)==0],0.025)\n",
    "    b = np.quantile(y_proba[np.array(y_test)==1],0.975)\n",
    "    print ('a = %.3f, b = %.3f' % (a, b))\n",
    "    y_proba = 1/(b-a) * (y_proba - a)\n",
    "    \n",
    "    y_proba = np.where (y_proba < 0.0, 0.0, y_proba)\n",
    "    y_proba = np.where (y_proba > 1.0, 1.0, y_proba)\n",
    "    y_pred = K.round(y_proba)\n",
    "\n",
    "    print ()\n",
    "    \n",
    "    return y_test, y_proba, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Transform_y_proba_Specified(y_test, y_proba, left, right):\n",
    "    print ('Linear_Transform_y_proba()')\n",
    "    print ()\n",
    "    \n",
    "    # I considered two methods.  \n",
    "    # One was to take the medians of the negative and positive classes and transform them to 0.25 and 0.75.\n",
    "    # That didn't always work the way I wanted.  \n",
    "    # Then I tried taking the 0.05 quantile to 0.05 and the 0.95 quantile to 0.95.\n",
    "    \n",
    "#    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "#    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    center = (N_median + P_median)/2\n",
    "#    print ('N_median = %.3f, P_median = %.3f, center = %.3f' % (N_median, P_median, center))\n",
    "#    y_proba = 0.25/(center - N_median) * (y_proba - center) + 0.5\n",
    "\n",
    "    \n",
    "    a = np.quantile(y_proba[np.array(y_test)==0],left)\n",
    "    b = np.quantile(y_proba[np.array(y_test)==1],right)\n",
    "    print ('a = %.3f, b = %.3f' % (a, b))\n",
    "    y_proba = 1/(b-a) * (y_proba - a)\n",
    "    \n",
    "    y_proba = np.where (y_proba < 0.0, 0.0, y_proba)\n",
    "    y_proba = np.where (y_proba > 1.0, 1.0, y_proba)\n",
    "    y_pred = K.round(y_proba)\n",
    "\n",
    "    print ()\n",
    "    \n",
    "    return y_test, y_proba, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085583e",
   "metadata": {},
   "source": [
    "## Evaluate_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97848247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_Model(y_test, y_proba, y_pred, center, filename):\n",
    "    print ('Evaluate_Model()')\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = [round(x) for x in y_proba]\n",
    "    y_pred = np.array(y_pred)\n",
    "#    print ('np.unique(y_proba) = ', np.unique(y_proba))\n",
    "#    print ('np.unique(y_pred) = ', np.unique(y_pred))\n",
    "    CM = confusion_matrix(y_test, y_pred)\n",
    "#    print(CM)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    \n",
    "    CSV = [[filename, CM[0][0], CM[0][1], CM[1][0], CM[1][1], center, auc_value]]\n",
    "    np.savetxt('./Confusion_Matrices/' + filename + '.csv', \n",
    "        CSV,\n",
    "        delimiter =\", \", \n",
    "        fmt ='% s'\n",
    "              )\n",
    "#    print ()\n",
    "    CM = confusion_matrix(y_test, y_pred, normalize='all')\n",
    "#    print(CM)\n",
    "#    print ()\n",
    "\n",
    "#    y_pred = y_pred.ravel()\n",
    "#    y_test = tf.convert_to_tensor(y_test)\n",
    "#    y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "#    print ('%.3f & Precision \\cr ' %  Precision_Metric(y_test, y_pred).numpy())\n",
    "#    print ('%.3f & Recall \\cr ' %  Recall_Metric(y_test, y_pred).numpy())\n",
    "#    print ('%.3f & F1 \\cr ' %  F1_Metric(y_test, y_pred).numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf928217",
   "metadata": {},
   "source": [
    "## Plot Prediction\n",
    "\n",
    "How to insert a .pgf plot into a \\LaTeX document:\n",
    "\n",
    "\\begin{figure}\n",
    "    \\begin{center}\n",
    "        \\input{Plot.pgf}\n",
    "    \\end{center}\n",
    "    \\caption{A PGF histogram from \\texttt{matplotlib}.}\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3496ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction(y_test, y_proba, filename, title):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 10\n",
    "    bins= [x/n for x in range (0, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "    \n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "    plt.xticks(\n",
    "        ticks = [0, 2.5, 5, 7.5, 10], \n",
    "        labels = ['0.0', '0.25', '0.5', '0.75', '1.0'],\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_Pred.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_Pred.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b634349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction_Zoom(y_test, y_proba, filename, title, left, right):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    B = B[A['HOSPITAL'] > left]\n",
    "    B = B[A['HOSPITAL'] < right]\n",
    "    A = A[A['HOSPITAL'] > left]\n",
    "    A = A[A['HOSPITAL'] < right]\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 10\n",
    "    bins= [left + (right-left)*x/n for x in range (-1, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "\n",
    "    ticks = [0, 2.5, 5, 7.5, 10]\n",
    "    labels = [str(round(left + (right-left) * t/10,2)) for t in ticks]\n",
    "    plt.xticks(\n",
    "        ticks = ticks, \n",
    "        labels = labels,\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_Pred_Zoom.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eda862",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Plot_Prediction_Wide(y_test, y_proba, filename, title):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print ('y_test = ', y_test)\n",
    "#    print ('y_proba = ',y_proba)\n",
    "\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    print (\"A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\")\n",
    "#    display(A)\n",
    "#    print (\"B = pd.DataFrame(y_test, columns=['HOSPITAL'])\")\n",
    "#    display(B)\n",
    "#    print (\"C = A[B['HOSPITAL']==0]\")\n",
    "#    display(C)\n",
    "#    print (\"D = A[B['HOSPITAL']==1]\")\n",
    "#    display(D)\n",
    "    n = 20\n",
    "#    bins= [x/n - 1/(2*n) for x in range (-1, n+3)]\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "#    print ('Bins = ', bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "#    print (\"E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\")\n",
    "#    display(E)\n",
    "#    print (\"F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\")\n",
    "#    display(F)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "#    print (\"G = E.value_counts(sort=False)\")\n",
    "#    display(G)\n",
    "#    print (\"H = F.value_counts(sort=False)\")\n",
    "#    display(H)\n",
    "\n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "#    print (\"G = G/len(y_proba)*100\")\n",
    "#    display(G)\n",
    "#    print (\"H = H/len(y_proba)*100\")\n",
    "#    display(H)\n",
    "\n",
    "    fig = plt.figure(figsize=(4.5,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "    ticks = [n/20*i for i in range (-1,22)]\n",
    "#    print ('ticks = ', ticks)\n",
    "    plt.xticks(\n",
    "        ticks = ticks,\n",
    "        labels = ['','0.0', '', '0.1', '', '0.2', '', '0.3', '', '0.4', '', '0.5', '', '0.6', '', '0.7', '', '0.8', '', '0.9', '', '1.0', ''],\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Wide.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Wide.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_Pred_Wide.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n",
    "\n",
    "def Test_Plot_Prediction_Wide():\n",
    "    \n",
    "    y_proba = (\n",
    "        [0.0]*5 + \n",
    "        [0.0]*0 + \n",
    "        [0.1]*6 + \n",
    "        [0.1]*1 + \n",
    "        [0.2]*7 + \n",
    "        [0.2]*2 + \n",
    "        [0.3]*6 + \n",
    "        [0.3]*1 + \n",
    "        [0.4]*8 + \n",
    "        [0.4]*2 + \n",
    "        [0.5]*9 + \n",
    "        [0.5]*2 + \n",
    "        [0.6]*8 + \n",
    "        [0.6]*2 + \n",
    "        [0.7]*6 + \n",
    "        [0.7]*3 + \n",
    "        [0.8]*5 + \n",
    "        [0.8]*3 + \n",
    "        [0.9]*3 + \n",
    "        [0.9]*2 + \n",
    "        [1.0]*0 + \n",
    "        [1.0]*2 \n",
    "    )\n",
    "    y_test = (\n",
    "        [0]*5 + \n",
    "        [1]*0 + \n",
    "        [0]*6 + \n",
    "        [1]*1 + \n",
    "        [0]*7 + \n",
    "        [1]*2 + \n",
    "        [0]*6 + \n",
    "        [1]*1 + \n",
    "        [0]*8 + \n",
    "        [1]*2 + \n",
    "        [0]*9 + \n",
    "        [1]*2 + \n",
    "        [0]*8 + \n",
    "        [1]*2 + \n",
    "        [0]*6 + \n",
    "        [1]*3 + \n",
    "        [0]*5 + \n",
    "        [1]*3 + \n",
    "        [0]*3 + \n",
    "        [1]*2 + \n",
    "        [0]*0 + \n",
    "        [1]*2 \n",
    "    )\n",
    "    Plot_Prediction_Wide(y_test, y_proba, 'Test', 'Test')\n",
    "    \n",
    "Test_Plot_Prediction_Wide()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e84dc",
   "metadata": {},
   "source": [
    "## Switching between FP/TP and Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee52a47",
   "metadata": {},
   "source": [
    "$$\\text{Precision} = \\frac{TP}{FP+TP}$$\n",
    "\n",
    "$$\\frac{1}{\\text{Precision}} = \\frac{FP+TP}{TP} = \\frac{FP}{TP} + \\frac{TP}{TP} = \\frac{FP}{TP} +  1$$\n",
    "\n",
    "$$\\frac{FP}{TP} + 1 = \\frac{1}{\\text{Precision}}$$\n",
    "\n",
    "$$\\frac{FP}{TP} = \\frac{1}{\\text{Precision}} - 1 = \\frac{1}{\\text{Precision}} - \\frac{\\text{Precision}}{\\text{Precision}}  = \\frac{1 - \\text{Precision}}{\\text{Precision}}$$\n",
    "\n",
    "- In a previous version I had wanted $FP/TP$ to equal either 2.0, 1.0, or 0.5, indicating that we were willing to send 2 unnecessary ambulances for each necessary one, etc.  \n",
    "    - $FP/TP = 2.0$ corresponds to precision = 1/3\n",
    "    - $FP/TP = 1.0$ corresponds to precision = 1/2\n",
    "    - $FP/TP = 0.5$ corresponds to precision = 2/3\n",
    "\n",
    "- Neg/Pos corresponds to marginal precision similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ec6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value_Counts_y_proba(y_proba, filename):\n",
    "#    print ()\n",
    "    print ('Value_Counts_y_proba')\n",
    "#    print (type(y_proba))\n",
    "    Y_proba = pd.Series(y_proba)\n",
    "    A = Y_proba.value_counts().reset_index(drop=True)\n",
    "    n = len(y_proba)\n",
    "    nA = len(A)\n",
    "#    display(Y_proba)\n",
    "#    display(A)\n",
    "    B = A.cumsum()\n",
    "#    display(B)\n",
    "#    print (B[10])\n",
    "#    print ()\n",
    "    cutoff_95 = B.sub(0.95*n).abs().idxmin() + 1\n",
    "    cutoff_90 = B.sub(0.90*n).abs().idxmin() + 1\n",
    "    cutoff_80 = B.sub(0.80*n).abs().idxmin() + 1\n",
    "    print (n, nA)\n",
    "    print (cutoff_95)\n",
    "#    print ()\n",
    "\n",
    "    n100 = min(100, len(B)-1)\n",
    "    n200 = min(200, len(B)-1)\n",
    "#    print ('n200 = ', n200)\n",
    "    f = open('./Analyze_Proba/Value_Counts_y_proba.csv', 'a')\n",
    "    f.write('%s,%d,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f\\n' % (\n",
    "        filename, n, nA, nA/n, \n",
    "        cutoff_95, cutoff_95/n,\n",
    "        cutoff_90, cutoff_90/n,\n",
    "        cutoff_80, cutoff_80/n,\n",
    "        B[10], B[10]/n,\n",
    "        B[20], B[20]/n,\n",
    "        B[n100], B[n100]/n,\n",
    "        B[n200], B[n200]/n,\n",
    "    ))\n",
    "    f.close()\n",
    "    \n",
    "    H = Y_proba.value_counts().head(100)\n",
    "    Filename = './Analyze_Proba/' + filename + '_Value_Counts.csv'\n",
    "    H.to_csv(Filename)\n",
    "    \n",
    "    \n",
    "    print ('Finished')\n",
    "    return 0\n",
    "    \n",
    "def Create_Files_for_Value_Counts_y_proba():\n",
    "    f = open('./Analyze_Proba/Value_Counts_y_proba.csv', 'w')\n",
    "    f.write(\"Filename,n,nUnique,nUnique/n,95%,95%/n,90%,90%/n,80%,80%/n,B[10],B[10]/n,B[20],B[20]/n,B[100],B[100]/n,B[200],B[200]/n,\\n\")\n",
    "    f.close()\n",
    "    \n",
    "#Create_Files_for_Value_Counts_y_proba()\n",
    "    \n",
    "    \n",
    "def Test_Value_Counts_y_proba():\n",
    "    A = [5]*50 + [6]*20 + [i for i in range (10,40)]*2 + [i for i in range (100,400)]\n",
    "    Value_Counts_y_proba(A, 'Test')\n",
    "\n",
    "Test_Value_Counts_y_proba()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de839f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_Prediction(y_test, y_proba, filename, title):\n",
    "    print ('Analyze_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "    Value_Counts_y_proba(y_proba, filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    print ('print (len(A), len(C), len(D), len(C) + len(D))')\n",
    "#    print (len(A), len(C), len(D), len(C) + len(D))\n",
    "\n",
    "    N = len(C)\n",
    "    P = len(D)\n",
    "    \n",
    "    ##### 10 bins\n",
    "    n = 10\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "#    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['mPrec'] = Analyze['Pos']/(Analyze['Pos'] + Analyze['Neg'])\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "#    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['FP/P'] =  Analyze['FP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\hat{p}$'] = (Analyze['TP'] + Analyze['FP'])/len(y_proba)\n",
    "    \n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "#    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['mPrec']=Analyze['mPrec'].apply('{:.2f}'.format)\n",
    "#    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec']=Analyze['Prec'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec']=Analyze['Rec'].apply('{:.2f}'.format)\n",
    "    Analyze['FP/P']=Analyze['FP/P'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\hat{p}$']=Analyze['$\\hat{p}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "#    print ('./Analyze_Proba/' + filename + '_10.tex')\n",
    "#    print (len(y_proba))\n",
    "#    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_10.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_10.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrrrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    ##### 20 bins\n",
    "    n = 20\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "#    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['mPrec'] = Analyze['Pos']/(Analyze['Pos'] + Analyze['Neg'])\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "#    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['FP/P'] =  Analyze['FP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\hat{p}$'] = (Analyze['TP'] + Analyze['FP'])/len(y_proba)\n",
    "    \n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "#    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['mPrec']=Analyze['mPrec'].apply('{:.2f}'.format)\n",
    "#    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec']=Analyze['Prec'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec']=Analyze['Rec'].apply('{:.2f}'.format)\n",
    "    Analyze['FP/P']=Analyze['FP/P'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\hat{p}$']=Analyze['$\\hat{p}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "#    print ('./Analyze_Proba/' + filename + '_10.tex')\n",
    "#    print (len(y_proba))\n",
    "#    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_20.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_20.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrrrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### 100 bins\n",
    "    n = 100\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "#    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['mPrec'] = Analyze['Pos']/(Analyze['Pos'] + Analyze['Neg'])\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "#    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['FP/P'] =  Analyze['FP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\hat{p}$'] = (Analyze['TP'] + Analyze['FP'])/len(y_proba)\n",
    "    \n",
    "    A = Analyze.copy(deep=True)\n",
    "    \n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "#    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['mPrec']=Analyze['mPrec'].apply('{:.2f}'.format)\n",
    "#    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec']=Analyze['Prec'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec']=Analyze['Rec'].apply('{:.2f}'.format)\n",
    "    Analyze['FP/P']=Analyze['FP/P'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\hat{p}$']=Analyze['$\\hat{p}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "#    print ('./Analyze_Proba/' + filename + '_10.tex')\n",
    "#    print (len(y_proba))\n",
    "#    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_100.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_100.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrrrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Append CSV files with results from multiple models\n",
    "    A.set_index('p', inplace=True)\n",
    "    A.insert(0, 'Filename', filename)\n",
    "    \n",
    "    # Remove rows with negligible number of samples\n",
    "    A = A[A['Neg'] >= 20]\n",
    "    A = A[A['Pos'] >= 20]\n",
    "    \n",
    "    \n",
    "    A_closest = A.iloc[(A['mPrec'] - 0.333).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/mPrec_0_333.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['mPrec'] - 0.5).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/mPrec_0_5.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['mPrec'] - 0.667).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/mPrec_0_667.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['Prec'] - 0.333).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_333.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['Prec'] - 0.5).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_5.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['Prec'] - 0.667).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_667.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\hat{p}$'] - 0.05).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/p_hat_0_05.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\hat{p}$'] - 0.10).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/p_hat_0_10.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\hat{p}$'] - 0.15).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/p_hat_0_15.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['FP/P'] - 0.05).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/FP_P_0_05.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    \n",
    "def Create_Files_for_Analyze_Prediction():\n",
    "    f = open('./Analyze_Proba/mPrec_0_5.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/mPrec_0_667.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/mPrec_0_333.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/Prec_0_5.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/Prec_0_667.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/Prec_0_333.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/p_hat_0_05.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/p_hat_0_10.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/p_hat_0_15.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/FP_P_0_05.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,FP/P,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "#Create_Files_for_Analyze_Prediction()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e092785",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Test_Plot_Prediction_Zoom():\n",
    "    print ('Idealized_Results()')\n",
    "    # Set randomness\n",
    "    np.random.seed(42) # NumPy\n",
    "    random.seed(42) # Python\n",
    "    tf.random.set_seed(42) # Tensorflow    \n",
    "\n",
    "    shape, scale = 3.7, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    a = np.random.gamma(shape, scale, 150771)\n",
    "    a = np.where(a>1.0, random.random(), a)\n",
    "    \n",
    "    shape, scale = 3.8, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    b = np.random.gamma(shape, scale, 26621)    \n",
    "    b = np.where(b>1.0, random.random(), b)\n",
    "    b = 1-b\n",
    "    \n",
    "    y_proba = np.concatenate((a,b),axis=0)\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)  \n",
    "    \n",
    "    display(y_proba[:20])\n",
    "    display(y_pred[:20])\n",
    "    \n",
    "    Plot_Prediction(y_test, y_proba, 'Test', 'Test')    \n",
    "    Plot_Prediction_Wide(y_test, y_proba, 'Test', 'Test')    \n",
    "    Plot_Prediction_Zoom(y_test, y_proba, 'Test', 'Test', 0.45, 0.55)\n",
    "    Analyze_Prediction(y_test, y_proba, 'Test', 'Test')    \n",
    "    \n",
    "#Test_Plot_Prediction_Zoom()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a04113",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e088e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(y_test, y_proba, p_values, filename):\n",
    "    print ('ROC()')\n",
    "    print (filename)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    \n",
    "    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    print ('N_median, P_median = ', N_median, P_median)\n",
    "\n",
    "    m = np.quantile(y_proba,0.50)\n",
    "    p = np.quantile(y_proba,0.25)\n",
    "    q = np.quantile(y_proba,0.75)\n",
    "    \n",
    "    Y = []\n",
    "    print ('p_values = ', p_values)\n",
    "    for X in p_values:\n",
    "        difference_array = np.absolute(thresholds-X)\n",
    "        index = difference_array.argmin()\n",
    "        F = fpr[index]\n",
    "        T = tpr[index]\n",
    "        Y.append([X,str(round(X,3)),F,T])\n",
    "    \n",
    "    auc_value = auc(fpr, tpr)\n",
    "    auc_value = round(auc_value,3)\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, color='black', label='AUC={:.3f}'.format(auc_value))\n",
    "    \n",
    "    for y in Y:\n",
    "#        plt.plot([y[2]], [y[3]], marker=\"o\", markersize=20, markeredgecolor=\"white\", markerfacecolor=\"white\")\n",
    "#        plt.annotate(\n",
    "#            y[1], # this is the text\n",
    "#            (y[2], y[3]), # these are the coordinates to position the label\n",
    "#            ha='center' # horizontal alignment can be left, right or center\n",
    "#        )\n",
    "        plt.text(\n",
    "            y[2], y[3], # these are the coordinates to position the label\n",
    "            y[1], # this is the text\n",
    "            backgroundcolor='white', # horizontal alignment can be left, right or center\n",
    "            bbox=dict(facecolor='white', edgecolor='none', boxstyle='square,pad=0.3')\n",
    "        )\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "#    plt.title('ROC with AUC {:.3f}'.format(auc_value))\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('./Images/' + filename + '_ROC.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_ROC.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_ROC.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n",
    "    return 0\n",
    "\n",
    "def Test_ROC():\n",
    "    y_test = [0,0,0,0,0,1]*10000\n",
    "#    y_proba = [abs(0.45 - y)+round(0.45*random.random(),2) for y in y_test]\n",
    "    y_proba = [abs(0.45 - y)+round(0.45*random.normalvariate(mu=0.2, sigma=0.2),3) for y in y_test]\n",
    "#    random.normalvariate(mu=0.0, sigma=1.0)\n",
    "    y_test = np.array(y_test)\n",
    "    y_proba = np.array(y_proba)\n",
    "    print (y_test)\n",
    "    print (y_proba)\n",
    "    ROC(y_test, y_proba, [0.5], \"tmp\")\n",
    "    \n",
    "#Test_ROC()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49200a4f",
   "metadata": {},
   "source": [
    "## 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ee811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Five_Fold_Cross_Validation(data, model, filename, title):\n",
    "    print ()\n",
    "    print ('------------------------')\n",
    "    print ()\n",
    "    print (filename)\n",
    "    print ()\n",
    "    \n",
    "    target = 'HOSPITAL'\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    target_column = data.loc[:,target]\n",
    "    y_test = []\n",
    "    y_proba = []\n",
    "    y_pred = []\n",
    "    \n",
    "    iteration = 0\n",
    "    for train_index, test_index in skf.split(data, target_column):\n",
    "        print ('K-fold iteration = ', iteration)\n",
    "        iteration += 1\n",
    "        \n",
    "#        print ('len(train_index) = ', len(train_index))\n",
    "#        print (train_index)\n",
    "#        print ('len(test_index) = ', len(test_index))\n",
    "#        print (test_index)\n",
    "        \n",
    "        train_fold = data.iloc[train_index]\n",
    "#        print ()\n",
    "#        print ('train_fold')\n",
    "#        display(train_fold)\n",
    "        \n",
    "        test_fold = data.iloc[test_index]\n",
    "#        print ()\n",
    "#        print ('test_fold')\n",
    "#        display(test_fold)\n",
    "#        print ('type(test_fold) = ', type(test_fold))\n",
    "        \n",
    "        \n",
    "        X_train_fold = train_fold.drop(columns=[target])\n",
    "        X_test_fold = test_fold.drop(columns=[target])\n",
    "        y_train_fold = train_fold[target].squeeze()        \n",
    "        y_test_fold = test_fold[target].squeeze()\n",
    "#        print ('type(y_test_fold) = ', type(y_test_fold))\n",
    "        \n",
    "#        print ()\n",
    "        model.fit(X_train_fold, y_train_fold.values.ravel())\n",
    "        y_proba_fold = model.predict_proba(X_test_fold)\n",
    "        y_proba_fold = [x[1] for x in y_proba_fold]\n",
    "        y_pred_fold = list(np.around(np.array(y_proba_fold),0))\n",
    "        \n",
    "        ###\n",
    "#        print ('X_train_fold')\n",
    "#        display(X_train_fold)\n",
    "#        print ('y_train_fold')\n",
    "#        display(y_train_fold)\n",
    "#        print ('y_train_fold.value_counts()')\n",
    "#        display(y_train_fold.value_counts())\n",
    "#        print ('y_proba_fold')\n",
    "#        print (y_proba_fold)\n",
    "#        ###\n",
    "#        \n",
    "        y_test = y_test + y_test_fold.to_list()\n",
    "        y_proba = y_proba + y_proba_fold\n",
    "#        print ('len(y_proba) = ', len(y_proba))\n",
    "        y_pred = y_pred + y_pred_fold\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    y_proba = np.array(y_proba)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    \n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    \n",
    "    print ()\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2658775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def BRFC_5_Fold(data, target, alpha, filename):\n",
    "     \n",
    "    title = ''\n",
    "    model = BalancedRandomForestClassifier(\n",
    "        bootstrap = True, ccp_alpha = 0.0, criterion = 'gini', \n",
    "        max_depth = None,\n",
    "#        max_depth = 40, \n",
    "        max_features = 'sqrt', \n",
    "        max_leaf_nodes = None,\n",
    "#        max_leaf_nodes = 10000,  \n",
    "        max_samples = None, \n",
    "        min_impurity_decrease = 0.0, \n",
    "        min_samples_leaf = 1, \n",
    "        min_samples_split = 2, \n",
    "        min_weight_fraction_leaf = 0.0, \n",
    "        n_estimators = 100, \n",
    "#        n_estimators = 1000, \n",
    "        n_jobs = None, \n",
    "        oob_score = False, \n",
    "        random_state = None, \n",
    "        replacement = False, \n",
    "        sampling_strategy = 'auto', \n",
    "        verbose = 0, \n",
    "        warm_start = False,\n",
    "        class_weight = {0:1-alpha, 1:alpha}\n",
    "    )\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost_5_Fold(data, target, filename):\n",
    "    title = ''\n",
    "    model = AdaBoostClassifier(n_estimators=100)\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855780c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUSBoost_5_Fold(data, target, filename):\n",
    "    title = ''\n",
    "    estimator = DecisionTreeClassifier(\n",
    "        max_depth=1,\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )    \n",
    "    model = RUSBoostClassifier(\n",
    "        n_estimators=1000, \n",
    "        estimator=estimator,\n",
    "        algorithm='SAMME.R', \n",
    "#        random_state=42\n",
    "    )\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8dac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BalancedBagging_5_Fold(data, target, filename):\n",
    "    title = ''\n",
    "    model = BalancedBaggingClassifier(\n",
    "#        random_state=42\n",
    "    )\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff286004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EasyEnsemble_5_Fold(data, target, filename):\n",
    "    title = ''\n",
    "    estimator = AdaBoostClassifier(n_estimators=10)\n",
    "    model = EasyEnsembleClassifier(n_estimators=10, estimator=estimator)\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_5_Fold(data, target, alpha, filename):\n",
    "    title = ''\n",
    "    model = LogisticRegression(\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)}\n",
    "        class_weight = {0:1-alpha, 1:alpha},\n",
    "        max_iter=1000,\n",
    "#        random_state=42,\n",
    "    )\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KBFC_5_Fold(data, target, alpha, gamma, filename):\n",
    "    print ()\n",
    "    print ('------------------------')\n",
    "    print ()\n",
    "    print (filename)\n",
    "    print ()\n",
    "    \n",
    "    target = 'HOSPITAL'\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    target_column = data.loc[:,target]\n",
    "    y_test = []\n",
    "    y_proba = []\n",
    "    y_pred = []\n",
    "    \n",
    "    iteration = 0\n",
    "    for train_index, test_index in skf.split(data, target_column):\n",
    "        print ('K-fold iteration = ', iteration)\n",
    "        iteration += 1\n",
    "        \n",
    "#        print ('len(train_index) = ', len(train_index))\n",
    "#        print (train_index)\n",
    "#        print ('len(test_index) = ', len(test_index))\n",
    "#        print (test_index)\n",
    "        \n",
    "        train_fold = data.iloc[train_index]\n",
    "#        print ()\n",
    "#        print ('train_fold')\n",
    "#        display(train_fold)\n",
    "        \n",
    "        test_fold = data.iloc[test_index]\n",
    "#        print ()\n",
    "#        print ('test_fold')\n",
    "#        display(test_fold)\n",
    "#        print ('type(test_fold) = ', type(test_fold))\n",
    "        \n",
    "        \n",
    "        X_train_fold = train_fold.drop(columns=[target])\n",
    "        X_test_fold = test_fold.drop(columns=[target])\n",
    "        y_train_fold = train_fold[target].squeeze()        \n",
    "        y_test_fold = test_fold[target].squeeze()\n",
    "#        print ('type(y_test_fold) = ', type(y_test_fold))\n",
    "\n",
    "#        print ('len(X_train_fold) = ', len(X_train_fold))\n",
    "#        print ('len(X_test_fold) = ', len(X_test_fold))\n",
    "#        print ('len(y_train_fold) = ', len(y_train_fold))\n",
    "#        print ('len(y_test_fold) = ', len(y_test_fold))\n",
    "#        print ()\n",
    "        \n",
    "#        print ()\n",
    "\n",
    "        loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "            apply_class_balancing=True,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "    #        from_logits=False,\n",
    "    #        label_smoothing=0.0,\n",
    "    #        axis=-1,\n",
    "    #        reduction=losses_utils.ReductionV2.AUTO,\n",
    "    #        name='binary_focal_crossentropy'\n",
    "        )   \n",
    "    \n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        print ('data.shape = ', data.shape, data.shape[-1])\n",
    "        model.add(Dense(60, input_shape=(data.shape[-1]-1,), activation='relu'))\n",
    "#        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))    \n",
    "        # Compile model\n",
    "        metrics = [\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"recall\"),\n",
    "    #        F1_Metric,\n",
    "        ]\n",
    "        model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\n",
    "        estimator = KerasClassifier(\n",
    "            model=model, \n",
    "    #        random_state=42,\n",
    "            metrics=metrics,\n",
    "            batch_size=128, \n",
    "            verbose=0,\n",
    "            epochs=20,\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "        estimator.fit(X_train_fold, y_train_fold.values.ravel())\n",
    "        y_proba_fold = estimator.predict_proba(X_test_fold)\n",
    "        y_proba_fold = [x[1] for x in y_proba_fold]\n",
    "        y_pred_fold = list(np.around(np.array(y_proba_fold),0))\n",
    "        \n",
    "        ###\n",
    "#        print ('X_train_fold')\n",
    "#        display(X_train_fold.head())\n",
    "#        print ('y_train_fold')\n",
    "#        display(y_train_fold.head())\n",
    "#        print ('y_train_fold.value_counts()')\n",
    "#        display(y_train_fold.value_counts())\n",
    "#        print ('X_test_fold')\n",
    "#        display(X_test_fold.head())\n",
    "#        print ('y_test_fold')\n",
    "#        display(y_test_fold.head())\n",
    "#        print ('y_test_fold.value_counts()')\n",
    "#        display(y_test_fold.value_counts())\n",
    "        print ('y_proba_fold')\n",
    "        print (y_proba_fold[:10])\n",
    "#        ###\n",
    "#        \n",
    "        y_test = y_test + y_test_fold.to_list()\n",
    "        y_proba = y_proba + y_proba_fold\n",
    "#        print ('len(y_proba) = ', len(y_proba))\n",
    "        y_pred = y_pred + y_pred_fold\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    y_proba = np.array(y_proba)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    \n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', '')\n",
    "    \n",
    "    \n",
    "    print ()\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cf7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OBFC_5_Fold(data, target, alpha, gamma_0, gamma_1, filename):\n",
    "    print ()\n",
    "    print ('------------------------')\n",
    "    print ()\n",
    "    print (filename)\n",
    "    print ()\n",
    "    \n",
    "    target = 'HOSPITAL'\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    target_column = data.loc[:,target]\n",
    "    y_test = []\n",
    "    y_proba = []\n",
    "    y_pred = []\n",
    "    \n",
    "    iteration = 0\n",
    "    for train_index, test_index in skf.split(data, target_column):\n",
    "        print ('K-fold iteration = ', iteration)\n",
    "        iteration += 1\n",
    "        \n",
    "#        print ('len(train_index) = ', len(train_index))\n",
    "#        print (train_index)\n",
    "#        print ('len(test_index) = ', len(test_index))\n",
    "#        print (test_index)\n",
    "        \n",
    "        train_fold = data.iloc[train_index]\n",
    "#        print ()\n",
    "#        print ('train_fold')\n",
    "#        display(train_fold)\n",
    "        \n",
    "        test_fold = data.iloc[test_index]\n",
    "#        print ()\n",
    "#        print ('test_fold')\n",
    "#        display(test_fold)\n",
    "#        print ('type(test_fold) = ', type(test_fold))\n",
    "        \n",
    "        \n",
    "        X_train_fold = train_fold.drop(columns=[target])\n",
    "        X_test_fold = test_fold.drop(columns=[target])\n",
    "        y_train_fold = train_fold[target].squeeze()        \n",
    "        y_test_fold = test_fold[target].squeeze()\n",
    "#        print ('type(y_test_fold) = ', type(y_test_fold))\n",
    "\n",
    "#        print ('len(X_train_fold) = ', len(X_train_fold))\n",
    "#        print ('len(X_test_fold) = ', len(X_test_fold))\n",
    "#        print ('len(y_train_fold) = ', len(y_train_fold))\n",
    "#        print ('len(y_test_fold) = ', len(y_test_fold))\n",
    "#        print ()\n",
    "        \n",
    "#        print ()\n",
    "\n",
    "        loss_function = focal_loss_with_parameters(alpha, gamma_0, gamma_1)    \n",
    "    \n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        print ('data.shape = ', data.shape, data.shape[-1])\n",
    "        model.add(Dense(60, input_shape=(data.shape[-1]-1,), activation='relu'))\n",
    "#        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))    \n",
    "        # Compile model\n",
    "        metrics = [\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"recall\"),\n",
    "    #        F1_Metric,\n",
    "        ]\n",
    "        model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\n",
    "        estimator = KerasClassifier(\n",
    "            model=model, \n",
    "    #        random_state=42,\n",
    "            metrics=metrics,\n",
    "            batch_size=128, \n",
    "            verbose=0,\n",
    "            epochs=20,\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "        estimator.fit(X_train_fold, y_train_fold.values.ravel())\n",
    "        y_proba_fold = estimator.predict_proba(X_test_fold)\n",
    "        y_proba_fold = [x[1] for x in y_proba_fold]\n",
    "        y_pred_fold = list(np.around(np.array(y_proba_fold),0))\n",
    "        \n",
    "        ###\n",
    "#        print ('X_train_fold')\n",
    "#        display(X_train_fold.head())\n",
    "#        print ('y_train_fold')\n",
    "#        display(y_train_fold.head())\n",
    "#        print ('y_train_fold.value_counts()')\n",
    "#        display(y_train_fold.value_counts())\n",
    "#        print ('X_test_fold')\n",
    "#        display(X_test_fold.head())\n",
    "#        print ('y_test_fold')\n",
    "#        display(y_test_fold.head())\n",
    "#        print ('y_test_fold.value_counts()')\n",
    "#        display(y_test_fold.value_counts())\n",
    "        print ('y_proba_fold')\n",
    "        print (y_proba_fold[:10])\n",
    "#        ###\n",
    "#        \n",
    "        y_test = y_test + y_test_fold.to_list()\n",
    "        y_proba = y_proba + y_proba_fold\n",
    "#        print ('len(y_proba) = ', len(y_proba))\n",
    "        y_pred = y_pred + y_pred_fold\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    y_proba = np.array(y_proba)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    \n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', '')\n",
    "    \n",
    "    \n",
    "    print ()\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399576a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72178501",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Run_with_Hard_Features():\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_Features(data)\n",
    "    write_filename_features = '_Hard'\n",
    "    data = Get_Dummies(data, target)\n",
    "    \n",
    "    y = data[target]\n",
    "    N = len(y)\n",
    "    n = len(y[y==1])\n",
    "    p = (N-n)/n\n",
    "    alpha_balanced = p/(p+1)\n",
    "    print ('p = ', p)\n",
    "    print ('alpha_balanced = ', alpha_balanced)    \n",
    "\n",
    "    alpha = 0.5\n",
    "    filename = 'BRFC_5_Fold_alpha_0_5' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    alpha = alpha_balanced\n",
    "    filename = 'BRFC_5_Fold_alpha_balanced' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    filename = 'LogReg_5_Fold_alpha_0_5' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    filename = 'LogReg_5_Fold_alpha_balanced' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    AdaBoost_5_Fold(data, target, 'AdaBoost_5_Fold' + write_filename_features)\n",
    "    BalancedBagging_5_Fold(data, target, 'BalBag_5_Fold' + write_filename_features)\n",
    "    EasyEnsemble_5_Fold(data, target, 'EEC_5_Fold' + write_filename_features)\n",
    "    RUSBoost_5_Fold(data, target, 'RUSBoost_5_Fold' + write_filename_features)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_5_Fold_alpha_0_5_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_5_Fold_alpha_balanced_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = 0.5\n",
    "    gamma = 1.0\n",
    "    filename = 'KBFC_5_Fold_alpha_0_5_gamma_1_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = 0.5\n",
    "    gamma = 2.0\n",
    "    filename = 'KBFC_5_Fold_alpha_0_5_gamma_2_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = 0.5\n",
    "    gamma_0 = 0.0\n",
    "    gamma_1 = 1.0\n",
    "    filename = 'OBFC_5_Fold_alpha_0_5_gamma_0_0_5_gamma_1_1_0' + write_filename_features\n",
    "    OBFC_5_Fold(data, target, alpha, gamma_0, gamma_1, filename)\n",
    "        \n",
    "    alpha = 0.5\n",
    "    gamma_0 = 0.0\n",
    "    gamma_1 = 2.0\n",
    "    filename = 'OBFC_5_Fold_alpha_0_5_gamma_0_0_5_gamma_1_2_0' + write_filename_features\n",
    "    OBFC_5_Fold(data, target, alpha, gamma_0, gamma_1, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbf6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_with_Medium_Features():\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Really_Thin_Features(data)\n",
    "    write_filename_features = '_Medium'\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    y = data[target]\n",
    "    N = len(y)\n",
    "    n = len(y[y==1])\n",
    "    p = (N-n)/n\n",
    "    alpha_balanced = p/(p+1)\n",
    "    print ('p = ', p)\n",
    "    print ('alpha_balanced = ', alpha_balanced)    \n",
    "\n",
    "    alpha = 0.5\n",
    "    filename = 'BRFC_5_Fold_alpha_0_5' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "    alpha = alpha_balanced\n",
    "    filename = 'BRFC_5_Fold_alpha_balanced' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    filename = 'LogReg_5_Fold_alpha_0_5' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    filename = 'LogReg_5_Fold_alpha_balanced' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    AdaBoost_5_Fold(data, target, 'AdaBoost_5_Fold' + write_filename_features)\n",
    "    BalancedBagging_5_Fold(data, target, 'BalBag_5_Fold' + write_filename_features)\n",
    "    EasyEnsemble_5_Fold(data, target, 'EEC_5_Fold' + write_filename_features)\n",
    "    RUSBoost_5_Fold(data, target, 'RUSBoost_5_Fold' + write_filename_features)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_5_Fold_alpha_0_5_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_5_Fold_alpha_balanced_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_with_Easy_Features():\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_to_Minimal_Features(data)\n",
    "    write_filename_features = '_Easy'\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    y = data[target]\n",
    "    N = len(y)\n",
    "    n = len(y[y==1])\n",
    "    p = (N-n)/n\n",
    "    alpha_balanced = p/(p+1)\n",
    "    print ('p = ', p)\n",
    "    print ('alpha_balanced = ', alpha_balanced)    \n",
    "\n",
    "    alpha = 0.5\n",
    "    filename = 'BRFC_5_Fold_alpha_0_5' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "    alpha = alpha_balanced\n",
    "    filename = 'BRFC_5_Fold_alpha_balanced' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    filename = 'LogReg_5_Fold_alpha_0_5' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    filename = 'LogReg_5_Fold_alpha_balanced' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    AdaBoost_5_Fold(data, target, 'AdaBoost_5_Fold' + write_filename_features)\n",
    "    BalancedBagging_5_Fold(data, target, 'BalBag_5_Fold' + write_filename_features)\n",
    "    EasyEnsemble_5_Fold(data, target, 'EEC_5_Fold' + write_filename_features)\n",
    "    RUSBoost_5_Fold(data, target, 'RUSBoost_5_Fold' + write_filename_features)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_5_Fold_alpha_0_5_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_5_Fold_alpha_balanced_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7623de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create_Files_for_Value_Counts_y_proba()\n",
    "#Create_Files_for_Analyze_Prediction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e23c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Run_with_Hard_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb76860",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Run_with_Medium_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534ca51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Run_with_Easy_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb852de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
