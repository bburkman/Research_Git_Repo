{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f1ccbc",
   "metadata": {},
   "source": [
    "This was an attempt to use a Tensorflow model, but \n",
    "- I don't understand enough of what's happening at each stage of the tensor stuff, and I can't figure out how to print it, so I can't check that it's doing what I want it to do, and \n",
    "- The Tensorflow stuff is too opaque for me to use many of the imbalanced data things.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb9a0ec",
   "metadata": {},
   "source": [
    "- The Tensorflow model is an adaptation of the Keras example, Structured data classification from scratch, https://keras.io/examples/structured_data/structured_data_classification_from_scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767154e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e1d2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7d47d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b4fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install Packages\n",
      "Python version: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]\n",
      "NumPy version: 1.24.0\n",
      "SciPy version:  1.9.3\n",
      "TensorFlow version:  2.10.0\n",
      "Keras version:  2.10.0\n",
      "Pandas version:  1.5.2\n",
      "Seaborn version: 0.12.1\n",
      "SciKit-Learn version: 1.2.0\n",
      "Imbalanced-Learn version: 0.10.0\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pydot in /opt/homebrew/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/homebrew/lib/python3.9/site-packages (from pydot) (3.0.9)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mFinished Installing Packages\n"
     ]
    }
   ],
   "source": [
    "print ('Install Packages')\n",
    "\n",
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "#from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import scipy as sc\n",
    "print ('SciPy version:  {}'.format(sc.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print ('TensorFlow version:  {}'.format(tf.__version__))\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "from tensorflow import keras\n",
    "print ('Keras version:  {}'.format(keras.__version__))\n",
    "\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "from keras.layers import IntegerLookup\n",
    "from keras.layers import Normalization\n",
    "from keras.layers import StringLookup\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import seaborn as sns\n",
    "print ('Seaborn version: {}'.format(sns.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Library for reading Microsoft Access files\n",
    "#import pandas_access as mdb\n",
    "\n",
    "import sklearn\n",
    "print ('SciKit-Learn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import imblearn\n",
    "print ('Imbalanced-Learn version: {}'.format(imblearn.__version__))\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "!pip install pydot\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "np.random.seed(42) # NumPy\n",
    "random.seed(42) # Python\n",
    "tf.random.set_seed(42) # Tensorflow\n",
    "\n",
    "print ('Finished Installing Packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437f109",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919fb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data():\n",
    "    print ('Get_Data()')\n",
    "    data = pd.read_csv(\n",
    "        '../../Big_Files/CRSS_Imputed_All_12_22_22.csv',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Get_Data()')\n",
    "    print ()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b900000",
   "metadata": {},
   "source": [
    "## Thin Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a0c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_Features(data):\n",
    "    print ('Thin_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "        'LGT_COND',\n",
    "        'MONTH',\n",
    "        'PEDS',\n",
    "        'PERMVIT',\n",
    "        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "        'RELJCT2',\n",
    "        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "        'VE_FORMS',\n",
    "        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "        'BODY_TYP',\n",
    "        'BUS_USE',\n",
    "        'EMER_USE',\n",
    "        'MAKE',\n",
    "        'MOD_YEAR',\n",
    "        'MODEL',\n",
    "        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "        'LOCATION',\n",
    "        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "        'VEH_AGE',\n",
    "    ]\n",
    "    \n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52366e1a",
   "metadata": {},
   "source": [
    "## Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47cefa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Get_Dummies\n",
      "Get_Dummies\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_a</th>\n",
       "      <th>A_b</th>\n",
       "      <th>B_a</th>\n",
       "      <th>B_b</th>\n",
       "      <th>B_c</th>\n",
       "      <th>C_1</th>\n",
       "      <th>C_2</th>\n",
       "      <th>C_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_a  A_b  B_a  B_b  B_c  C_1  C_2  C_3\n",
       "0    1    0    0    1    0    1    0    0\n",
       "1    0    1    1    0    0    0    1    0\n",
       "2    1    0    0    0    1    0    0    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Get_Dummies(data):\n",
    "    print ('Get_Dummies')\n",
    "    data = data.astype('category')\n",
    "    data_Dummies = pd.get_dummies(data, prefix = data.columns)\n",
    "#    for feature in data_Dummies:\n",
    "#        print (feature)\n",
    "    print ()\n",
    "\n",
    "    return data_Dummies\n",
    "\n",
    "def Test_Get_Dummies():\n",
    "    print ('Test_Get_Dummies')\n",
    "    A = pd.DataFrame({\n",
    "        'A': ['a', 'b', 'a'], \n",
    "        'B': ['b', 'a', 'c'], \n",
    "        'C': [1, 2, 3]})\n",
    "    C = Get_Dummies(A)\n",
    "    display(C)\n",
    "    print ()\n",
    "\n",
    "Test_Get_Dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d43d6e",
   "metadata": {},
   "source": [
    "## Test-Train Split\n",
    "- We're using sklearn's train_test_split rather than Pandas's sample because the former has a 'stratify' option that will put the same proportion of HOSPITAL==1 into each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490cfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Data(data, target, test_size):\n",
    "    print ('Split_Data()')\n",
    "    X = data.drop(columns=[target])\n",
    "    y = data[target]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    a = y_train[y_train==1].shape[0]\n",
    "    b = y_test[y_test==1].shape[0]\n",
    "    print (\n",
    "        x_train.shape, \n",
    "        y_train.shape, a, round((a/(a+b)*100),2), '%')\n",
    "    print (\n",
    "        x_test.shape, \n",
    "        y_test.shape, b, round((b/(a+b)*100),2), '%'\n",
    "    )\n",
    "    print ()\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459da657",
   "metadata": {},
   "source": [
    "## Convert from Pandas Dataframe to Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2eeef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Dataframe_to_Dataset()\n",
      "Dataframe_to_Dataset()\n",
      "ds.shape() =  4\n",
      "ds.shape() =  4\n",
      "\n",
      "[({'A': 1, 'B': 5.0}, array([1])), ({'A': 2, 'B': 6.0}, array([2])), ({'A': 3, 'B': 7.0}, array([3])), ({'A': 3, 'B': 7.0}, array([3]))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Dataframe_to_Dataset(dataframe, labels):\n",
    "    print ('Dataframe_to_Dataset()')\n",
    "#    dataframe = dataframe.astype('int64')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    print ('ds.shape() = ', len(ds))\n",
    "#    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "#    ds = ds.shuffle(buffer_size=1000)\n",
    "    print ('ds.shape() = ', len(ds))\n",
    "    print ()\n",
    "    \n",
    "    return dataframe, ds\n",
    "\n",
    "def Test_Dataframe_to_Dataset():\n",
    "    print ('Test_Dataframe_to_Dataset()')\n",
    "    dataframe = pd.DataFrame({\n",
    "        'A': [1,2,3,3], \n",
    "        'B': [5,6.0,7,7], \n",
    "    })\n",
    "    labels = pd.DataFrame({        \n",
    "        'C': [1, 2, 3, 3]\n",
    "    })\n",
    "    dataframe, D = Dataframe_to_Dataset(dataframe, labels)\n",
    "    print (list(D.as_numpy_iterator()))\n",
    "    print ()\n",
    "\n",
    "Test_Dataframe_to_Dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ea7a0",
   "metadata": {},
   "source": [
    "## Batch the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b0cee62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Batch()\n",
      "Batch()\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),\n",
       " array([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,\n",
       "        49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]),\n",
       " array([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n",
       "        81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]),\n",
       " array([96, 97, 98, 99])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Batch(ds):\n",
    "    print ('Batch()')\n",
    "    ds = ds.batch(32)\n",
    "    print ()\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def Test_Batch():\n",
    "    print ('Test_Batch()')\n",
    "    dataset = tf.data.Dataset.range(100)\n",
    "    dataset = Batch(dataset)\n",
    "    display (list(dataset.as_numpy_iterator()))\n",
    "    print ()\n",
    "\n",
    "Test_Batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bc899",
   "metadata": {},
   "source": [
    "## Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19f3aed8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_integer_dataset(data):\n",
    "    # Adapted from https://www.tensorflow.org/guide/keras/preprocessing_layers\n",
    "    lookup = layers.IntegerLookup(output_mode=\"one_hot\")\n",
    "    encoded_data = lookup.adapt(data)\n",
    "    \n",
    "    return encoded_data\n",
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    print ('encode_numerical_feature()')\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    print ()\n",
    "    \n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    print ('encode_categorical_feature(%s)' % name)\n",
    "    \n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    print ('feature_ds = dataset.map(lambda x, y: x[name])')\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "#    print ('feature_ds')\n",
    "#    print (feature_ds)\n",
    "    print ('list(feature_ds.as_numpy_iterator())')\n",
    "    for x in list(feature_ds.as_numpy_iterator())[:10]:\n",
    "        print (x)\n",
    "    print ()\n",
    "    \n",
    "    print ('feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))')\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    print ('list(feature_ds.as_numpy_iterator())')\n",
    "    for x in list(feature_ds.as_numpy_iterator())[:2]:\n",
    "        for y in x:\n",
    "            print (y, end=', ')\n",
    "    print ()\n",
    "    \n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    print ('lookup.adapt(feature_ds)')\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    print ('list(feature_ds.as_numpy_iterator())')\n",
    "    for x in list(feature_ds.as_numpy_iterator())[:2]:\n",
    "        for y in x:\n",
    "            print (y, end=', ')\n",
    "    print ()\n",
    "    \n",
    "    \n",
    "    # Turn the string input into integer indices\n",
    "    \n",
    "    print ('encoded_feature = lookup(feature)')\n",
    "    encoded_feature = lookup(feature)\n",
    "    print ('encoded_feature')\n",
    "    print (encoded_feature)\n",
    "    print ('list(encoded_feature.as_numpy_iterator())')\n",
    "    \n",
    "#    the_np = encoded_feature.eval(session=K.get_session())\n",
    "#    display(the_np)\n",
    "#    for x in list(encoded_feature.as_numpy_iterator())[:2]:\n",
    "#        print (x)\n",
    "#        for y in x:\n",
    "#            print (y, end=', ')\n",
    "    print ()\n",
    "    \n",
    "#    print ('encoded_feature = tf.keras.backend.print_tensor(encoded_feature)')\n",
    "#    display(tf.keras.backend.print_tensor(encoded_feature))\n",
    "#    tf.keras.backend.eval(encoded_feature)\n",
    "    \n",
    "    print ('end encode_categorical_feature()')\n",
    "    print ()\n",
    "\n",
    "    return encoded_feature\n",
    "\n",
    "def Understand_Encode_Features():\n",
    "    print ('Understand_Encode_Features()')\n",
    "    dataframe = pd.DataFrame({\n",
    "        'A': [1,2,3], \n",
    "        'B': ['a', 'b', 'c'], \n",
    "    })\n",
    "    labels = pd.DataFrame({        \n",
    "        'C': [0,1,0]\n",
    "    })\n",
    "    display(dataframe)\n",
    "    display(labels)\n",
    "    print ()\n",
    "    X = Dataframe_to_Dataset(dataframe, labels)\n",
    "    print ('list(X.as_numpy_iterator())')\n",
    "    print (list(X.as_numpy_iterator()))\n",
    "    \n",
    "    A = keras.Input(shape=(1,), name=\"A\", dtype=\"int64\")\n",
    "    A_encoded = encode_categorical_feature(A, \"A\", X, False)\n",
    "    print ('A_encoded')\n",
    "    print (A_encoded)\n",
    "\n",
    "    B = keras.Input(shape=(1,), name=\"B\", dtype=\"int64\")\n",
    "    B_encoded = encode_categorical_feature(B, \"B\", X, True) \n",
    "\n",
    "    all_inputs = [A,B]\n",
    "    all_features = layers.concatenate([A_encoded,B_encoded])\n",
    "\n",
    "    x = layers.Dense(32, activation=\"relu\")(all_features)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(all_inputs, output)\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    display(model.summary())\n",
    "    print (model.summary())\n",
    "    print ()\n",
    "\n",
    "#Understand_Encode_Features()\n",
    "\n",
    "def Understand_Encode_Features_2():\n",
    "    print ('Understand_Encode_Features_2()')\n",
    "    dataframe = pd.DataFrame({\n",
    "        'A': [1,2,3], \n",
    "        'B': [4, 5, 6], \n",
    "    })\n",
    "    labels = pd.DataFrame({        \n",
    "        'C': [0,1,0]\n",
    "    })\n",
    "    dataframe, dataset = Dataframe_to_Dataset(dataframe, labels)\n",
    "    display(dataframe)\n",
    "    display(labels)\n",
    "    print ()\n",
    "    print ('list(dataset.as_numpy_iterator())')\n",
    "    print (list(dataset.as_numpy_iterator()))\n",
    "    \n",
    "    All_Inputs, All_Features = Encode_Features(dataframe, dataset)\n",
    "    print (All_Inputs)\n",
    "    print (All_Features)\n",
    "    \n",
    "    \n",
    "def Encode_Features(df, ds):\n",
    "    print ('Encode_Features()')\n",
    "    Inputs = list(df.columns)\n",
    "#    print ('display(Inputs)')\n",
    "#    display(Inputs)\n",
    "    All_Inputs = []\n",
    "    Features = []\n",
    "    for i, input in enumerate (Inputs):\n",
    "        print ('display(df[input].value_counts())')\n",
    "        display(df[input].value_counts())\n",
    "        print ()\n",
    "        locals()[input] = keras.Input(shape=(1,), name=input, dtype=\"int64\")\n",
    "        locals()[input + '_encoded'] = encode_categorical_feature(\n",
    "            locals()[input], input, ds, False\n",
    "        )\n",
    "        All_Inputs.append(locals()[input])\n",
    "        Features.append(locals()[input + '_encoded'])\n",
    "    All_Features = layers.concatenate(Features)\n",
    "    \n",
    "    print ()\n",
    "    return All_Inputs, All_Features\n",
    "\n",
    "#Understand_Encode_Features_2()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef652b",
   "metadata": {},
   "source": [
    "## Connectivity Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8bebe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Connectivity_Graph(model):\n",
    "    print ('Connectivity Graph()')\n",
    "    keras.utils.plot_model(\n",
    "        model, \n",
    "        show_shapes=True, \n",
    "        rankdir=\"LR\", \n",
    "        to_file = 'Paper_03_Build_Model_12_26_22_Model.png'\n",
    "    )\n",
    "    print ()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dcfd4d",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf4a61",
   "metadata": {},
   "source": [
    "## Alpha Weighted Binary Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3646fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_weighted_binary_crossentropy(y_true, y_pred):\n",
    "    p = 88.8\n",
    "    alpha = (p/(p+1))*1.0\n",
    "    # Note:  y_true has to be the same type as y_pred to use in the \"keras.backend.binary_crossentropy\" function below.\n",
    "    y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "\n",
    "\n",
    "    binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "#    print (binary_crossentropy.numpy())\n",
    "    weights = tf.where(tf.equal(y_true,1),alpha, 1-alpha)\n",
    "#    print (weights.numpy())\n",
    "    product = tf.multiply(binary_crossentropy, weights)\n",
    "#    print (product.numpy())\n",
    "    loss = keras.backend.mean(product)\n",
    "#    print (loss.numpy())\n",
    "#    print (loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74dedab",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f61cb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    # The dataset has  259077  elements.\n",
    "    # The target group has  31891  elements.\n",
    "    # Our target is  12.3095 % of the dataset.\n",
    "    # There are  8.12  negative elements for each positive.    \n",
    "#    p = 8.12\n",
    "    p = 7.46\n",
    "\n",
    "    alpha = (p/(p+1))*1.0\n",
    "\n",
    "    gamma_1 = 0.0 # Must be float for the tf.math.pow() function to work.\n",
    "    gamma_2 = 0.0\n",
    "    y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "    binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "#    print (binary_crossentropy.numpy())\n",
    "    weights = tf.where(tf.equal(y_true,1),alpha, 1-alpha)\n",
    "#    print (weights.numpy())\n",
    "    focal = tf.where(tf.equal(y_true,1), (1.0-y_pred), (y_pred))\n",
    "    power = tf.where(tf.equal(y_true,1), gamma_1, gamma_2)\n",
    "    focal_power = tf.math.pow(focal,power)\n",
    "#    print (focal.numpy())\n",
    "#    print (power.numpy())\n",
    "#    print (focal_power.numpy())\n",
    "    product = tf.multiply(binary_crossentropy, weights)\n",
    "    focal_power_product = tf.multiply(product, focal_power)\n",
    "#    print (focal_power_product.numpy())\n",
    "    loss = keras.backend.mean(focal_power_product)\n",
    "#    print (loss.numpy())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaef565",
   "metadata": {},
   "source": [
    "## Focal Loss with Parameters\n",
    "- Adapted from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c48a20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_with_parameters(alpha=0.5, gamma_1=0.0, gamma_2=0.0):\n",
    "    print ('focal_loss_with_parameters')\n",
    "    print ('alpha = ', alpha, ' gamma_1 = ', gamma_1, ' gamma_2 = ', gamma_2)\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        print ('focal_loss')\n",
    "        print ('alpha = ', alpha, ' gamma_1 = ', gamma_1, ' gamma_2 = ', gamma_2)\n",
    "\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        weights = tf.where(tf.equal(y_true,1),alpha, 1-alpha)\n",
    "        focal = tf.where(tf.equal(y_true,1), (1.0-y_pred), (y_pred))\n",
    "        power = tf.where(tf.equal(y_true,1), gamma_1, gamma_2)\n",
    "        focal_power = tf.math.pow(focal,power)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        focal_power_product = tf.multiply(product, focal_power)\n",
    "        loss = keras.backend.mean(focal_power_product)\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b909c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a36e3f",
   "metadata": {},
   "source": [
    "# Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3635b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Model (All_Inputs, All_Features):\n",
    "    print ('Make_Model()')\n",
    "    x = layers.Dense(32, activation=\"relu\")(All_Features)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(All_Inputs, output)\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    print ('display(model.summary())')\n",
    "    display(model.summary())\n",
    "    print ()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78cb5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model (train_ds, test_ds, model):\n",
    "    model.fit(train_ds, epochs=50, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764615d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8115af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c238c61",
   "metadata": {},
   "source": [
    "# Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397fb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1105c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b3877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07093edf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_Data()\n",
      "data.shape:  (619027, 82)\n",
      "End Get_Data()\n",
      "\n",
      "Thin_Features()\n",
      "data.shape:  (619027, 40)\n",
      "End Thin_Features()\n",
      "\n",
      "AGE\n",
      "BODY_TYP\n",
      "BUS_USE\n",
      "DAY_WEEK\n",
      "EMER_USE\n",
      "HOSPITAL\n",
      "HOUR\n",
      "INT_HWY\n",
      "LGT_COND\n",
      "MAKE\n",
      "MODEL\n",
      "MONTH\n",
      "NUMOCCS\n",
      "PEDS\n",
      "PERMVIT\n",
      "PERNOTMVIT\n",
      "PER_TYP\n",
      "PJ\n",
      "PSU\n",
      "PVH_INVL\n",
      "REGION\n",
      "RELJCT1\n",
      "RELJCT2\n",
      "REL_ROAD\n",
      "SCH_BUS\n",
      "SEX\n",
      "TYP_INT\n",
      "URBANICITY\n",
      "VALIGN\n",
      "VEH_AGE\n",
      "VE_FORMS\n",
      "VE_TOTAL\n",
      "VPROFILE\n",
      "VSPD_LIM\n",
      "VSURCOND\n",
      "VTRAFCON\n",
      "VTRAFWAY\n",
      "WEATHER\n",
      "WRK_ZONE\n",
      "YEAR\n",
      "\n",
      "Split_Data()\n",
      "(433318, 39) (433318,) 1657 69.97 %\n",
      "(185709, 39) (185709,) 711 30.03 %\n",
      "\n",
      "X_train.shape =  (433318, 39)\n",
      "X_test.shape =  (185709, 39)\n",
      "y_train.shape =  (433318,)\n",
      "y_test.shape =  (185709,)\n",
      "\n",
      "Dataframe_to_Dataset()\n",
      "ds.shape() =  433318\n",
      "ds.shape() =  433318\n",
      "\n",
      "Dataframe_to_Dataset()\n",
      "ds.shape() =  185709\n",
      "ds.shape() =  185709\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(        AGE  BODY_TYP  BUS_USE  DAY_WEEK  EMER_USE  HOUR  INT_HWY  LGT_COND  \\\n",
       " 479093    2         5        1         1         1     3        0         2   \n",
       " 592833    2         1        1         0         1     2        0         3   \n",
       " 30478     2         5        1         1         1     0        0         3   \n",
       " 519220    2         1        1         1         1     4        0         3   \n",
       " 342558    2         4        1         1         1     1        0         3   \n",
       " ...     ...       ...      ...       ...       ...   ...      ...       ...   \n",
       " 542775    2         1        1         1         1     3        0         1   \n",
       " 409731    3         0        1         1         1     2        0         3   \n",
       " 481832    2         3        1         1         1     0        0         1   \n",
       " 234606    2         0        1         1         1     3        0         3   \n",
       " 579718    0         1        1         1         1     2        0         3   \n",
       " \n",
       "         MAKE  MODEL  ...  VE_FORMS  VE_TOTAL  VPROFILE  VSPD_LIM  VSURCOND  \\\n",
       " 479093     2      4  ...         3         3         0         7         3   \n",
       " 592833     0      1  ...         2         2         1         2         1   \n",
       " 30478      7      4  ...         1         1         2         1         2   \n",
       " 519220     0      1  ...         3         3         1         5         1   \n",
       " 342558     8      3  ...         2         2         1         1         1   \n",
       " ...      ...    ...  ...       ...       ...       ...       ...       ...   \n",
       " 542775     8      3  ...         2         2         1         4         1   \n",
       " 409731     0      0  ...         2         2         1         4         1   \n",
       " 481832     0      2  ...         4         4         1         2         1   \n",
       " 234606     0      0  ...         2         2         1         4         1   \n",
       " 579718     4      2  ...         2         2         1         2         1   \n",
       " \n",
       "         VTRAFCON  VTRAFWAY  WEATHER  WRK_ZONE  YEAR  \n",
       " 479093         3         0        4         0  2017  \n",
       " 592833         1         0        1         0  2020  \n",
       " 30478          1         0        2         0  2020  \n",
       " 519220         1         0        1         0  2018  \n",
       " 342558         1         3        1         0  2019  \n",
       " ...          ...       ...      ...       ...   ...  \n",
       " 542775         2         0        3         0  2018  \n",
       " 409731         2         2        1         0  2016  \n",
       " 481832         1         2        3         0  2017  \n",
       " 234606         1         2        1         0  2018  \n",
       " 579718         2         1        1         0  2019  \n",
       " \n",
       " [433318 rows x 39 columns],\n",
       " <TensorSliceDataset element_spec=({'AGE': TensorSpec(shape=(), dtype=tf.int64, name=None), 'BODY_TYP': TensorSpec(shape=(), dtype=tf.int64, name=None), 'BUS_USE': TensorSpec(shape=(), dtype=tf.int64, name=None), 'DAY_WEEK': TensorSpec(shape=(), dtype=tf.int64, name=None), 'EMER_USE': TensorSpec(shape=(), dtype=tf.int64, name=None), 'HOUR': TensorSpec(shape=(), dtype=tf.int64, name=None), 'INT_HWY': TensorSpec(shape=(), dtype=tf.int64, name=None), 'LGT_COND': TensorSpec(shape=(), dtype=tf.int64, name=None), 'MAKE': TensorSpec(shape=(), dtype=tf.int64, name=None), 'MODEL': TensorSpec(shape=(), dtype=tf.int64, name=None), 'MONTH': TensorSpec(shape=(), dtype=tf.int64, name=None), 'NUMOCCS': TensorSpec(shape=(), dtype=tf.int64, name=None), 'PEDS': TensorSpec(shape=(), dtype=tf.int64, name=None), 'PERMVIT': TensorSpec(shape=(), dtype=tf.int64, name=None), 'PERNOTMVIT': TensorSpec(shape=(), dtype=tf.int64, name=None), 'PER_TYP': TensorSpec(shape=(), dtype=tf.int64, name=None), 'PJ': TensorSpec(shape=(), dtype=tf.int64, name=None), 'PSU': TensorSpec(shape=(), dtype=tf.int64, name=None), 'PVH_INVL': TensorSpec(shape=(), dtype=tf.int64, name=None), 'REGION': TensorSpec(shape=(), dtype=tf.int64, name=None), 'RELJCT1': TensorSpec(shape=(), dtype=tf.int64, name=None), 'RELJCT2': TensorSpec(shape=(), dtype=tf.int64, name=None), 'REL_ROAD': TensorSpec(shape=(), dtype=tf.int64, name=None), 'SCH_BUS': TensorSpec(shape=(), dtype=tf.int64, name=None), 'SEX': TensorSpec(shape=(), dtype=tf.int64, name=None), 'TYP_INT': TensorSpec(shape=(), dtype=tf.int64, name=None), 'URBANICITY': TensorSpec(shape=(), dtype=tf.int64, name=None), 'VALIGN': TensorSpec(shape=(), dtype=tf.int64, name=None), 'VEH_AGE': TensorSpec(shape=(), dtype=tf.int64, name=None), 'VE_FORMS': TensorSpec(shape=(), dtype=tf.int64, name=None), 'VE_TOTAL': TensorSpec(shape=(), dtype=tf.int64, name=None), 'VPROFILE': TensorSpec(shape=(), dtype=tf.int64, name=None), 'VSPD_LIM': TensorSpec(shape=(), dtype=tf.int64, name=None), 'VSURCOND': TensorSpec(shape=(), dtype=tf.int64, name=None), 'VTRAFCON': TensorSpec(shape=(), dtype=tf.int64, name=None), 'VTRAFWAY': TensorSpec(shape=(), dtype=tf.int64, name=None), 'WEATHER': TensorSpec(shape=(), dtype=tf.int64, name=None), 'WRK_ZONE': TensorSpec(shape=(), dtype=tf.int64, name=None), 'YEAR': TensorSpec(shape=(), dtype=tf.int64, name=None)}, TensorSpec(shape=(), dtype=tf.int64, name=None))>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m     Train_Model (train_ds, test_ds, model)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 59\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 42\u001b[0m, in \u001b[0;36mMain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     display(train_ds)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m ()\n\u001b[0;32m---> 42\u001b[0m     train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     test_ds \u001b[38;5;241m=\u001b[39m Batch(test_ds)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#    print (list(test_ds.as_numpy_iterator())[0])\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#    print ()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mBatch\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBatch\u001b[39m(ds):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m(\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m ()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "def Main():\n",
    "    target = 'HOSPITAL'\n",
    "\n",
    "    data = Get_Data()\n",
    "#    display(data.dtypes)\n",
    "    data = data.astype('int64')\n",
    "#    print ()\n",
    "#    display(data.dtypes)\n",
    "#    print ()\n",
    "    \n",
    "        \n",
    "    data = Thin_Features(data)\n",
    "    for feature in data:\n",
    "        print (feature)\n",
    "    print ()\n",
    "    \n",
    "#    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "#    data = pd.read_csv(file_url)\n",
    "#    target = 'target'\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.90)\n",
    "#    data = X_train\n",
    "#    data['HOSPITAL'] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "    print ('X_train.shape = ', X_train.shape)\n",
    "    print ('X_test.shape = ', X_test.shape)\n",
    "    print ('y_train.shape = ', y_train.shape)\n",
    "    print ('y_test.shape = ', y_test.shape)\n",
    "    print ()\n",
    "    \n",
    "    # On a bigger machine, increase the shuffle buffer size.\n",
    "    train_ds = Dataframe_to_Dataset(X_train, y_train)\n",
    "    test_ds = Dataframe_to_Dataset(X_test, y_test)    \n",
    "    # Before printing to check whether the thing is doing what we want it to do,\n",
    "    # change the 'test_size' in Train_Test_Split to 0.01 or less;\n",
    "    # otherwise, this print will take too long.  \n",
    "    display(train_ds)\n",
    "    print ()\n",
    "\n",
    "    train_ds = Batch(train_ds)\n",
    "    test_ds = Batch(test_ds)\n",
    "#    print (list(test_ds.as_numpy_iterator())[0])\n",
    "#    print ()\n",
    "\n",
    "    All_Inputs, All_Features = Encode_Features(X_train, train_ds)\n",
    "    \n",
    "    \n",
    "\n",
    "    model = Make_Model (All_Inputs, All_Features)\n",
    "    Connectivity_Graph(model)\n",
    "    \n",
    "#    keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\", to_file='model.png')\n",
    "    Train_Model (train_ds, test_ds, model)\n",
    "\n",
    "    return model\n",
    "        \n",
    "model = Main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673cf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb03bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
