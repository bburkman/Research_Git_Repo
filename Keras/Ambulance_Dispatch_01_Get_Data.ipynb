{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767154e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054af39c",
   "metadata": {},
   "source": [
    "# readme\n",
    "## Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905ac92",
   "metadata": {},
   "source": [
    "We designed this code to fit in a GitHub repository with files under 100MB.  Many of the files we input are over this limit, and after preprocessing (selecting features, binning features) we saved the data as a .csv file of about 150 MB so we can tweak later code without having to run the preprocessing again.  We saved it again after imputing missing data.  To keep the files in our GitHub repository under 100 MB, we saved these into a different directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884df33",
   "metadata": {},
   "source": [
    "- CRSS Data Files\n",
    "    - We use six years of data, 2016-2021. Once later years come out, they can be easily added. \n",
    "    - Each year's data is 100-200 MB\n",
    "    - The files we're really interested in each year are these.  The names were uppercase until 2018, then lowercase, and also after 2018 the file sizes jumped.\n",
    "        - accident.csv or ACCIDENT.csv, now about 30 MB\n",
    "        - vehicle.csv or VEHICLE.csv, now about 180 MB\n",
    "        - person.csv or PERSON.csv, now about 150 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698bb8d",
   "metadata": {},
   "source": [
    "- Big_Files\n",
    "    - CRSS_Files\n",
    "        - CRSS2016CSV (22 files, 160 MB)\n",
    "        - CRSS2017CSV (22 files, 189 MB)\n",
    "        - CRSS2018CSV (22 files, 169 MB)\n",
    "        - CRSS2019CSV (23 files, 633 MB)\n",
    "        - CRSS2020CSV (29 files, 719 MB)\n",
    "        - CRSS2021CSV (29 files, 736 MB)\n",
    "        \n",
    "        \n",
    "    - *Intermediate .csv files*\n",
    "- GitHub_Repository\n",
    "    - Code_Files\n",
    "        - Analyze_Proba\n",
    "        - Confusion_Matrices\n",
    "        - Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e1d2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7d47d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b4fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install Packages\n",
      "Python version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:26:08) [Clang 14.0.6 ]\n",
      "NumPy version: 1.24.2\n",
      "SciPy version:  1.7.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bburkman/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.11.0\n",
      "Keras version:  2.11.0\n",
      "Pandas version:  1.5.3\n",
      "SciKit-Learn version: 1.2.2\n",
      "Imbalanced-Learn version: 0.10.1\n",
      "Finished Installing Packages\n"
     ]
    }
   ],
   "source": [
    "print ('Install Packages')\n",
    "\n",
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "#from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import scipy as sc\n",
    "print ('SciPy version:  {}'.format(sc.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print ('TensorFlow version:  {}'.format(tf.__version__))\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "from tensorflow import keras\n",
    "print ('Keras version:  {}'.format(keras.__version__))\n",
    "\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "from keras.layers import IntegerLookup\n",
    "from keras.layers import Normalization\n",
    "from keras.layers import StringLookup\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.utils import tf_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "#    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Library for reading Microsoft Access files\n",
    "#import pandas_access as mdb\n",
    "\n",
    "import sklearn\n",
    "print ('SciKit-Learn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import imblearn\n",
    "print ('Imbalanced-Learn version: {}'.format(imblearn.__version__))\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "#!pip install pydot\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "#np.random.seed(42) # NumPy\n",
    "#random.seed(42) # Python\n",
    "#tf.random.set_seed(42) # Tensorflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Installing Packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d0f6e",
   "metadata": {},
   "source": [
    "# Get Data and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b290fc",
   "metadata": {},
   "source": [
    "## Read CRSS Files\n",
    "- We have the CRSS dataset in \n",
    "    - Big_Files/CRSS_2020_Update/\n",
    "- In one directory for each year,\n",
    "    - CRSS2016CSV\n",
    "    - CRSS2017CSV\n",
    "    - CRSS2018CSV\n",
    "    - CRSS2019CSV\n",
    "    - CRSS2020CSV    \n",
    "    - CRSS2021CSV    \n",
    "- In each year, the CRSS dataset comes in three main files, \n",
    "    - Accident.csv\n",
    "    - Vehicle.csv\n",
    "    - Person.csv\n",
    "- Collect those and merge into three files,\n",
    "    - Accident_Raw.csv\n",
    "    - Vehicle_Raw.csv\n",
    "    - Person_Raw.csv\n",
    "- and also three files with category names,\n",
    "    - Accident_Raw_with_Names.csv\n",
    "    - Vehicle_Raw_with_Names.csv\n",
    "    - Person_Raw_with_Names.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13504c",
   "metadata": {},
   "source": [
    "### accident.csv from CRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48cc11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data_Accident(NAMES):\n",
    "    print ('Import_Data_Accident()')\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "#    for year in ['2018']:\n",
    "    for year in ['2016','2017','2018']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/ACCIDENT.CSV'\n",
    "        temp = pd.read_csv(filename, index_col=None)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "#    for year in ['2020']:\n",
    "    for year in ['2019','2020','2021']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/accident.csv'\n",
    "        temp = pd.read_csv(filename, index_col=None)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "    \n",
    "    if NAMES==0:\n",
    "        for feature in df:\n",
    "            if 'NAME' in feature:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print (df.shape)\n",
    "    print ()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bca27",
   "metadata": {},
   "source": [
    "### vehicle.csv from CRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250e2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data_Vehicle(NAMES):\n",
    "    print ('Import_Data_Vehicle()')\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "    for year in ['2016','2017','2018']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/VEHICLE.CSV'\n",
    "        temp = pd.read_csv(filename, index_col=None, low_memory=False)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    for year in ['2019','2020','2021']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/vehicle.csv'\n",
    "        temp = pd.read_csv(filename, index_col=None, encoding='latin1', low_memory=False)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    if NAMES==0:\n",
    "        for feature in df:\n",
    "            if 'NAME' in feature:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print (df.shape)\n",
    "    print ()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7b6cf",
   "metadata": {},
   "source": [
    "### person.csv from CRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62699818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data_Person(NAMES):\n",
    "    print ('Import_Data_Person()')\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "    for year in ['2016','2017','2018']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/PERSON.CSV'\n",
    "        temp = pd.read_csv(filename, index_col=None)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    for year in ['2019','2020','2021']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/person.csv'\n",
    "        temp = pd.read_csv(filename, index_col=None, encoding='latin1')\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    if NAMES==0:\n",
    "        for feature in df:\n",
    "            if 'NAME' in feature:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print (df.shape)\n",
    "    print ()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d05f7",
   "metadata": {},
   "source": [
    "### Get Data\n",
    "- The Get_Data_from_Original() reads the (original) CRSS files from the CRSS directory, preprocesses it, and writes it to files in a folder outside this GitHub repo (because the files are too large for my subscription), and returns the dataframes.\n",
    "- The Get_Data_from_Temp_Files() reads the temp files and returns the dataframes.  I created this option for running repeatedly during writing and debugging, because it's much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f93a66f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data_from_Original():\n",
    "    print ('Get_Data_from_Original()')\n",
    "    \n",
    "    df_Accident = Import_Data_Accident(0)\n",
    "    df_Vehicle = Import_Data_Vehicle(0)\n",
    "    df_Person = Import_Data_Person(0)\n",
    "    \n",
    "    df_Accident.to_csv('../../Big_Files/Accident_Raw.csv', index=False)\n",
    "    df_Vehicle.to_csv('../../Big_Files/Vehicle_Raw.csv', index=False)\n",
    "    df_Person.to_csv('../../Big_Files/Person_Raw.csv', index=False)\n",
    "    \n",
    "\n",
    "    df_Accident = Import_Data_Accident(1)\n",
    "    df_Vehicle = Import_Data_Vehicle(1)\n",
    "    df_Person = Import_Data_Person(1)\n",
    "    \n",
    "    df_Accident.to_csv('../../Big_Files/Accident_Raw_with_NAMES.csv', index=False)\n",
    "    df_Vehicle.to_csv('../../Big_Files/Vehicle_Raw_with_NAMES.csv', index=False)\n",
    "    df_Person.to_csv('../../Big_Files/Person_Raw_with_NAMES.csv', index=False)\n",
    "    \n",
    "\n",
    "    return df_Accident, df_Vehicle, df_Person\n",
    "\n",
    "#df_Accident, df_Vehicle, df_Person = Get_Data_from_Original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9ba441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_New_Files():\n",
    "    print ('Check_New_Files')\n",
    "    Files = [\n",
    "        'Accident_Raw',\n",
    "        'Vehicle_Raw',\n",
    "        'Person_Raw',\n",
    "        'Accident_Raw_with_Names',\n",
    "        'Vehicle_Raw_with_Names',\n",
    "        'Person_Raw_with_Names'\n",
    "    ]\n",
    "    for filename in Files:\n",
    "        df = pd.read_csv('../../Big_Files/' + filename + '.csv', low_memory=False)\n",
    "        print (filename, df.shape)\n",
    "    \n",
    "    return 0    \n",
    "\n",
    "#Check_New_Files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a5add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data_from_Temp_Files():\n",
    "    print ('Get_Data')\n",
    "    df_Acc = pd.read_csv('../../Big_Files/Accident_Raw.csv', low_memory=False)\n",
    "    df_Veh = pd.read_csv('../../Big_Files/Vehicle_Raw.csv', low_memory=False)\n",
    "    df_Per = pd.read_csv('../../Big_Files/Person_Raw.csv', low_memory=False)\n",
    "    \n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Veh.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    return df_Acc, df_Veh, df_Per\n",
    "\n",
    "#df_Acc, df_Veh, df_Per = Get_Data_from_Temp_Files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabf849",
   "metadata": {},
   "source": [
    "## Drop Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2e435",
   "metadata": {},
   "source": [
    "- We now have three dataframes from the Accident, Vehicle, and Person files.  \n",
    "- Some features are repeated, so we will drop the ones in Vehicle or Person that appear in Accident, and drop those in Person that appear in Vehicle. \n",
    "- There are two repeated features we need to keep for merging the three data:\n",
    "    - CASENUM tells us to which accident the vehicle and person correspond\n",
    "    - VEH_NO tells us which vehicle the person was in.\n",
    "- Some features have no predictive power and/or resemble random numbers, like the VIN (Vehicle Identification Number) and the minute of the accident time.  \n",
    "- For details on the features, see the *Crash Report Sampling System Analytical User's Manual 2016-2020.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cc781",
   "metadata": {},
   "source": [
    "### Drop Repeated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4717cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_Repeated_Features(df_Acc, df_Veh, df_Per):\n",
    "    print ('Drop_Repeated_Features()')\n",
    "    Acc_Cols = df_Acc.columns.tolist()\n",
    "    Veh_Cols = df_Veh.columns.tolist()\n",
    "    Per_Cols = df_Per.columns.tolist()\n",
    "    \n",
    "    Drop_Veh = [x for x in Veh_Cols if x in Acc_Cols]\n",
    "    Drop_Per = [x for x in Per_Cols if (x in Acc_Cols or x in Veh_Cols)]\n",
    "        \n",
    "    \"\"\"\n",
    "    print ('Drop_Veh:')\n",
    "    for item in Drop_Veh:\n",
    "        print (item)\n",
    "    print ()\n",
    "\n",
    "    print ('Drop_Per:')\n",
    "    for item in sorted(Drop_Per):\n",
    "        print (item)\n",
    "    print ()\n",
    "    \"\"\"    \n",
    "    \n",
    "    # We need to keep these for merging the dataframes.\n",
    "    Drop_Veh.remove('CASENUM')\n",
    "    Drop_Per.remove('CASENUM')\n",
    "    Drop_Per.remove('VEH_NO')\n",
    "    \n",
    "    df_Veh.drop(columns=Drop_Veh, inplace=True)\n",
    "    df_Per.drop(columns=Drop_Per, inplace=True)\n",
    "\n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Vet.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    return df_Acc, df_Veh, df_Per\n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819611e1",
   "metadata": {},
   "source": [
    "### Drop Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d43160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_Irrelevant_Features(df_Acc, df_Veh, df_Per):\n",
    "    \n",
    "    print ('Drop_Irrelevant_Features')\n",
    "    \n",
    "    Drop_Accident = [\n",
    "        'CF1',\n",
    "        'CF2',\n",
    "        'CF3',\n",
    "        'MINUTE',\n",
    "        'MINUTE_IM',\n",
    "        'PSU_VAR',\n",
    "        'PSUSTRAT',\n",
    "        'STRATUM',\n",
    "        'WEATHER1',\n",
    "        'WEATHER2',\n",
    "        'WEIGHT',\n",
    "    ]\n",
    "    \n",
    "    df_Acc.drop(columns=Drop_Accident, inplace=True)\n",
    "    \n",
    "    # List of features in df_Veh that aren't repeats from df_Acc \n",
    "    # that we don't want to use, even for imputation, because\n",
    "    # they're only for some years or are like random numbers\n",
    "    Drop_Vehicle = [\n",
    "        'DR_SF1',\n",
    "        'DR_SF2',\n",
    "        'DR_SF3',\n",
    "        'DR_SF4',\n",
    "        'GVWR',\n",
    "        'GVWR_FROM',\n",
    "        'GVWR_TO',\n",
    "        'HAZ_ID',\n",
    "        'ICFINALBODY',\n",
    "        'MCARR_I1',\n",
    "        'MCARR_I2',\n",
    "        'MCARR_ID',\n",
    "        'TRLR1GVWR',\n",
    "        'TRLR1VIN',\n",
    "        'TRLR2GVWR',\n",
    "        'TRLR2VIN',\n",
    "        'TRLR3GVWR',\n",
    "        'TRLR3VIN',\n",
    "        'UNITTYPE',\n",
    "        'V_CONFIG',\n",
    "        'V_Config',\n",
    "        'VEH_SC1',\n",
    "        'VEH_SC2',\n",
    "        'VIN',\n",
    "        'VPICBODYCLASS',\n",
    "        'VPICMAKE',\n",
    "        'VPICMODEL',\n",
    "    ]\n",
    "    \n",
    "    df_Veh.drop(columns=Drop_Vehicle, inplace=True)\n",
    "    \n",
    "    Drop_Person = [\n",
    "        'ATST_TYP',\n",
    "        'DRUGRES1',\n",
    "        'DRUGRES2',\n",
    "        'DRUGRES3',\n",
    "        'DRUGTST1',\n",
    "        'DRUGTST2',\n",
    "        'DRUGTST3',\n",
    "        'DSTATUS',\n",
    "        'HELM_MIS',\n",
    "        'HELM_USE',\n",
    "        'P_SF1',\n",
    "        'P_SF2',\n",
    "        'P_SF3',\n",
    "        'STR_VEH',\n",
    "    ]\n",
    "    \n",
    "    df_Per.drop(columns=Drop_Person, inplace=True)\n",
    "    \n",
    "    \n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Veh.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    \n",
    "    return df_Acc, df_Veh, df_Per"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8142873",
   "metadata": {},
   "source": [
    "## Merge Accident, Vehicle, and Person Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05306685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(df_Acc, df_Veh, df_Per):\n",
    "    print ('Merge()')\n",
    "    print ()\n",
    "\n",
    "    data = pd.merge(\n",
    "        df_Acc, df_Veh, \n",
    "        on=['CASENUM'],\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "    \n",
    "    print ('df_Acc.shape')\n",
    "    print (df_Acc.shape)\n",
    "    print ('df_Veh.shape')\n",
    "    print (df_Veh.shape)\n",
    "    print ('data.shape')\n",
    "    print (data.shape)\n",
    "    print ()\n",
    "\n",
    "    \n",
    "    data = pd.merge(\n",
    "        data, df_Per, \n",
    "        on=['CASENUM', 'VEH_NO'],\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "    \n",
    "    print ('df_Acc.shape')\n",
    "    print (df_Acc.shape)\n",
    "    print ('df_Veh.shape')\n",
    "    print (df_Veh.shape)\n",
    "    print ('df_Per.shape')\n",
    "    print (df_Per.shape)\n",
    "    print ('data.shape')\n",
    "    print (data.shape)\n",
    "    print ()\n",
    "\n",
    "\n",
    "    print (data.head())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eff2d1",
   "metadata": {},
   "source": [
    "## Drop Pedestrian Crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a5c26",
   "metadata": {},
   "source": [
    "A vehicle hitting another vehicle, a tree, or something else large can result in sudden deceleration different enough from hard braking to trigger an automated notification, but an impact with a pedestrian or bicycle is not.  Our work needs to focus on crashes likely to trigger an automated notification, so we will drop pedestrian crashes from our dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9950da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Pedestrian_Crashes(data):\n",
    "    print ('Remove_Pedestrian_Crashes()')\n",
    "    display(data.PEDS.value_counts())\n",
    "    n = len(data[data.PEDS>0])\n",
    "    print ('Removing %d crashes that involve a pedestrian.' % n)\n",
    "    data = data[data.PEDS==0]\n",
    "    print ('data.shape: ', data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5af9a",
   "metadata": {},
   "source": [
    "## Run:  Get Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d75392b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess_Data()\n",
      "Get_Data\n",
      "df_Acc.shape =  (313277, 51)\n",
      "df_Veh.shape =  (553099, 98)\n",
      "df_Per.shape =  (778008, 69)\n",
      "\n",
      "Drop_Repeated_Features()\n",
      "df_Acc.shape =  (313277, 51)\n",
      "df_Vet.shape =  (553099, 84)\n",
      "df_Per.shape =  (778008, 38)\n",
      "\n",
      "Drop_Irrelevant_Features\n",
      "df_Acc.shape =  (313277, 40)\n",
      "df_Veh.shape =  (553099, 57)\n",
      "df_Per.shape =  (778008, 24)\n",
      "\n",
      "Merge()\n",
      "\n",
      "df_Acc.shape\n",
      "(313277, 40)\n",
      "df_Veh.shape\n",
      "(553099, 57)\n",
      "data.shape\n",
      "(553099, 96)\n",
      "\n",
      "df_Acc.shape\n",
      "(313277, 40)\n",
      "df_Veh.shape\n",
      "(553099, 57)\n",
      "df_Per.shape\n",
      "(778008, 24)\n",
      "data.shape\n",
      "(747342, 118)\n",
      "\n",
      "        CASENUM  PSU   PJ  VE_TOTAL  VE_FORMS  PVH_INVL  PEDS  PERMVIT  \\\n",
      "0  201600014311   44  388         2         2         0     0        2   \n",
      "1  201600014311   44  388         2         2         0     0        2   \n",
      "2  201600014315   44  388         2         2         0     0        4   \n",
      "3  201600014315   44  388         2         2         0     0        4   \n",
      "4  201600014315   44  388         2         2         0     0        4   \n",
      "\n",
      "   PERNOTMVIT  NUM_INJ  ...  ALC_RES  DRUGS  HOSPITAL  LOCATION  SEX_IM  \\\n",
      "0           0        0  ...      995      0         0         0       1   \n",
      "1           0        0  ...      995      0         0         0       1   \n",
      "2           0        4  ...      995      0         0         0       1   \n",
      "3           0        4  ...      995      0         0         0       2   \n",
      "4           0        4  ...      996      0         0         0       1   \n",
      "\n",
      "   INJSEV_IM  EJECT_IM  PERALCH_IM  SEAT_IM  AGE_IM  \n",
      "0          0         0           0       11      72  \n",
      "1          0         0           0       11      31  \n",
      "2          1         0           0       11      17  \n",
      "3          1         0           0       11      18  \n",
      "4          1         0           0       12      18  \n",
      "\n",
      "[5 rows x 118 columns]\n",
      "Remove_Pedestrian_Crashes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     713566\n",
       "1      32648\n",
       "2        974\n",
       "3        110\n",
       "4         32\n",
       "6          8\n",
       "5          1\n",
       "11         1\n",
       "7          1\n",
       "8          1\n",
       "Name: PEDS, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 33776 crashes that involve a pedestrian.\n",
      "data.shape:  (713566, 118)\n",
      "Finished Preprocess_Data()\n"
     ]
    }
   ],
   "source": [
    "def Preprocess_Data():\n",
    "    print ('Preprocess_Data()')\n",
    "#    df_Acc, df_Veh, df_Per = Get_Data_from_Original()\n",
    "    df_Acc, df_Veh, df_Per = Get_Data_from_Temp_Files()\n",
    "    df_Acc, df_Veh, df_Per = Drop_Repeated_Features(df_Acc, df_Veh, df_Per)    \n",
    "    df_Acc, df_Veh, df_Per = Drop_Irrelevant_Features (df_Acc, df_Veh, df_Per)\n",
    "\n",
    "    data = Merge (df_Acc, df_Veh, df_Per)\n",
    "    \n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    \n",
    "    # Bin the target variable.  \n",
    "    # Either the person went to the hospital or didn't; we don't care how the person got to the hospital.\n",
    "    data['HOSPITAL'] = data['HOSPITAL'].apply(lambda x:1 if x in [1,2,3,4,5] else 0)\n",
    "      \n",
    "    data.to_csv('../../Big_Files/CRSS_Merged_Raw_Data.csv', index=False)\n",
    "    print ('Finished Preprocess_Data()')\n",
    "\n",
    "Preprocess_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bface0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c9b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86570e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec6303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
