%%%%%
\section{Metrics}

%%%
\subsection{The Problem:  Imbalanced Data Set}

In an unbalanced data set, the number of actual negatives ($N = TN + FP$) is much different from the number of actual positives ($P = FN + TP$).  In our case, if our independent variable is fatal crashes, the negatives are $99.574714\%$ of the data set, and the positives are just $0.425286\%$.

The standard metrics get thrown off by the imbalance.  If we predict that every crash is nonfatal, we have accuracy of 99.57\%, which sounds really impressive.  

The recall (true positive rate) is not thrown off by an imbalanced data set, because it only works with TP and FN, the actual positives.  Similarly for specificity (true negative rate).

The precision is thrown off by an imbalanced data set, because it works with both a subset of the actual positives (TP) and a subset of the actual negatives (FP).  


%%%
\subsection{Standard Metrics}

\hfil \begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{Actual}&N & TN & FP \cr\cline{3-4}
	&P & FN & TP \cr\cline{3-4}
\end{tabular}

\begin{align*}
	\text{Accuracy} &= \frac{TN+TP}{TN+FP+FN+TP} \cr
	\text{Recall or TPR} &= \frac{TP}{TP + FN} \cr
	\text{Specificity, Selectivity, or TNR} &= \frac{TN}{TN + FP} \cr
	\text{Precision} & = \frac{TP}{TP + FP}\cr
\end{align*}

%%%
\subsection{Balanced Precision and Balanced f1 in the Penalty Function}

Most ML algorithms work using a {\it penalty function} that measures how bad the current solution is, then iteratively improving the solution in the direction that minimizes the penalty.  We should be able to write a custom penalty function.  

\

Update:  How-to instructions for changing the metrics in {\tt scikit-learn}.  The example is how to use recall instead of accuracy.  

\url{https://stackoverflow.com/questions/54267745}


\

{\it Recall} only deals with the minority class, so the balance of the data set doesn't matter.  {\it Precision}, on the other hand, takes results from both classes, so we can balance it by scaling the count of False Positive results, giving a {\it Balanced Precision} metric.  From Recall and Balanced Precision we can get a {\it Balanced f1} metric.  

If our penalty function uses balanced precision and balanced f1, it may not matter that our data set is imbalanced, and we can use all of, and only, the original data to build our model.  

%%%
\subsection{Balanced Precision in the Literature}

{\it Balanced Accuracy} frequently appears in the literature.  I have not found {\it balanced precision} in the literature.  Two possible reasons.  Either nobody has thought of it, or they did, found it not useful, and abandoned the idea.

\

Update:  {\tt imbalanced-learn} has more metrics than {\it scikit-learn}, but still no balanced precision.  

\url{https://imbalanced-learn.org/dev/metrics.html}



%%%
\subsection{Balanced Accuracy}

There is a metric called {\it balanced accuracy}.  You get it from the definition of {\it accuracy} by multiplying the actual negative elements (TN and FP) by the ratio of the positives to negatives, 

$$\frac{P}{N} = \frac{FN+TP}{TN+FP}$$

so that the total number of actual negatives and total number of actual positives in the sample are equal.

[I suppose you could also get it by multiplying the actual positive elements (FN and TP) by the reciprocal.]

I got this derivation by intuiting about what I would want {\it balanced accuracy} to mean, and it matches the definition I found in Wikipedia.  

\url{https://en.wikipedia.org/wiki/precision_and_recall#Imbalanced_data}

Wikipedia says [I'm sure I can find a more authoritative source.]

$$\text{Balanced Accuracy} = \frac{TPR + TNR}{2}$$

\begin{align*}
	\text{Recall or TPR} &= \frac{TP}{TP+FN} \cr
	\text{Specificity or TNR} &= \frac{TN}{TN+FP} \cr
	\text{Accuracy} &= \frac{TN+TP}{TN+FP+FN+TP} \cr
	\text{Balanced Accuracy} &=  \frac{TN \cdot \frac{P}{N}+TP}{TN \cdot \frac{P}{N}+FP \cdot \frac{P}{N}+FN+TP} \cr
		&= \frac{TN \cdot P+TP \cdot N}{TN \cdot P+FP \cdot P+FN \cdot N+TP\cdot N} \cr
	&= \frac{TN \cdot P+TP \cdot N}{(TN+FP) \cdot P+(FN+TP) \cdot  N} \cr
	&= \frac{TN (FN+TP)+TP (TN+FP)}{(TN+FP) (FN+TP)+(FN+TP) (TN+FP)} \cr
	&= \frac{TN (FN+TP)+TP (TN+FP)}{2(TN+FP) (FN+TP)} \cr
	&= \frac{TN (FN+TP)}{2(TN+FP) (FN+TP)}  + \frac{TP (TN+FP)}{2(TN+FP) (FN+TP)} \cr
	&= \frac{TN}{2(TN+FP) }  + \frac{TP }{2 (FN+TP)} \cr
	&= \frac{TNR+TPR}{2} \cr
\end{align*}

%%%
\subsection{Balanced Precision}

I haven't found {\it balanced precision} in a brief Google search, although Google knows the kind of stuff I look up and sent me to articles on balanced accuracy.  Finding it will take some work, because ``balanced precision'' has different meanings in other tech fields.  

We can make balanced precision the same way we made balanced accuracy, by taking the actual negative results (TN and FP) and scaling them  so that the total number of actual negatives equals the total number of actual positives, by multiplying by $\frac{P}{N} = \frac{FN+TP}{TN+FP}$.

Is this related to the G-mean?  [No]

$$\text{G-mean} = \sqrt{\text{Precision} \times \text{Specificity}}$$

\begin{align*}
	\text{Precision} &= \frac{TP}{TP+FP} \cr
	\text{Balanced Precision} &= \frac{TP}{TP+FP \cdot \frac{P}{N}} \cr
		&= \frac{TP \cdot N}{TP \cdot N + FP \cdot P} \cr
		&= \frac{TP (TN+FP)}{TP(TN+FP) + FP (FN+TP)} \cr
		&= \frac{TP (TN+FP)}{TP(TN+FP) + FP (FN+TP)} \cr
		&= \dots \cr
\end{align*}

Giving up here on finding some nice, concise connection between Balanced Precision and other metrics.  


%%%
\subsection{Balancing Two Metrics:  F1 and Gmean}

From Elassad 2020:
\cite{ELAMRANIABOUELASSAD2020102708}

\begin{quote}
F1 score, is a highly informative measure as it considers both precision and recall measures, which makes it very suitable forimbalanced classification (Qian et al., 2014; Sun et al., 2018); itâ€™s deemed to be a special measure that conveys the balance betweenthe precision and recall in order to find an effective and efficient trade-off. Another useful metric is G-mean, which is considered as ametric of stability between correct classification of positive class and negative class viewed independently. It is usually adopted inorder to resist the imbalances in the dataset (Kubat et al., 1997).
\end{quote}


\subsubsection{F1 Metric}
\index{F1}

F1 is the harmonic mean of Precision and Recall.  

$$\text{F1} = \frac{2}{\frac{1}{\text{Precision}} + \frac{1}{\text{Recall}}} 
	= \frac{2 \cdot TP}{2\cdot TP + FP + FN}$$

%%%
\subsubsection{Gmean}
\index{Gmean}

Gmean is the geometric mean of Precision and Specificity (TNR).

\begin{align*}
	\text{Precision} & = \frac{TP}{TP + FP}\cr
	\text{Specificity, Selectivity, or TNR} &= \frac{TN}{TN + FP} \cr
	\text{Gmean} &= \sqrt{ \text{Precision} \times \text{Specificity}} \cr
	&= \sqrt{
		\frac{TP}{TP+FP} \times \frac{TN}{TN+FP}
		}
\end{align*}





