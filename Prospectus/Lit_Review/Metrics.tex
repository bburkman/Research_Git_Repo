%%%%%
\section{Metrics}
\label{sec:Imbalanced_Metrics}

%%%
\subsection{The Problem:  Imbalanced Data Set}

In an unbalanced data set, the number of actual negatives ($N = TN + FP$) is much different from the number of actual positives ($P = FN + TP$).  In our case, if our independent variable is fatal crashes, the negatives are $99.574714\%$ of the data set, and the positives are just $0.425286\%$.

The standard metrics get thrown off by the imbalance.  If we predict that every crash is nonfatal, we have accuracy of 99.57\%, which sounds really impressive.  

The recall (true positive rate) is not thrown off by an imbalanced data set, because it only works with TP and FN, the actual positives.  Similarly for specificity (true negative rate).

The precision is thrown off by an imbalanced data set, because it works with both a subset of the actual positives (TP) and a subset of the actual negatives (FP).  


%%%
\subsection{Standard Metrics}

\hfil \begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{Negative} & \multicolumn{1}{c}{Positive} \cr\cline{3-4}
	\multirow{2}{*}{Actual}&Negative & True Negative & False Positive \cr\cline{3-4}
	&Positive & False Negative & True Positive \cr\cline{3-4}
\end{tabular}
\qquad
\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{Actual}&N & TN & FP \cr\cline{3-4}
	&P & FN & TP \cr\cline{3-4}
\end{tabular}

\begin{align*}
	\text{Accuracy} &= \frac{TN+TP}{TN+FP+FN+TP} \cr
	\text{Recall or TPR} &= \frac{TP}{TP + FN} \cr
	\text{Specificity, Selectivity, or TNR} &= \frac{TN}{TN + FP} \cr
	\text{Precision} & = \frac{TP}{TP + FP}\cr
\end{align*}

%%%
\subsection{Balanced Precision and Balanced F1 in the Penalty Function}
\index{Balanced Precision}
\index{Balanced F1}
\label{sec:Balanced_Metrics}


Most ML algorithms work using a {\it penalty function} that measures how bad the current solution is, then iteratively improving the solution in the direction that minimizes the penalty.  We should be able to write a custom penalty function.  

\

Update:  How-to instructions for changing the metrics in {\tt scikit-learn}.  The example is how to use recall instead of accuracy.  

\url{https://stackoverflow.com/questions/54267745}


\

{\it Recall} only deals with the minority class, so the balance of the data set doesn't matter.  {\it Precision}, on the other hand, takes results from both classes, so we can balance it by scaling the count of False Positive results, giving a {\it Balanced Precision} metric.  From Recall and Balanced Precision we can get a {\it Balanced f1} metric.  

If our penalty function uses balanced precision and balanced f1, it may not matter that our data set is imbalanced, and we can use all of, and only, the original data to build our model.  

%%%
\subsection{Balanced Precision in the Literature}

{\it Balanced Accuracy} frequently appears in the literature.  I have not found {\it balanced precision} in the literature.  Two possible reasons.  Either nobody has thought of it, or they did, found it not useful, and abandoned the idea.

\

{\tt imbalanced-learn} has more metrics than {\it scikit-learn}, but still no balanced precision.  

\url{https://imbalanced-learn.org/dev/metrics.html}



%%%
\subsection{Balanced Accuracy}
\index{Balanced Accuracy}

There is a metric called {\it balanced accuracy}.  You get it from the definition of {\it accuracy} by multiplying the actual negative elements (TN and FP) by the ratio of the positives to negatives, 

$$\frac{P}{N} = \frac{FN+TP}{TN+FP}$$

so that the total number of actual negatives and total number of actual positives in the sample are equal.

[I suppose you could also get it by multiplying the actual positive elements (FN and TP) by the reciprocal.]

I got this derivation by intuiting about what I would want {\it balanced accuracy} to mean, and it matches the definition I found in Wikipedia.  

\url{https://en.wikipedia.org/wiki/precision_and_recall#Imbalanced_data}

Wikipedia says [I'm sure I can find a more authoritative source.]

$$\text{Balanced Accuracy} = \frac{TPR + TNR}{2}$$

\begin{align*}
	\text{Recall or TPR} &= \frac{TP}{TP+FN} \cr
	\text{Specificity or TNR} &= \frac{TN}{TN+FP} \cr
	\text{Accuracy} &= \frac{TN+TP}{TN+FP+FN+TP} \cr
	\text{Balanced Accuracy} &=  \frac{TN \cdot \frac{P}{N}+TP}{TN \cdot \frac{P}{N}+FP \cdot \frac{P}{N}+FN+TP} \cr
		&= \frac{TN \cdot P+TP \cdot N}{TN \cdot P+FP \cdot P+FN \cdot N+TP\cdot N} \cr
	&= \frac{TN \cdot P+TP \cdot N}{(TN+FP) \cdot P+(FN+TP) \cdot  N} \cr
	&= \frac{TN (FN+TP)+TP (TN+FP)}{(TN+FP) (FN+TP)+(FN+TP) (TN+FP)} \cr
	&= \frac{TN (FN+TP)+TP (TN+FP)}{2(TN+FP) (FN+TP)} \cr
	&= \frac{TN (FN+TP)}{2(TN+FP) (FN+TP)}  + \frac{TP (TN+FP)}{2(TN+FP) (FN+TP)} \cr
	&= \frac{TN}{2(TN+FP) }  + \frac{TP }{2 (FN+TP)} \cr
	&= \frac{TNR+TPR}{2} \cr
\end{align*}


%%%
\subsection{Balancing Two Metrics:  F1 and Gmean}

From Elassad 2020:
\cite{ELAMRANIABOUELASSAD2020102708}

\begin{quote}
F1 score, is a highly informative measure as it considers both precision and recall measures, which makes it very suitable for
imbalanced classification (Qian et al., 2014; Sun et al., 2018); itâ€™s deemed to be a special measure that conveys the balance between
the precision and recall in order to find an effective and efficient trade-off. Another useful metric is G-mean, which is considered as a
metric of stability between correct classification of positive class and negative class viewed independently. It is usually adopted in
order to resist the imbalances in the dataset (Kubat et al., 1997).
\end{quote}


\subsubsection{F1 Metric}
\index{F1}

F1 is the harmonic mean of Precision and Recall.  

$$\text{F1} = \frac{2}{\frac{1}{\text{Precision}} + \frac{1}{\text{Recall}}} 
	= \frac{2 \cdot TP}{2\cdot TP + FP + FN}$$

%%%
\subsubsection{Gmean}
\index{Gmean}

Gmean is the geometric mean of Precision and Specificity (TNR).

\begin{align*}
	\text{Precision} & = \frac{TP}{TP + FP}\cr
	\text{Specificity, Selectivity, or TNR} &= \frac{TN}{TN + FP} \cr
	\text{Gmean} &= \sqrt{ \text{Precision} \times \text{Specificity}} \cr
	&= \sqrt{
		\frac{TP}{TP+FP} \times \frac{TN}{TN+FP}
		}
\end{align*}

%%%
\subsection{Balanced Precision}
\index{Balanced Precision}

I have not seen Balanced Precision in the literature, although it could be called something else and have been used in places I did not look.  

We can make balanced precision the same way we made balanced accuracy, by taking the actual negative results (TN and FP) and scaling them  so that the total number of actual negatives equals the total number of actual positives, by multiplying by $\frac{P}{N} = \frac{FN+TP}{TN+FP}$.

Is this related to the G-mean?  [No]

$$\text{G-mean} = \sqrt{\text{Precision} \times \text{Specificity}}$$

\begin{align*}
	\text{Precision} &= \frac{TP}{TP+FP} \cr
	\text{Balanced Precision} &= \frac{TP}{TP+FP \cdot \frac{P}{N}} \cr
		&= \frac{TP \cdot N}{TP \cdot N + FP \cdot P} \cr
		&= \frac{TP (TN+FP)}{TP(TN+FP) + FP (FN+TP)} \cr
		&= \frac{TP (TN+FP)}{TP(TN+FP) + FP (FN+TP)} \cr
		&= \dots \cr
\end{align*}

I cannot find some nice, concise connection between Balanced Precision and other metrics.  





