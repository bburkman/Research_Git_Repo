Given a model and a choice of decision threshold $\theta$, the total number of needed ambulances we send (TP) divided by the total number we send (FP + TP) is called the {\it precision}.  Note that TP is all of the elements of the positive class with $p> \theta$, FP is all of the elements of the negative class with $p > \theta$, and FP+TP is all of the elements of either class with $p > \theta$.  

The {\it marginal precision} at $\theta$ is the ratio of the number of positive samples to the total number of samples in the neighborhood of $p$ around $\theta$.  The marginal precision the minimum probability that an ambulance sent is needed.  In the language of economics, it is the probability that last ambulance sent is needed.

For example, if the decision makers are willing to send two unneeded ambulances ($\text{FP} = 2k$ for some $k$) for every one that is needed ($\text{TP} = 1k$), we look for the value of $p$ where $\text{Prec} = \frac{1}{2+1} = 1/3$.  If we want each ambulance sent to have at least a $1/3$ probability of being needed, then we look for the neighborhood of $p$ where $m\text{Prec} = 1/3$.  

The marginal precision is equivalent to the slope of the ROC curve, as there is an invertible mapping between them.  

\begin{align*}
	m\text{ROC} 
	&= \frac{\Delta \text{TPR}}{\Delta \text{FPR}}
	= \frac{ \Delta (\text{TP}/\text{P})}{\Delta (\text{FP}/\text{N})} 
	\cr &
	= \frac{ (\Delta \text{TP})/\text{P}}{(\Delta \text{FP})/\text{N}} \quad  \text{ (because in a given model on a given data set, P and N are constant) }
	\cr &
	= \frac{\text{N}}{\text{P}} \cdot \frac{\Delta \text{TP}}{\Delta \text{FP}}
	= \frac{\text{N}}{\text{P}} \cdot \frac{\Delta \text{TP}/\Delta p}{\Delta \text{FP}/\Delta p}
	= \frac{\text{N}}{\text{P}}  \cdot\frac{\text{Pos}}{\text{Neg}} 
	\cr &
	= \frac{\text{N}}{\text{P}} \cdot \frac{1}{ \frac{\text{Neg}}{\text{Pos}} }
	= \frac{\text{N}}{\text{P}} \cdot \frac{1}{ \frac{\text{Neg}}{\text{Pos}} + 1 - 1 }
	= \frac{\text{N}}{\text{P}} \cdot \frac{1}{ \frac{\text{Neg} + \text{Pos}}{\text{Pos}} - 1 }
	= \frac{\text{N}}{\text{P}} \cdot \frac{1}{ \frac{1}{m\text{Prec}} - 1 }
	= \frac{\text{N}}{\text{P}} \cdot \frac{m\text{Prec}}{1 - m\text{Prec}}
	\cr	
	m\text{Prec} &= \frac{ \text{P} \cdot m\text{ROC}}{\text{N} + \text{P} \cdot m\text{ROC}} \cr 
\end{align*}
 
A challenge with calculating the marginal precision is choosing the margin $\epsilon$ for the neighborhood about $p$.  If we make $\epsilon$ just large enough, the marginal precision will be a decreasing function of $p$ and we will glean one value of $\theta$ where $m\text{Prec}$ is closest to the goal.  Because our data set is discrete, however, too small values of $\epsilon$ will yield some neighborhoods with few or no values of the positive or negative class.  Because two of our model algorithms give most values of $p$ rounded to two decimal places, we have chosen to use one hundred non-overlapping intervals of $p$ ($\epsilon = 0.005$) for our analysis.

The table below gives the values for each of a hundred $p$ neighborhoods for one of our models.  Looking at $p=0.45$ and 0.46, for instance, for $p \in [0, 0.45]$ the model has correctly classified 117,225 of the 180,245 elements of the negative class and 26,573 of the 33,825 elements of the positive class.  Moving from $p=0.45$ to $p=0.46$, the model correctly classifies 3,079 more elements of the negative class and 436 fewer elements of the positive class.  

Claim:  The precision is an increasing function is equivalent to $$\frac{\text{Pos}}{\text{Neg}} 
%= \frac{- \Delta \text{TP}}{ - \Delta \text{FP}} 
%= \frac{\Delta \text{TP}}{\Delta \text{FP}} 
< \frac{\text{TP}}{\text{FP}}$$

which is not necessarily true if we zoom in to a sufficiently small interval of $p$, because of the stochastic and discrete nature of our data set.  Over sufficiently large intervals of $p$, however, it is generally true that precision is an increasing function of the decision boundary $\theta$, being equivalent to the ROC curve curving down.  

If the politicians have decided that they will trade off two immediately dispatched ambulances for each needed ambulance, then we choose the decision threshold $\theta$ at the value of $p$ where $\text{Prec} = 0.33$, which happens around $p = 0.49$.  Note that in this case we would be sending some ambulances to some crashes where there is only a 15\% chance that the ambulance is needed.  Similarly for other political decisions about total tradeoffs; we will investigate $\text{Prec} = 1/2$ and $\text{Prec} = 2/3$. 

If the politicians decide that they want to automatically dispatch ambulances only to notifications where the likelihood that the victim requires an ambulance is greater than 1/3, then choose the decision threshold $\theta$ at the value of $p$ where $m\text{Prec} = 0.33$, which happens around $p = 0.68$.  The marginal precision is much more volatile than the total precision, but we can narrow it down to somewhere in that region.  At this value of $\theta$ over half, $13,617/(12,766 + 13,617) \approx 0.52$, of the ambulances that we automatically dispatch turn out to be needed.  Similarly for other politically-chosen minimum percentages; we will also investigate 1/2 and 2/3.  

\newpage

%\verb|BRFC_Hard_Tomek_0_alpha_0_5_v1_Test_100.tex|

\noindent Balanced Random Forest Classifier, Hard features, No Tomek undersampling, No class weights, Test set, Version 1

\

\hfil\begin{tabular}{rrrrrrrrrrrrrr}
\toprule
p &    Neg &  Pos & $m$Prec &       TN &       FP &      FN &      TP &  Prec &   Rec & $\hat{p}$ \\
\midrule
0.00 &    107 &    0 &  0.00 &      107 &  180,138 &       0 &  33,825 &  0.16 &  1.00 &      1.00 \\
0.01 &    238 &    3 &  0.01 &      345 &  179,900 &       3 &  33,822 &  0.16 &  1.00 &      1.00 \\
0.02 &    331 &    2 &  0.01 &      676 &  179,569 &       5 &  33,820 &  0.16 &  1.00 &      1.00 \\
0.03 &    441 &    3 &  0.01 &    1,117 &  179,128 &       8 &  33,817 &  0.16 &  1.00 &      0.99 \\
0.04 &    526 &    6 &  0.01 &    1,643 &  178,602 &      14 &  33,811 &  0.16 &  1.00 &      0.99 \\
0.05 &    751 &    6 &  0.01 &    2,394 &  177,851 &      20 &  33,805 &  0.16 &  1.00 &      0.99 \\
0.06 &    854 &    8 &  0.01 &    3,248 &  176,997 &      28 &  33,797 &  0.16 &  1.00 &      0.98 \\
0.07 &  1,060 &    9 &  0.01 &    4,308 &  175,937 &      37 &  33,788 &  0.16 &  1.00 &      0.98 \\
0.08 &  1,235 &   13 &  0.01 &    5,543 &  174,702 &      50 &  33,775 &  0.16 &  1.00 &      0.97 \\
0.09 &  1,375 &   16 &  0.01 &    6,918 &  173,327 &      66 &  33,759 &  0.16 &  1.00 &      0.97 \\
0.10 &  1,653 &   16 &  0.01 &    8,571 &  171,674 &      82 &  33,743 &  0.16 &  1.00 &      0.96 \\
$\vdots$ &&&&& $\vdots$ &&&&& $\vdots$ \\
\begin{comment}
0.11 &  1,787 &   23 &  0.01 &   10,358 &  169,887 &     105 &  33,720 &  0.17 &  1.00 &      0.95 \\
0.12 &  1,993 &   34 &  0.02 &   12,351 &  167,894 &     139 &  33,686 &  0.17 &  1.00 &      0.94 \\
0.13 &  2,106 &   47 &  0.02 &   14,457 &  165,788 &     186 &  33,639 &  0.17 &  0.99 &      0.93 \\
0.14 &  2,177 &   44 &  0.02 &   16,634 &  163,611 &     230 &  33,595 &  0.17 &  0.99 &      0.92 \\
0.15 &  2,372 &   40 &  0.02 &   19,006 &  161,239 &     270 &  33,555 &  0.17 &  0.99 &      0.91 \\
0.16 &  2,447 &   58 &  0.02 &   21,453 &  158,792 &     328 &  33,497 &  0.17 &  0.99 &      0.90 \\
0.17 &  2,566 &   66 &  0.03 &   24,019 &  156,226 &     394 &  33,431 &  0.18 &  0.99 &      0.89 \\
0.18 &  2,621 &   66 &  0.02 &   26,640 &  153,605 &     460 &  33,365 &  0.18 &  0.99 &      0.87 \\
0.19 &  2,866 &   88 &  0.03 &   29,506 &  150,739 &     548 &  33,277 &  0.18 &  0.98 &      0.86 \\
0.20 &  2,924 &  110 &  0.04 &   32,430 &  147,815 &     658 &  33,167 &  0.18 &  0.98 &      0.85 \\
0.21 &  3,034 &   91 &  0.03 &   35,464 &  144,781 &     749 &  33,076 &  0.19 &  0.98 &      0.83 \\
0.22 &  2,976 &  106 &  0.03 &   38,440 &  141,805 &     855 &  32,970 &  0.19 &  0.97 &      0.82 \\
0.23 &  3,158 &  120 &  0.04 &   41,598 &  138,647 &     975 &  32,850 &  0.19 &  0.97 &      0.80 \\
0.24 &  3,190 &  146 &  0.04 &   44,788 &  135,457 &   1,121 &  32,704 &  0.19 &  0.97 &      0.79 \\
0.25 &  3,388 &  156 &  0.04 &   48,176 &  132,069 &   1,277 &  32,548 &  0.20 &  0.96 &      0.77 \\
0.26 &  3,336 &  149 &  0.04 &   51,512 &  128,733 &   1,426 &  32,399 &  0.20 &  0.96 &      0.75 \\
0.27 &  3,421 &  185 &  0.05 &   54,933 &  125,312 &   1,611 &  32,214 &  0.20 &  0.95 &      0.74 \\
0.28 &  3,421 &  159 &  0.04 &   58,354 &  121,891 &   1,770 &  32,055 &  0.21 &  0.95 &      0.72 \\
0.29 &  3,516 &  204 &  0.05 &   61,870 &  118,375 &   1,974 &  31,851 &  0.21 &  0.94 &      0.70 \\
0.30 &  3,562 &  226 &  0.06 &   65,432 &  114,813 &   2,200 &  31,625 &  0.22 &  0.93 &      0.68 \\
0.31 &  3,595 &  225 &  0.06 &   69,027 &  111,218 &   2,425 &  31,400 &  0.22 &  0.93 &      0.67 \\
0.32 &  3,544 &  234 &  0.06 &   72,571 &  107,674 &   2,659 &  31,166 &  0.22 &  0.92 &      0.65 \\
0.33 &  3,488 &  283 &  0.08 &   76,059 &  104,186 &   2,942 &  30,883 &  0.23 &  0.91 &      0.63 \\
0.34 &  3,620 &  281 &  0.07 &   79,679 &  100,566 &   3,223 &  30,602 &  0.23 &  0.90 &      0.61 \\
0.35 &  3,594 &  297 &  0.08 &   83,273 &   96,972 &   3,520 &  30,305 &  0.24 &  0.90 &      0.59 \\
0.36 &  3,493 &  319 &  0.08 &   86,766 &   93,479 &   3,839 &  29,986 &  0.24 &  0.89 &      0.58 \\
0.37 &  3,536 &  337 &  0.09 &   90,302 &   89,943 &   4,176 &  29,649 &  0.25 &  0.88 &      0.56 \\
0.38 &  3,508 &  310 &  0.08 &   93,810 &   86,435 &   4,486 &  29,339 &  0.25 &  0.87 &      0.54 \\
0.39 &  3,361 &  328 &  0.09 &   97,171 &   83,074 &   4,814 &  29,011 &  0.26 &  0.86 &      0.52 \\
0.40 &  3,457 &  396 &  0.10 &  100,628 &   79,617 &   5,210 &  28,615 &  0.26 &  0.85 &      0.51 \\
0.41 &  3,278 &  379 &  0.10 &  103,906 &   76,339 &   5,589 &  28,236 &  0.27 &  0.83 &      0.49 \\
0.42 &  3,382 &  381 &  0.10 &  107,288 &   72,957 &   5,970 &  27,855 &  0.28 &  0.82 &      0.47 \\
0.43 &  3,450 &  387 &  0.10 &  110,738 &   69,507 &   6,357 &  27,468 &  0.28 &  0.81 &      0.45 \\
0.44 &  3,281 &  471 &  0.13 &  114,019 &   66,226 &   6,828 &  26,997 &  0.29 &  0.80 &      0.44 \\
\end{comment}
0.45 &  3,206 &  424 &  0.12 &  117,225 &   63,020 &   7,252 &  26,573 &  0.30 &  0.79 &      0.42 \\
0.46 &  3,079 &  436 &  0.12 &  120,304 &   59,941 &   7,688 &  26,137 &  0.30 &  0.77 &      0.40 \\
0.47 &  3,021 &  547 &  0.15 &  123,325 &   56,920 &   8,235 &  25,590 &  0.31 &  0.76 &      0.39 \\
0.48 &  2,990 &  453 &  0.13 &  126,315 &   53,930 &   8,688 &  25,137 &  0.32 &  0.74 &      0.37 \\
0.49 &  3,020 &  533 &  0.15 &  129,335 &   50,910 &   9,221 &  24,604 &  0.33 &  0.73 &      0.35 \\
0.50 &  2,874 &  501 &  0.15 &  132,209 &   48,036 &   9,722 &  24,103 &  0.33 &  0.71 &      0.34 \\
0.51 &  2,804 &  533 &  0.16 &  135,013 &   45,232 &  10,255 &  23,570 &  0.34 &  0.70 &      0.32 \\
0.52 &  2,675 &  542 &  0.17 &  137,688 &   42,557 &  10,797 &  23,028 &  0.35 &  0.68 &      0.31 \\
0.53 &  2,543 &  526 &  0.17 &  140,231 &   40,014 &  11,323 &  22,502 &  0.36 &  0.67 &      0.29 \\
0.54 &  2,438 &  545 &  0.18 &  142,669 &   37,576 &  11,868 &  21,957 &  0.37 &  0.65 &      0.28 \\
0.55 &  2,350 &  579 &  0.20 &  145,019 &   35,226 &  12,447 &  21,378 &  0.38 &  0.63 &      0.26 \\
$\vdots$ &&&&& $\vdots$ &&&&& $\vdots$ \\
\begin{comment}
0.56 &  2,348 &  583 &  0.20 &  147,367 &   32,878 &  13,030 &  20,795 &  0.39 &  0.61 &      0.25 \\
0.57 &  2,192 &  615 &  0.22 &  149,559 &   30,686 &  13,645 &  20,180 &  0.40 &  0.60 &      0.24 \\
0.58 &  2,150 &  570 &  0.21 &  151,709 &   28,536 &  14,215 &  19,610 &  0.41 &  0.58 &      0.22 \\
0.59 &  1,926 &  617 &  0.24 &  153,635 &   26,610 &  14,832 &  18,993 &  0.42 &  0.56 &      0.21 \\
\end{comment}
0.60 &  1,877 &  587 &  0.24 &  155,512 &   24,733 &  15,419 &  18,406 &  0.43 &  0.54 &      0.20 \\
0.61 &  1,756 &  597 &  0.25 &  157,268 &   22,977 &  16,016 &  17,809 &  0.44 &  0.53 &      0.19 \\
0.62 &  1,674 &  632 &  0.27 &  158,942 &   21,303 &  16,648 &  17,177 &  0.45 &  0.51 &      0.18 \\
0.63 &  1,611 &  604 &  0.27 &  160,553 &   19,692 &  17,252 &  16,573 &  0.46 &  0.49 &      0.17 \\
0.64 &  1,582 &  586 &  0.27 &  162,135 &   18,110 &  17,838 &  15,987 &  0.47 &  0.47 &      0.16 \\
0.65 &  1,439 &  618 &  0.30 &  163,574 &   16,671 &  18,456 &  15,369 &  0.48 &  0.45 &      0.15 \\
0.66 &  1,376 &  561 &  0.29 &  164,950 &   15,295 &  19,017 &  14,808 &  0.49 &  0.44 &      0.14 \\
0.67 &  1,288 &  637 &  0.33 &  166,238 &   14,007 &  19,654 &  14,171 &  0.50 &  0.42 &      0.13 \\
0.68 &  1,241 &  554 &  0.31 &  167,479 &   12,766 &  20,208 &  13,617 &  0.52 &  0.40 &      0.12 \\
0.69 &  1,082 &  631 &  0.37 &  168,561 &   11,684 &  20,839 &  12,986 &  0.53 &  0.38 &      0.12 \\
0.70 &  1,053 &  570 &  0.35 &  169,614 &   10,631 &  21,409 &  12,416 &  0.54 &  0.37 &      0.11 \\
0.71 &    922 &  587 &  0.39 &  170,536 &    9,709 &  21,996 &  11,829 &  0.55 &  0.35 &      0.10 \\
0.72 &    897 &  559 &  0.38 &  171,433 &    8,812 &  22,555 &  11,270 &  0.56 &  0.33 &      0.09 \\
0.73 &    783 &  587 &  0.43 &  172,216 &    8,029 &  23,142 &  10,683 &  0.57 &  0.32 &      0.09 \\
0.74 &    831 &  558 &  0.40 &  173,047 &    7,198 &  23,700 &  10,125 &  0.58 &  0.30 &      0.08 \\
0.75 &    711 &  602 &  0.46 &  173,758 &    6,487 &  24,302 &   9,523 &  0.59 &  0.28 &      0.07 \\
$\vdots$ &&&&& $\vdots$ &&&&& $\vdots$ \\
\begin{comment}
0.76 &    664 &  558 &  0.46 &  174,422 &    5,823 &  24,860 &   8,965 &  0.61 &  0.27 &      0.07 \\
0.77 &    627 &  524 &  0.46 &  175,049 &    5,196 &  25,384 &   8,441 &  0.62 &  0.25 &      0.06 \\
0.78 &    585 &  532 &  0.48 &  175,634 &    4,611 &  25,916 &   7,909 &  0.63 &  0.23 &      0.06 \\
0.79 &    568 &  529 &  0.48 &  176,202 &    4,043 &  26,445 &   7,380 &  0.65 &  0.22 &      0.05 \\
0.80 &    458 &  484 &  0.51 &  176,660 &    3,585 &  26,929 &   6,896 &  0.66 &  0.20 &      0.05 \\
0.81 &    429 &  514 &  0.55 &  177,089 &    3,156 &  27,443 &   6,382 &  0.67 &  0.19 &      0.04 \\
0.82 &    399 &  535 &  0.57 &  177,488 &    2,757 &  27,978 &   5,847 &  0.68 &  0.17 &      0.04 \\
0.83 &    337 &  513 &  0.60 &  177,825 &    2,420 &  28,491 &   5,334 &  0.69 &  0.16 &      0.04 \\
0.84 &    350 &  477 &  0.58 &  178,175 &    2,070 &  28,968 &   4,857 &  0.70 &  0.14 &      0.03 \\
0.85 &    286 &  491 &  0.63 &  178,461 &    1,784 &  29,459 &   4,366 &  0.71 &  0.13 &      0.03 \\
0.86 &    271 &  467 &  0.63 &  178,732 &    1,513 &  29,926 &   3,899 &  0.72 &  0.12 &      0.03 \\
0.87 &    222 &  454 &  0.67 &  178,954 &    1,291 &  30,380 &   3,445 &  0.73 &  0.10 &      0.02 \\
0.88 &    185 &  396 &  0.68 &  179,139 &    1,106 &  30,776 &   3,049 &  0.73 &  0.09 &      0.02 \\
0.89 &    192 &  451 &  0.70 &  179,331 &      914 &  31,227 &   2,598 &  0.74 &  0.08 &      0.02 \\
0.90 &    173 &  398 &  0.70 &  179,504 &      741 &  31,625 &   2,200 &  0.75 &  0.07 &      0.01 \\
0.91 &    161 &  392 &  0.71 &  179,665 &      580 &  32,017 &   1,808 &  0.76 &  0.05 &      0.01 \\
0.92 &    135 &  364 &  0.73 &  179,800 &      445 &  32,381 &   1,444 &  0.76 &  0.04 &      0.01 \\
0.93 &    115 &  331 &  0.74 &  179,915 &      330 &  32,712 &   1,113 &  0.77 &  0.03 &      0.01 \\
0.94 &    102 &  259 &  0.72 &  180,017 &      228 &  32,971 &     854 &  0.79 &  0.03 &      0.01 \\
\end{comment}
0.95 &     74 &  251 &  0.77 &  180,091 &      154 &  33,222 &     603 &  0.80 &  0.02 &      0.00 \\
0.96 &     61 &  211 &  0.78 &  180,152 &       93 &  33,433 &     392 &  0.81 &  0.01 &      0.00 \\
0.97 &     55 &  168 &  0.75 &  180,207 &       38 &  33,601 &     224 &  0.85 &  0.01 &      0.00 \\
0.98 &     22 &  125 &  0.85 &  180,229 &       16 &  33,726 &      99 &  0.86 &  0.00 &      0.00 \\
0.99 &     12 &   66 &  0.85 &  180,241 &        4 &  33,792 &      33 &  0.89 &  0.00 &      0.00 \\
1.00 &      4 &   33 &  0.89 &  180,245 &        0 &  33,825 &       0 &   nan &  0.00 &      0.00 \\
\bottomrule
\end{tabular}

\newpage

Visually on the histogram below, when we say that the precision of $1/3$ happens at $p \approx 0.50$, we mean that on the interval $p \in [0.50,1.0]$, the area under the Neg curve is twice the area under the Pos curve.  When we say that the marginal precision of $1/3$ happens at $p \approx 0.7$, we mean that the Neg bar at $p=0.7$ is twice as tall as the Pos bar.  

On the ROC curve below, $m\text{Prec} = 1/3$ happens at $p \approx 0.7$, on the curve at $(\text{FPR},\text{TPR}) = (0.06,0.37)$ because

$$\text{FPR} = \frac{\text{FP}}{\text{N}} = \frac{10,631}{180,091} = 0.06 
\qquad \text{ and } \qquad 
\text{TPR} = \frac{\text{TP}}{\text{P}} = \frac{12,416}{33,825} = 0.37$$

and at that point 
$$m\text{ROC} 
= \frac{\text{N}}{\text{P}} \cdot \frac{m\text{Prec}}{1 - m\text{Prec}} 
= \frac{180,254}{33,825} \cdot \frac{1/3}{1 - 1/3}
\approx 3
$$

\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.3in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\hfil Raw Model Output
	
	\input{../Keras/Images/BRFC_Hard_Tomek_0_alpha_0_5_v1_Test_Pred_Wide.pgf}	
&
	\vskip 0pt
	\hfil ROC Curve
	
	\input{../Keras/Images/BRFC_Hard_Tomek_0_alpha_0_5_v1_Test_ROC.pgf}
	
\end{tabular}














