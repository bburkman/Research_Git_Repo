\subsection{Abstract}

I made most of the changes you suggested for the abstract.  (Above)

I changed ``police'' to ``emergency dispatcher'' rather than ``911 dispatcher'' so as to not annoy the people (like the editor) in countries that use 119.  

I think the abstract is now too long, but that's a problem for later.  

\subsection{Threshold, Prior and Posterior Probabilities}

Thank you for your summary of the decision problem.  I will adapt the language here. I will try to answer your questions through an example.   Please let me know if I don't understand it well.  

\

I'm trying to understand the $\hat{p}$ you used.  I understand $p$ as the probability that a given sample is in Class 1.  Because you related $\hat{p}$ to $\pi_1$, I suspect what $\hat{p}$ means is the percentage of samples the model (with our choice of decision threshold) classifies as being in Class 1.  Is that correct?  In the big chart below, 

$$\hat{p} = \frac{FP + TP}{TN + FP + FN + TP}$$

\

Using the CRSS dataset, our problem has two outcomes.  

\begin{center}
\begin{tabular}{llll}
	Class 0 & ``No ambulance needed'' & $\pi_0 \approx 85\%$ \cr
	Class 1 & ``Ambulance needed'' & $\pi_1 \approx 15\%$ \cr
\end{tabular}
\end{center}

We will run several models, but a particular model gives, for each input vector (sample), a probability $p$ that the sample is in Class 1.  We want to pick a discrimination threshold (decision threshold) $\theta$ such that if for a particular crash notification $p > \theta$, then our recommendation system recommends that the emergency dispatcher send an ambulance.  

Below is the test results of one of our best models, the Balanced Random Forest Classifier with no class weights ($\alpha = 0.5$).  The histogram gives, for each range of $p$, the percent of the total dataset with Class 0 (Neg) and Class 1 in that range of $p$.  

Until we choose the discrimination threshold $\theta$, we can't have a confusion matrix.  Once we choose $\theta$, then all of the negative samples with $p< \theta$ are true negatives, and all of the negative samples with $p > \theta$ are false positives; conversely for the positive samples.  

\


\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/BRFC_Hard_Tomek_0_alpha_0_5_v1_Test_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/BRFC_Hard_Tomek_0_alpha_0_5_v1_Test_ROC.pgf}
\end{tabular}

\
	
In the table below,

\begin{itemize}
	\item The first three columns are the same information as in the histogram.  ``Neg'' (``Pos'') is the number of negative (positive) samples whose probability of being in Class 1 is in that range of $p$.  
	\item The fourth column, ``Neg/Pos,'' is the number of unneeded ambulances sent for each needed ambulance sent, for samples with $p$ in that range.    This is the marginal cost of having $\theta$ below this value of $p$.  These values are given in this plot, emphasizing where Neg/Pos = $\Delta FP/\Delta TP = 2$. 
	
\input{../Keras/Images/BRFC_Hard_Tomek_0_alpha_0_5_v1_Test_FP_TP.pgf}	

The ``TN,'' ``FP,'' ``FN,'' and ``TP'' columns are the confusion matrix if we choose $\theta$ in that range of $p$.  

	\item The seventh column, ``FP/TP'' is related, but it's the total cost in terms of how many unneeded ambulances we'd send for each one we needed if $\theta$ were that value of $p$.  
	
	\item The last three columns are Precision, Recall, and $\hat{p}$ (If I understand $\hat{p}$ correctly).
\end{itemize}



\begin{center}
\input{{../Keras/Analyze_Proba/BRFC_Hard_Tomek_0_alpha_0_5_v1_Test_20.tex}}
\end{center}

Here I've zoomed in on three smaller ranges.  Interestingly, it's not useful to zoom in further, because most values of $p$ given by this implementation of the classifier are only given to two digits.  

\begin{center}
\begin{tabular}{rrrrrrrrrrrrrr}
\toprule
{} &    Neg &  Pos & Neg/Pos &       TN &       FP &      FN &      TP & FP/TP & Prec. &  Rec. & $\hat{p}$ \\
p    &        &      &         &          &          &         &         &       &       &       &           \\
\midrule
0.48 &  2,990 &  453 &    6.60 &  126,315 &   53,930 &   8,688 &  25,137 &  2.15 &  0.32 &  0.74 &      0.37 \\
0.49 &  3,020 &  533 &    5.67 &  129,335 &   50,910 &   9,221 &  24,604 &  2.07 &  0.33 &  0.73 &      0.35 \\
0.50 &  2,874 &  501 &    5.74 &  132,209 &   48,036 &   9,722 &  24,103 &  1.99 &  0.33 &  0.71 &      0.34 \\
0.51 &  2,804 &  533 &    5.26 &  135,013 &   45,232 &  10,255 &  23,570 &  1.92 &  0.34 &  0.70 &      0.32 \\
\hline
0.64 &  1,582 &  586 &    2.70 &  162,135 &   18,110 &  17,838 &  15,987 &  1.13 &  0.47 &  0.47 &      0.16 \\
0.65 &  1,439 &  618 &    2.33 &  163,574 &   16,671 &  18,456 &  15,369 &  1.08 &  0.48 &  0.45 &      0.15 \\
0.66 &  1,376 &  561 &    2.45 &  164,950 &   15,295 &  19,017 &  14,808 &  1.03 &  0.49 &  0.44 &      0.14 \\
0.67 &  1,288 &  637 &    2.02 &  166,238 &   14,007 &  19,654 &  14,171 &  0.99 &  0.50 &  0.42 &      0.13 \\
0.68 &  1,241 &  554 &    2.24 &  167,479 &   12,766 &  20,208 &  13,617 &  0.94 &  0.52 &  0.40 &      0.12 \\
0.69 &  1,082 &  631 &    1.71 &  168,561 &   11,684 &  20,839 &  12,986 &  0.90 &  0.53 &  0.38 &      0.12 \\
0.70 &  1,053 &  570 &    1.85 &  169,614 &   10,631 &  21,409 &  12,416 &  0.86 &  0.54 &  0.37 &      0.11 \\
0.71 &    922 &  587 &    1.57 &  170,536 &    9,709 &  21,996 &  11,829 &  0.82 &  0.55 &  0.35 &      0.10 \\
\hline
0.76 &    664 &  558 &    1.19 &  174,422 &    5,823 &  24,860 &   8,965 &  0.65 &  0.61 &  0.27 &      0.07 \\
0.77 &    627 &  524 &    1.20 &  175,049 &    5,196 &  25,384 &   8,441 &  0.62 &  0.62 &  0.25 &      0.06 \\
0.78 &    585 &  532 &    1.10 &  175,634 &    4,611 &  25,916 &   7,909 &  0.58 &  0.63 &  0.23 &      0.06 \\
0.79 &    568 &  529 &    1.07 &  176,202 &    4,043 &  26,445 &   7,380 &  0.55 &  0.65 &  0.22 &      0.05 \\
0.80 &    458 &  484 &    0.95 &  176,660 &    3,585 &  26,929 &   6,896 &  0.52 &  0.66 &  0.20 &      0.05 \\
0.81 &    429 &  514 &    0.83 &  177,089 &    3,156 &  27,443 &   6,382 &  0.49 &  0.67 &  0.19 &      0.04 \\
0.82 &    399 &  535 &    0.75 &  177,488 &    2,757 &  27,978 &   5,847 &  0.47 &  0.68 &  0.17 &      0.04 \\
\bottomrule
\end{tabular}
\end{center}

How shall we choose $\theta$, the discrimination threshold?  

For example, if we choose the default $\theta = 0.50$, we would send ambulances to 34\% of the automatically reported crashes.  The total cost is 1.99 unneeded ambulances per needed ambulance.  The marginal cost, the difference between making $\theta = 0.50$ and making $\theta = 0.51$, is over 5 (5.74) unneeded ambulances per needed ambulance.  We would be sending an ambulance to each crash with at least a $1/(5.74+1) \approx 15\%$ chance of needing an ambulance.  

One goal of my analysis is figuring out how to choose $\theta$ given some marginal probability that an ambulance is needed, given explicitly or implicitly by the people funding the emergency services.  I think this is actually how the decision is likely to be made by the politicians:  We're willing to send an ambulance early (before an eyewitness report) if there is some probability that it's needed.  

The option I'm exploring is sending an ambulance when there's at least a 33\% chance it will be needed, which happens when Neg/Pos = 2, at about $\theta = 0.68$.  The total cost would be 0.99 unneeded ambulance, and we would be sending an ambulance to 13.16\% of the crashes.  

If we wanted there to be at least a 50\% chance that an ambulance is needed, then we would choose $\theta = 0.80$, where Neg/Pos = 1.  

If we wanted $\hat{p} \approx \pi_1$, then we would choose $\theta = 0.65$.  

\

Does that decision-making method make sense?

\subsection{Other Questions:  Dataset}

The CRSS dataset intentionally over represents more serious crashes; in all police-reported crashes Class 1 is much smaller (2-3\%), but it is also true that very minor crashes (parking lot fender benders) have a similar deceleration profile to hard braking, so minor crashes are less likely to be detected by the phone.   Also, the automated report only goes to the emergency dispatcher if the phone's owner does not respond promptly to the phone.  So we are going to wave our hands and say that the CRSS data is the best approximation we have to the set of crashes reported by automated cell phone reports.  Does that approach seem reasonable?

%%%%%
\subsection{Other Questions:  Validation Set}

I've done my work so far with the data split 70/30 into training and test, and then I did that twice, splitting with a different random seed, to compare results and see whether the differences were within randomness.  Should I do a 60/20/20 training/validation/test set, or use 5-fold cross-validation on the training set?  Or does it matter?  

\newpage
\subsection{Other Questions:  Class Weights}

Below are the raw model outputs \verb|y_proba| for the (neural network) Keras Binary Crossentropy Classifier with three different class weights $\alpha$.  

I had thought the point of class weights was that class weights would put more weight on the misclassified elements of the positive class and make the algorithm would do a better job of separating the two classes.  Using the area under the ROC curve (AUC) as a measure of how well the model separates the classes, the difference between these three models with different class weights is within randomness.  It seems that raising the class weight just pushes both classes together to the right with no useful effect.  Do class weights basically have the same effect as shifting the decision threshold $\theta$?

On the next page I show that if you linearly transform the $p$ values so that $p=0.5$ where $\Delta FP/\Delta TP = 2.0$, then you get nearly the same confusion matrix.  The AUC is invariant under the transformation.  

\
	
Model 1:  $\alpha = 0.5$ for no class weights

\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_0_5_gamma_0_0_v1_Test_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_0_5_gamma_0_0_v1_Test_ROC.pgf}
\end{tabular}

\

Model 2:  $\alpha = 0.67$ for 33\% chance the ambulance is needed

\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_target_gamma_0_0_v1_Test_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_target_gamma_0_0_v1_Test_ROC.pgf}
\end{tabular}

	

\

Model 3:  $\alpha = \pi_0 = 0.84$ for class balance

\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_balanced_gamma_0_0_v1_Test_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_balanced_gamma_0_0_v1_Test_ROC.pgf}
\end{tabular}


\newpage
	
Model 1:  $\alpha = 0.5$ for no class weights

\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_0_5_gamma_0_0_v1_Test_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad FP/TP
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_0_5_gamma_0_0_v1_Test_FP_TP.pgf}
\end{tabular}


\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Transformed Model Output:  Map $0.549$ to 0.5 and 0 to 0.
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_0_5_gamma_0_0_v1_Test_Linear_Transform_Pred_Wide.pgf}
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
168,762 & 11,483
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
22,661 & 11,164
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}

	\hfil\begin{tabular}{ll}
	\cr
0.493 & Precision \cr	0.330 & Recall \cr	0.395 & F1 \cr
\end{tabular}
\end{tabular}

\

Model 2:  $\alpha = 0.67$ for 33\% chance an ambulance is needed

\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-3pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_target_gamma_0_0_v1_Test_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad FP/TP
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_target_gamma_0_0_v1_Test_FP_TP.pgf}
\end{tabular}


\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Transformed Model Output:  Map $0.714$ to 0.5 and 0 to 0.
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_target_gamma_0_0_v1_Test_Linear_Transform_Pred_Wide.pgf}
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
168,598 & 11,647
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
22,513 & 11,312
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}

	\hfil\begin{tabular}{ll}
	\cr
0.493 & Precision \cr	0.334 & Recall \cr	0.398 & F1 \cr
\end{tabular}
\end{tabular}


\

Model 3:  $\alpha = \pi_0 = 0.84$ for class balance

\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-3pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_balanced_gamma_0_0_v1_Test_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad FP/TP
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_balanced_gamma_0_0_v1_Test_FP_TP.pgf}
\end{tabular}


\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Transformed Model Output:  Map $0.873$ to 0.5 and 0 to 0.
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_balanced_gamma_0_0_v1_Test_Linear_Transform_Pred_Wide.pgf}
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
168,381 & 11,864
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
22,587 & 11,238
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}

	\hfil\begin{tabular}{ll}
	\cr
0.487 & Precision \cr	0.332 & Recall \cr	0.395 & F1 \cr
\end{tabular}
\end{tabular}

I ran the same Keras three with the same three class weights, but using $\Delta FP/\Delta TP = 1$, and got similar results, that when you ``normalize'' the output to center where $\Delta FP/\Delta TP = 1$, you get basically the same results.  One had better recall (0.146 instead of 0.156), but otherwise the same.  Then I tried it with $\Delta FP/\Delta TP = 3$, and also basically the same.  

Any differences may be attributable to the values of $p$ not really being continuous, as most of them only have two decimal places, like $0.52$, with very few like $0.5246$, so the differences in the metrics may be attributable to rounding errors.  



%%%%%
\newpage
\subsection{Other Questions:  Overfitting?}

When I use the Keras Binary Crossentropy Classifier and test for overfitting by running the classifier on both the training and test sets, I get basically the same thing, which (I think) means it's not overfitting.  

\

\verb|y_proba = estimator.predict_proba(X_train)|

\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output on Training Set
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_0_5_gamma_0_0_v1_Train_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_0_5_gamma_0_0_v1_Train_ROC.pgf}
\end{tabular}

\verb|y_proba = estimator.predict_proba(X_test)|


\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output on Test Set
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_0_5_gamma_0_0_v1_Test_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/KBFC_Hard_Tomek_0_alpha_0_5_gamma_0_0_v1_Test_ROC.pgf}
\end{tabular}

[continued on next page]
\newpage


When I use the Balanced Random Forest  Classifier, however, I get really different results, which is a sign that it's overfitting.  

\ 

\verb|y_proba = estimator.predict_proba(X_train)|

\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output on Training Set
	
	\input{../Keras/Images/BRFC_Hard_Tomek_0_alpha_0_5_v1_Train_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/BRFC_Hard_Tomek_0_alpha_0_5_v1_Train_ROC.pgf}
\end{tabular}

\verb|y_proba = estimator.predict_proba(X_test)|


\noindent\begin{tabular}{@{\hspace{-6pt}}p{4.5in} @{\hspace{-6pt}}p{2.0in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output on Test Set
	
	\input{../Keras/Images/BRFC_Hard_Tomek_0_alpha_0_5_v1_Test_Pred_Wide.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/BRFC_Hard_Tomek_0_alpha_0_5_v1_Test_ROC.pgf}
\end{tabular}

I tried several of the usual tricks to get it to not overfit, like changing the number of trees, the maximum depth, and the maximum number of leaf nodes, and they gave me poorer results for both the training set and the test set, so I don't know that that's better.  The Balanced Random Forest Classifier is by far the best model algorithm I've used for giving the best AUC, precision, accuracy, and F1.  Is overfitting a problem if it gives the best results on the test set?







