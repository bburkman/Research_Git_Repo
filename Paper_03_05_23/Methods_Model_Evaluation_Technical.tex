%%%
\subsection{Model Evaluation:  Baselines for Comparison}

What do good results look like, what do bad results look like, how do we measure it, and when we compare two results, how much of the difference could be due to randomness?

In the supervised learning method we used here, for each of the $\approx 600,000$ samples (people) in the dataset, we know the answer (the {\it label} or {\it ground truth}) to the question, whether the person needed an ambulance, $y=0$ for ``no'' and $y=1$ for ``yes.''  We are trying use historical data to build a model to predict the label for new data (incoming automated crash notifications).

{\bf
Explain this better.  I confused the internal workings of the model building with the results.
}

We split the data 70/30 into a training set and a test set, making sure to keep the same proportion of positive and negative samples in both.  The binary classification models we used take the training data and training labels (\verb|X_train| and \verb|y_train|) and build a model, then apply the model to the test data (\verb|X_test|) and returns \verb|y_proba| that gives, for each sample, a continuous probability $p \in (0,1)$ that the sample belongs in the positive class.   If a sample has $p = 0.1$, the model is 90\% confident that this sample is in the negative class.   We then pick a threshold, usually but not necessarily $threshold = 0.5$, and make a binary prediction, that samples with $p > threshold$ need an ambulance, and those with $p < threshold$ do not.   

While building the model, the algorithm picks a starting point, measures how badly the model predicts the training data using the {\it loss function}, tweaks the model, measures again, and either keeps or rejects the candidate model based on the loss function.  The loss function used by the model is the sum not of how many binary predictions were incorrect, but how strongly incorrect the continuous predictions were.  If two negative samples ($y=0$) had $p=0.1$ and $p=0.4$, both correct classifications if $threshold=0.5$, then the $p=0.4$ sample would add much more to the loss value.  

A perfect model would not only predict each sample's label correctly, but would do it with perfect certainty.  In the real world, with interesting questions about real data, we will have false positives ($y=0$ and $p>threshold$) and false negatives ($y=1$ and $p<threshold$), but we hope those are few, and that the predictions are strongly correct, meaning the predictions are close to their labels.

When we get results for our models based on crash data, we need some frame of reference for what is ``good'' and ``bad,'' so we have created some sets of entirely artificial results using a gamma distribution for ideal results and a uniform distribution for awful results.  

The histogram below of the percent of samples with predictions $p$ in each range illustrates the best results we can hope for in the real world.  The positive class is small because the data is imbalanced. about 15\% of the dataset, as in our CRSS data.  There are some false positives and negatives, but the overwhelming majority of the predictions are correct, and most with strong confidence.  

The Receiver Operating Characteristic (ROC) is a parameterized curve following the probability threshold from $p=0$ to $p=1$, plotting the true positive rate (TPR) versus the false positive rate (FPR).  The Area Under the ROC curve (AUC) is often used to compare two models, with AUC of 1 indicating perfect prediction and AUC of 0.5 indicating no discernable pattern.  

We have added to the typical ROC curve labels for the medians of the probabilities in the negative and positive classes (0.338 and 0.655) and the default decision threshold $thr = 0.5$.

\begin{tabular}{p{0.5\textwidth} p{0.5\textwidth}}
  \vspace{0pt} \input{../Keras/Images/Ideal_Pred.pgf}
  &
  \vspace{0pt} \input{../Keras/Images/Ideal_ROC.pgf}
\end{tabular}

{\bf Incorporate $thr=0.5$ into the discussion.}

The {\it confusion matrix} for this ideal data set, here given as percentages of the entire dataset, shows few false positives and false negatives.  The metrics below are the ones we will watch when evaluating models.  Each of them tells a different story about what the model does well.

\vskip 6pt

$\displaystyle Precision = \frac{TP}{PP} = \frac{TP}{TP+FP}$ tells what proportion of the ambulances we sent were needed.  

\vskip 6pt

$\displaystyle Recall = \frac{TP}{P} = \frac{TP}{TP + FN}$ tells what proportion of ambulances we needed were sent.

\vskip 6pt

$\displaystyle F1 = \frac{2}{ \frac{1}{Precision} + \frac{1}{Recall}}$ is the harmonic mean of precision and recall. 

\vskip 6pt

Why the harmonic mean, not the arithmetic or geometric mean?  If $a$ and $b$ are positive numbers with $a<b$, then $a < \text{harmonic} < \text{geometric} < \text{arithmetic} < b$.  The harmonic, while being influenced by the larger number, is closest to the smaller, so the harmonic mean emphasizes what the model does poorly.  



\begin{center}
\begin{tabular}{cc}
\begin{tabular}{cc|c|c|c}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{Neg} & \multicolumn{1}{c}{Pos} & \multicolumn{1}{c}{Total} \cr\cline{3-4}
	\multirow{2}{*}{Actual}&Neg &TN = 67.0\% & FP = 18.7\% & N = 85.7\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&Pos & FN = 3.2\% & TP = 11.1\% & P = 14.3\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\multicolumn{2}{r}{Total} & \multicolumn{1}{c}{PN = 70.2\%} & \multicolumn{1}{c}{PP = 29.8\%} \vrule width 0pt height 14pt depth 2pt \cr
\end{tabular}
&
\begin{tabular}{ll}
0.372 & Precision \cr 
0.774 & Recall \cr 
0.502 & F1 \cr 
\end{tabular}
\end{tabular}
\end{center}

%%%%%
If we do not address the data imbalance, the model building algorithm will maximize accuracy by classifying most (or all) of the samples as ``No Ambulance'' with $p < 0.5$  We built the artificial results below by multiplying the probabilities in the above results by $0.5$.  Note that the Area Under the Curve (AUC) did not change.  

\begin{center}
\begin{tabular}{p{0.5\textwidth} p{0.5\textwidth}}
  \vspace{0pt} \input{../Keras/Images/Ideal_50_Pred.pgf}
  &
  \vspace{0pt} \input{../Keras/Images/Ideal_50_ROC.pgf}
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{Actual}&N & 85.7\% & 0.0\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 14.3\% & 0.0\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
\end{tabular}
&
\begin{tabular}{ll}
0.857 & Accuracy \cr 
0.500 & Balanced Accuracy \cr 
0.000 & Precision \cr 
0.000 & Balanced Precision \cr 
0.000 & Recall \cr 
0.000 & F1 \cr 
0.000 & Balanced F1 \cr 
0.000 & Gmean \cr 
	\end{tabular}
\end{tabular}
\end{center}

%%%%%
Such a recommendation system (``Never send an ambulance'') would be useless, but note that the distribution still separates the negative and positive classes, just not at $p=0.5$.  We can fix that in two ways; the first is to shift the distribution to be centered at $p=0.5$.  By ``centered,'' we mean that the average of the medians of the negative and positive classes (the 0.107 and 0.293 on the ROC curve above) will now be 0.5.  Further research can explore whether centering the distribution at the $p=0.5$ threshold or another value of $p$ is most useful.  

\begin{center}
\begin{tabular}{p{0.5\textwidth} p{0.5\textwidth}}
  \vspace{0pt} \input{Ideal_50_Shifted_Pred.pgf}
  &
  \vspace{0pt} \input{Ideal_50_Shifted_ROC.pgf}
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{Actual}&N & 67.2\% & 18.5\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 3.06\% & 11.22\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
\end{tabular}
&
\begin{tabular}{ll}
0.784 & Accuracy \cr 
0.785 & Balanced Accuracy \cr 
0.377 & Precision \cr 
0.784 & Balanced Precision \cr 
0.786 & Recall \cr 
0.510 & F1 \cr 
0.785 & Balanced F1 \cr 
0.543 & Gmean \cr 	\end{tabular}
\end{tabular}
\end{center}

Another way is to linearly transform the probabilities.   Whether the distribution was clustered to the left or right, or clustered at the center, is not necessarily relevant, so we want to see it spread out.  We have arbitrarily chosen a transformation to put next the original models in our results to see if it will make a better model; tuning the transformation is an avenue for future work.  We have chosen to take the 0.05 quantile of the negative class and map it to $p=0.05$, and the 0.95 quantile of the positive class and map it to $p=0.95$.  This linear transformation gives the same metrics as the shift, and the ROC curve is the same except for the two labeled medians, now at 0.305 and 0.688.

\begin{center}
\begin{tabular}{p{0.5\textwidth} p{0.5\textwidth}}
  \vspace{0pt} \input{Ideal_50_Linear_Transform_Pred.pgf}
  &
  \vspace{0pt} \input{Ideal_50_Linear_Transform_ROC.pgf}
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{Actual}&N & 67.5\% & 18.2\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 3.11\% & 11.2\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
\end{tabular}
&
\begin{tabular}{ll}
0.787 & Accuracy \cr 
0.785 & Balanced Accuracy \cr 
0.380 & Precision \cr 
0.786 & Balanced Precision \cr 
0.782 & Recall \cr 
0.512 & F1 \cr 
0.784 & Balanced F1 \cr 
0.547 & Gmean \cr 
	\end{tabular}
\end{tabular}
\end{center}




%%%%%
In the ideal results above, the algorithm learned a useful model from the patterns in the data.  The results below illustrate the worst case scenario, where the algorithm does not learn a good model, usually because the data does not have a pattern that predicts the target variable.  In the ROC curve, the median values of the probabilities for the two classes are so close that the labels are on top of each other.  

\begin{center}
\begin{tabular}{p{0.5\textwidth} p{0.5\textwidth}}
  \vspace{0pt} \input{Awful_Pred.pgf}
  &
  \vspace{0pt} \input{Awful_ROC.pgf}
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{Actual}&N & 41.1\% & 44.6\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 5.71\% & 8.57\% \vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
\end{tabular}
&
\begin{tabular}{ll}
0.497 & Accuracy \cr 
0.540 & Balanced Accuracy \cr 
0.161 & Precision \cr 
0.536 & Balanced Precision \cr 
0.600 & Recall \cr 
0.254 & F1 \cr 
0.566 & Balanced F1 \cr 
0.278 & Gmean \cr 	\end{tabular}
\end{tabular}
\end{center}






